          Майк Икс Коэн




Прикладная линейная алгебра
 для исследователей данных
             Майк Икс Коэн




     Прикладная
  линейная алгебра
 для исследователей
       данных

       От ключевых концепций
до приложений с использованием Python
УДК 512.64
ББК 22.143
    К76




      Коэн M. И.
К76   Прикладная линейная алгебра для исследователей данных / пер. с англ.
      А. В. Логунова. – М.: ДМК Пресс, 2023. – 328 с.: ил.

      ISBN 978-6-01798-945-3
         В этой книге рассказывается о ключевых концепциях линейной алгебры, реа-
      лизованных на Python, и о том, как их использовать в науке о данных, машинном
      и глубоком обучении и вычислительном моделировании. Рассматриваются ин-
      терпретации и приложения векторов и матриц, матричная арифметика, важные
      разложения, используемые в прикладной линейной алгебре, и пр. Прочитав книгу,
      вы научитесь внедрять и адаптировать под свои задачи целый ряд современных
      методов анализа и алгоритмов.
         Издание адресовано специалистам по обработке данных, а также будет полезно
      студентам и широкому кругу разработчиков ПО.




                                                                          УДК 512.64
                                                                          ББК 22.143




         Authorized Russian translation of the English edition of Practical Linear Algebra for Data
      Science ISBN 9781098120610 © 2022 Syncxpress BV. This translation is published and sold by
      permission of O’Reilly Media, Inc., which owns or controls all rights to publish and sell the same.

         Все права защищены. Любая часть этой книги не может быть воспроизведена в ка-
      кой бы то ни было форме и какими бы то ни было средствами без письменного разрешения
      владельцев авторских прав.




ISBN 978-1-098-12061-0 (англ.)                               © 2022 Syncxpress BV
ISBN 978-6-01798-945-3 (каз.)                                © Перевод, оформление, издание,
                                                                Books.kz, 2023
Содержание

От издательства. ...................................................................................................12
Об авторе. ................................................................................................................13
Колофон....................................................................................................................14
Предисловие...........................................................................................................15

Глава 1. Введение . ..............................................................................................17
Что такое линейная алгебра и зачем ее изучать?..................................................17
Об этой книге. .............................................................................................................18
Предварительные требования..................................................................................19
  Математика.............................................................................................................19
  Отношение. .............................................................................................................19
  Программирование................................................................................................20
Математические доказательства в противовес интуитивному
пониманию на основе программирования............................................................20
Рабочий код в книге и предназначенный для скачивания онлайн....................22
Упражнения по программированию.......................................................................22
Как пользоваться этой книгой (для учителей и самообучающихся)..................23


Глава 2. Векторы. Часть 1.................................................................................24
Создание и визуализация векторов в NumPy.........................................................24
  Геометрия векторов. ..............................................................................................27
Операции на векторах. ..............................................................................................28
  Сложение двух векторов........................................................................................28
  Вычитание двух векторов. ....................................................................................29
  Геометрия сложения и вычитания векторов......................................................30
  Умножение вектора на скаляр..............................................................................31
  Сложение скаляра с вектором. .............................................................................32
  Геометрия умножения вектора на скаляр. .........................................................32
6  Содержание

  Транспонирование.................................................................................................33
  Транслирование векторов в Python.....................................................................34
Модуль вектора и единичные векторы. ..................................................................35
Точечное произведение векторов............................................................................36
  Точечное произведение является дистрибутивным.........................................38
  Геометрия точечного произведения. ..................................................................39
Другие умножения векторов.....................................................................................40
  Адамарово умножение. .........................................................................................40
  Внешнее произведение. ........................................................................................41
  Перекрестное и тройное произведения..............................................................42
Ортогональное разложение векторов. ....................................................................42
Резюме..........................................................................................................................46
Упражнения по программированию.......................................................................46


Глава 3. Векторы. Часть 2.................................................................................49
Множества векторов...................................................................................................49
Линейно-взвешенная комбинация..........................................................................50
Линейная независимость..........................................................................................51
  Математика линейной независимости...............................................................53
  Независимость и вектор нулей.............................................................................54
Подпространство и охват. .........................................................................................54
Базис.............................................................................................................................57
  Определение базиса...............................................................................................60
Резюме..........................................................................................................................61
Упражнения по программированию.......................................................................62


Глава 4. Применения векторов. ....................................................................64
Корреляция и косинусное сходство.........................................................................64
Фильтрация временных рядов и обнаружение признаков..................................67
Кластеризация методом k-средних.........................................................................68
Упражнения по программированию.......................................................................71
  Упражнения по корреляции. ................................................................................71
  Упражнения по фильтрации и обнаружению признаков.................................73
  Упражнения по алгоритму k-средних.................................................................75


Глава 5. Матрицы. Часть 1................................................................................76
Создание и визуализация матриц в NumPy. ..........................................................76
  Визуализация, индексация и нарезка матриц...................................................76
  Специальные матрицы..........................................................................................78
Матричная математика: сложение, умножение на скаляр, адамарово
умножение...................................................................................................................80
  Сложение и вычитание..........................................................................................80
  «Сдвиг» матрицы....................................................................................................81
                                                                                                          Содержание  7

  Умножение на скаляр и адамарово умножение. ...............................................82
Стандартное умножение матриц. ............................................................................82
  Правила допустимости умножения матриц.......................................................83
  Умножение матриц. ...............................................................................................84
  Умножение матрицы на вектор. ..........................................................................85
    Линейно-взвешенные комбинации................................................................86
    Результаты геометрических преобразований...............................................86
Матричные операции: транспонирование.............................................................88
  Обозначение точечного и внешнего произведений.........................................88
Матричные операции: LIVE EVIL (порядок следования операций). ..................89
Симметричные матрицы...........................................................................................89
  Создание симметричных матриц из несимметричных...................................90
Резюме..........................................................................................................................91
Упражнения по программированию.......................................................................92


Глава 6. Матрицы. Часть 2................................................................................97
Нормы матриц. ...........................................................................................................97
  След матрицы и норма Фробениуса....................................................................99
Пространства матрицы (столбцовое, строчное, нуль-пространство)...............100
  Столбцовое пространство. ..................................................................................100
  Строчное пространство.......................................................................................104
  Нуль-пространства...............................................................................................104
Ранг.............................................................................................................................108
  Ранги специальных матриц................................................................................110
  Ранг сложенных и умноженных матриц...........................................................112
  Ранг сдвинутых матриц.......................................................................................113
  Теория и практика................................................................................................113
Применения ранга....................................................................................................114
  В столбцовом пространстве................................................................................115
  Линейная независимость множества векторов...............................................116
Определитель............................................................................................................117
  Вычисление определителя..................................................................................117
  Определитель с линейными зависимостями...................................................119
  Характеристический многочлен........................................................................119
Резюме........................................................................................................................121
Упражнения по программированию.....................................................................123


Глава 7. Применения матриц........................................................................128
Матрицы ковариаций многопеременных данных..............................................128
Геометрические преобразования посредством умножения матриц
на векторы. ................................................................................................................131
Обнаружение признаков изображения.................................................................135
Резюме........................................................................................................................138
Упражнения по программированию.....................................................................138
8  Содержание

   Упражнения по матрицам ковариаций и корреляций...................................138
   Упражнения по геометрическим преобразованиям.......................................140
   Упражнения по обнаружению признаков изображения................................142


Глава 8. Обратные матрицы..........................................................................144
Обратная матрица....................................................................................................144
Типы обратных матриц и условия обратимости.................................................145
Вычисление обратной матрицы.............................................................................146
  Обратная матрица матрицы 2×2........................................................................146
  Обратная матрица диагональной матрицы.....................................................148
  Инвертирование любой квадратной полноранговой матрицы....................149
  Односторонние обратные матрицы..................................................................151
Уникальность обратной матрицы..........................................................................153
Псевдообратная матрица Мура–Пенроуза...........................................................154
Численная стабильность обратной матрицы.......................................................155
Геометрическая интерпретация обратной матрицы..........................................156
Резюме........................................................................................................................158
Упражнения по программированию.....................................................................158


Глава 9. Ортогональные матрицы и QR-разложение.......................162
Ортогональные матрицы.........................................................................................162
Процедура Грама–Шмидта......................................................................................164
QR-разложение..........................................................................................................165
  Размеры матриц Q и R.........................................................................................166
    Почему матрица R является верхнетреугольной. .......................................168
  QR и обратные матрицы......................................................................................169
Резюме........................................................................................................................169
Упражнения по программированию.....................................................................170


Глава 10. Приведение строк и LU-разложение...................................174
Системы уравнений. ................................................................................................174
  Конвертирование уравнений в матрицы..........................................................175
  Работа с матричными уравнениями. ................................................................176
Приведение строк.....................................................................................................178
  Метод устранения по Гауссу. ..............................................................................180
  Метод устранения по Гауссу–Жордану. ............................................................181
  Обратная матрица посредством метода устранения
  по Гауссу–Жордану...............................................................................................182
LU-разложение..........................................................................................................183
  Взаимообмен строками посредством матриц перестановок........................185
Резюме........................................................................................................................186
Упражнения по программированию.....................................................................186
                                                                                                       Содержание  9


Глава 11. Общие линейные модели и наименьшие
квадраты.................................................................................................................189
Общие линейные модели. .......................................................................................190
  Терминология. ......................................................................................................190
  Настройка общей линейной модели. ................................................................190
Решение общих линейных моделей. .....................................................................192
  Является ли решение точным?...........................................................................193
  Геометрическая перспектива наименьших квадратов...................................194
  В чем причина работы метода наименьших квадратов?...............................195
Общая линейная модель на простом примере. ...................................................197
Наименьшие квадраты посредством QR-разложения........................................201
Резюме........................................................................................................................202
Упражнения по программированию.....................................................................203


Глава 12. Применения метода наименьших квадратов.................207
Предсказывание количеств велопрокатов на основе погоды............................207
  Регрессионная таблица с использованием библиотеки statsmodels............212
  Мультиколлинеарность. ......................................................................................213
  Регуляризация.......................................................................................................213
Полиномиальная регрессия. ...................................................................................215
Поиск в параметрической решетке для отыскания модельных
параметров................................................................................................................218
Резюме........................................................................................................................220
Упражнения по программированию.....................................................................221
  Упражнения по аренде велосипедов.................................................................221
  Упражнения по мультиколлинеарности...........................................................222
  Упражнения по регуляризации..........................................................................223
  Упражнение по полиномиальной регрессии. ..................................................224
  Упражнения по поиску в параметрической решетке.....................................225


Глава 13. Собственное разложение. .........................................................227
Интерпретации собственных чисел и собственных векторов...........................228
  Геометрия. .............................................................................................................228
  Статистика (анализ главных компонент).........................................................229
  Подавление шума.................................................................................................230
  Уменьшение размерности (сжатие данных). ...................................................231
Отыскание собственных чисел...............................................................................231
Отыскание собственных векторов.........................................................................234
  Неопределенность собственных векторов по знаку и шкале........................235
Диагонализация квадратной матрицы.................................................................236
Особая удивительность симметричных матриц..................................................238
Ортогональные собственные векторы. .................................................................238
  Действительно-значные собственные числа...................................................240
10  Содержание

Собственное разложение сингулярных матриц...................................................241
Квадратичная форма, определенность и собственные числа............................243
  Квадратичная форма матрицы. .........................................................................243
  Определенность ...................................................................................................245
  ATA является положительной (полу)определенной.........................................245
Обобщенное собственное разложение..................................................................246
Резюме........................................................................................................................248
Упражнения по программированию.....................................................................249


Глава 14. Сингулярное разложение..........................................................254
Общая картина сингулярного разложения...........................................................254
  Сингулярные числа и ранг матрицы. ................................................................256
Сингулярное разложение на Python......................................................................256
Сингулярное разложение и одноранговые «слои» матрицы. ............................257
Сингулярное разложение из собственного разложения.....................................259
  Сингулярное разложение матрицы АТА............................................................260
  Конвертация сингулярных чисел в дисперсию: объяснение.........................260
  Кондиционное число. ..........................................................................................261
Сингулярное разложение и псевдообратная матрица Mура–Пенроуза...........262
Резюме........................................................................................................................263
Упражнения по программированию.....................................................................264


Глава 15. Применения собственного и сингулярного
разложений...........................................................................................................268
Анализ главных компонент с использованием собственного
и сингулярногоразложений....................................................................................268
   Математика анализа главных компонент........................................................269
   Шаги выполнения PCA.........................................................................................271
   PCA посредством сингулярного разложения. ..................................................272
Линейный дискриминантный анализ...................................................................273
Низкоранговая аппроксимация посредством сингулярного разложения.......275
   Сингулярное разложение для шумоподавления. ............................................276
Резюме........................................................................................................................276
Упражнения...............................................................................................................277
   Анализ главных компонент (PCA)......................................................................277
   Линейный дискриминантный анализ (LDA)....................................................281
   Сингулярное разложение для низкоранговых аппроксимаций. ..................285
   Сингулярное разложение для шумоподавления в изображениях................287


Глава 16. Краткое руководство по языку Python...............................291
Почему Python и какие есть альтернативы?.........................................................291
Интерактивные среды разработки. .......................................................................292
Использование Python локально и онлайн...........................................................292
                                                                                                     Содержание  11

  Работа с файлами исходного кода в Google Colab. ..........................................293
Переменные...............................................................................................................294
  Типы данных.........................................................................................................296
  Индексация............................................................................................................297
Функции.....................................................................................................................297
  Методы в качестве функций...............................................................................299
  Написание своих собственных функций..........................................................299
  Библиотеки............................................................................................................301
  NumPy.....................................................................................................................301
  Индексация и нарезка в NumPy.........................................................................302
Визуализация. ...........................................................................................................303
Переложение формул в исходный код...................................................................305
Форматирование печати и F-строки. ....................................................................308
Поток управления.....................................................................................................309
  Компараторы.........................................................................................................309
  Инструкции if........................................................................................................310
    Инструкции elif и else......................................................................................310
    Несколько условий...........................................................................................311
  Циклы for. ..............................................................................................................312
  Вложенные инструкции управления.................................................................312
Измерение времени вычислений...........................................................................313
Получение помощи и приобретение новых знаний...........................................313
  Что делать, когда дела идут наперекосяк. ........................................................314
Резюме........................................................................................................................314


Дополнение А. Теорема о ранге и нульности. .....................................315
Тематический указатель.................................................................................317
От издательства

Отзывы и пожелания
Мы всегда рады отзывам наших читателей. Расскажите нам, что вы ду­маете
об этой книге – что понравилось или, может быть, не понравилось. Отзывы
важны для нас, чтобы выпускать книги, которые будут для вас максимально
полезны.
  Вы можете написать отзыв на нашем сайте www.dmkpress.com, зайдя на
страницу книги и оставив комментарий в разделе «Отзывы и рецензии».
Также можно послать письмо главному редактору по адресу dmkpress@gmail.
com; при этом укажите название книги в теме письма.
  Если вы являетесь экспертом в какой-либо области и заинтересованы в на-
писании новой книги, заполните форму на нашем сайте по адресу http://
dmkpress.com/authors/publish_book/ или напишите в издательство по адресу
dmkpress@gmail.com.

Список опечаток
Хотя мы приняли все возможные меры для того, чтобы обеспечить высо-
кое качество наших текстов, ошибки все равно случаются. Если вы найдете
ошибку в одной из наших книг, мы будем очень благодарны, если вы сооб-
щите о ней главному редактору по адресу dmkpress@gmail.com. Сделав это,
вы избавите других читателей от недопонимания и поможете нам улучшить
последующие издания этой книги.

Нарушение авторских прав
Пиратство в интернете по-прежнему остается насущной проблемой. Издатель-
ство «ДМК Пресс» очень серьезно относится к вопросам защиты авторских прав
и лицензирования. Если вы столкнетесь в интернете с незаконной публикацией
какой-либо из наших книг, пожалуйста, пришлите нам ссылку на интернет-ре-
сурс, чтобы мы могли применить санкции.
   Ссылку на подозрительные материалы можно прислать по адресу элект­
ронной почты dmkpress@gmail.com.
   Мы высоко ценим любую помощь по защите наших авторов, благодаря
которой мы можем предоставлять вам качественные материалы.
                                             Об авторе

Майк Икс Коэн – адъюнкт-профессор неврологии1 в Институте Дондерса
(Медицинский центр Университета Радбуда) в Нидерландах. Имеет более
чем 20-летний опыт преподавания научного программирования, анализа
данных, статистики и смежных тем, а также является автором нескольких
онлайновых курсов2 и учебников. У него подозрительно сухое чувство юмора,
и ему нравится все фиолетовое.




1
    См. https://oreil.ly/Ee23F.
2
    См. https://oreil.ly/BurUH.
                                                Колофон

Животное на обложке книги «Прикладная линейная алгебра для исследовате-
лей данных» – это антилопа ньяла, также именуемая низменной ньялой либо
просто ньялой (Tragelaphus angasii). Самки и молодые ньялы обычно имеют
светло-красновато-коричневый окрас шерсти, в то время как взрослые сам-
цы имеют темно-коричневую или даже сероватую шерсть. Как у самцов, так
и у самок есть белые полосы вдоль тела и белые пятна на боках. У самцов
спиралевидные рога, которые вырастают до 33 дюймов в длину, а их шерсть
намного более лохматая, с длинной бахромой, свисающей от горла до задних
конечностей, и гривой густых черных волос вдоль позвоночника. Самки ве-
сят около 130 фунтов, тогда как самцы могут весить до 275 фунтов.
  Ньялы обитают в лесах Юго-Восточной Африки, ареал которых включает
Малави, Мозамбик, Южную Африку, Эсватини, Замбию и Зимбабве. Они пуг­
ливы и предпочитают пастись ранним утром, ближе к вечеру либо ночью,
проводя бóльшую часть жаркой части дня, отдыхая в укрытии. Ньялы обра-
зуют свободные стада численностью до десяти особей, хотя самцы постарше
ведут одиночный образ жизни. Они не являются территориальными живот-
ными, хотя самцы будут бороться за доминирование во время спаривания.
  Ньялы считаются видом, вызывающим наименьшее беспокойство, хотя
выпас скота, сельское хозяйство и потеря среды обитания представляют для
них угрозу. Многие животные на обложках издательства O'Reilly находятся
под угрозой исчезновения, и все они важны для нашего мира.
  Иллюстрация на обложке выполнена Карен Монтгомери по мотивам ста-
ринной линейной гравюры из Histoire Naturelle.
                                        Предисловие

Условные обозначения в книге
В книге используются следующие типографические условные обозначения:
курсивный шрифт
  обозначает новые термины, URL-адреса, адреса электронной почты, имена
  файлов и расширения файлов.
моноширинный шрифт
  используется для листингов программ, а также внутри абзацев для ссылки
  на элементы программ, такие как переменные или имена функций, базы
  данных, типы данных, переменные среды, инструкции и ключевые слова.

    Данный элемент обозначает общее замечание.

    Данный элемент обозначает предупреждение или предостережение.


Использование примеров исходного кода
Дополнительные материалы (примеры исходного кода, упражнения и т. д.)
доступны для скачивания по адресу https://github.com/mikexcohen/LinAlg4Da-
taScience.
  Если у вас есть технический вопрос или проблема с использованием при-
меров исходного кода, то, пожалуйста, отправьте электронное письмо по
адресу bookquestions@oreilly.com.

Благодарности
Должен признаться, я действительно не люблю писать разделы с признания­
ми. И это не потому, что мне не хватает благодарности или я считаю, что
мне некого благодарить, – совсем наоборот: у меня слишком много людей,
которых нужно поблагодарить, и я не знаю, с чего начать, кого перечислить
по имени, а кого пропустить. Должен ли я поблагодарить своих родителей за
их роль в формировании меня таким человеком, который написал эту кни-
гу? Возможно, их родителей за то, что они сформировали моих родителей?
Помню, как моя учительница в четвертом классе говорила мне, что я, должно
быть, стану писателем, когда вырасту. (Я не помню ее имени и не уверен,
когда я вырасту, но, возможно, она оказала некоторое влияние на эту книгу.)
Я написал бóльшую часть этой книги во время поездок на Канарские острова
с работой на удалении; возможно, мне следует поблагодарить пилотов, кото-
16  Предисловие

рые доставили меня туда? Или электриков, которые устанавливали проводку
в коворкингах? Вероятно, я должен быть благодарен Оздемиру Паше за его
роль в популяризации кофе, который одновременно облегчал и отвлекал
меня от писательства. И давайте не будем забывать фермеров, которые вы-
ращивали вкусную еду, которая меня поддерживала и делала счастливым.
   Видите, к чему это ведет: мои пальцы печатали, но потребовалась вся
история человеческой цивилизации, чтобы создать меня и среду, которая
позволила мне написать эту книгу – и которая позволила вам прочитать эту
книгу. Таким образом, спасибо человечеству!
   Ну да ладно, я также могу посвятить один абзац более традиционному
разделу благодарностей. Самое главное, я благодарен всем моим студентам
на моих курсах в университете и летней школе с живым преподаванием,
а также на моих онлайновых курсах Udemy за то, что они доверили мне
преподавание у них и мотивировали меня продолжать совершенствовать
свои объяснения прикладной математики и других технических тем. Я также
благодарен Джесс Хаберману, редактору отдела закупок в O'Reilly, которая
установила «первый контакт», чтобы спросить, не буду ли я заинтересован
в написании этой книги. Шира Эванс (редактор разработки), Джонатан Оуэн
(производственный редактор), Элизабет Оливер (выпускающий редактор),
Кристен Браун (менеджер по контентным услугам) и два эксперта – техни-
ческих рецензента сыграли непосредственную роль в трансформации моих
нажатий клавиш в книгу, которую вы сейчас читаете. Уверен, что этот список
неполон, потому что другие люди, которые помогли опубликовать эту книгу,
мне неизвестны либо потому, что я забыл их из-за потери памяти в моем
преклонном возрасте1. Благодарности всем читающим этот текст, кто чув-
ствует, что внес хотя бы ничтожный вклад в эту книгу.




1
    ЛОЛ, когда я написал эту книгу, мне было 42 года.
Глава       1
                                               Введение


Что такое линейная алгебра и зачем
ее изучать?
Линейная алгебра имеет интересную историю в математике, восходящую
к XVII веку на Западе и гораздо раньше в Китае. Матрицы – развернутые
таблицы чисел, лежащие в основе линейной алгебры, – использовались для
обеспечения компактной системы счисления с целью хранения наборов чи-
сел, таких как геометрические координаты (это было первоначальное приме-
нение мат­риц Декартом), и систем уравнений (впервые введенных Гауссом).
В XX веке мат­ри­цы и векторы использовались для многопеременной матема-
тики, включая математическое исчисление, дифференциальные уравнения,
физику и экономику.
  Но до недавнего времени большинству людей не нужно было заботиться
о матрицах. Однако, как оказалось, компьютеры чрезвычайно эффектив-
ны в работе с мат­ри­ца­ми. И поэтому современные вычисления породили
современную линейную алгебру. Современная линейная алгебра является
вычислительной, в то время как традиционная линейная алгебра является
абстрактной. Современную линейную алгебру лучше всего изучать на ис-
ходном коде и приложениях в области графики, статистики, науки о данных,
искусственного интеллекта и численного моделирования, в то время как
традиционная линейная алгебра изучается на доказательствах и размышле-
ниях о бесконечномерных пространствах векторов. Современная линейная
алгебра предоставляет структурные элементы, которые поддерживают почти
каждый алгоритм, реализованный на компьютерах, в то время как тради-
ционная линейная алгебра зачастую является интеллектуальной пищей для
продвинутых студентов математических университетов.
  Добро пожаловать в современную линейную алгебру.
  Стоит ли вам изучать линейную алгебру? Это зависит от того, хотите ли вы
понимать алгоритмы и процедуры или же просто применять методы, разра-
ботанные другими. Я не хочу принижать последнее – в сущности, нет ничего
18  Введение

плохого в использовании инструментов, которые вы не понимаете (я пишу
это на ноутбуке, который я могу использовать, но не смог создать его с нуля).
Но, учитывая, что вы читаете книгу с таким названием в коллекции книг
O’Reilly, я предполагаю, что вы (1) хотите узнать принцип работы алгоритмов
либо (2) хотите разрабатывать или адаптировать вычислительные методы.
Так что да, вы должны изучать линейную алгебру, и вы должны изучить ее
современную версию.



Об этой книге
  Предназначение этой книги – научить вас современной линейной алгебре.
Но речь идет не о том, чтобы запомнить несколько ключевых уравнений
и пройтись по абстрактным доказательствам; цель в том, чтобы научить вас
думать о матрицах, векторах и операциях на них. Вы разовьете геометриче-
ское понимание на уровне интуиции относительно того, почему линейная
алгебра такова, какая она есть. И вы поймете, как реализовывать концепции
линейной алгебры в рабочем коде Python, уделяя особое внимание прило-
жениям в области машинного обучения и науки о данных.
  Во многих традиционных учебниках по линейной алгебре избегаются чис-
ловые примеры в интересах обобщений, ожидается, что вы самостоятельно
получите сложные доказательства, и преподается множество концепций,
которые имеют мало отношения либо вообще не имеют отношения к при-
менению или реализации на компьютерах. Я пишу эти строки не как кри-
тику – абстрактная линейная алгебра прекрасна и элегантна. Но если ваша
цель – использовать линейную алгебру (и математику в целом) в качестве
инструмента для понимания данных, статистики, глубокого обучения, об-
работки изображений и т. д., то традиционные учебники по линейной алгеб­
ре, возможно, покажутся досадной тратой времени, которая поставит вас
в замешательство и обеспокоит, взяв под сомнение ваши потенциальные
возможности в технической области.
  Эта книга написана с учетом того, что учащиеся занимаются самообуче­
нием. Возможно, у вас есть степень в области математики, инженерии или
физики, но вам нужно научиться реализовывать линейные алгоритмы в ра-
бочем коде. Или же, вполне вероятно, вы не изучали математику в универ-
ситете и теперь осознаете важность линейной алгебры для вашей учебы
или работы. В любом случае, эта книга является самостоятельным ресурсом;
она не представляет собой исключительно дополнение к лекционному курсу
(хотя ее можно было бы использовать для этой цели).
  Если, читая последние три абзаца, вы кивнули головой в знак согласия, то
эта книга определенно для вас.
  Если вы хотите погрузиться в линейную алгебру поглубже, с большим
количеством доказательств и разведывательной работы, то существует не-
сколько отличных учебников, о прочтении которых вы можете подумать,
                                               Предварительные требования  19

включая мой учебник «Линейная алгебра: теория, интуиция, исходный код
(Sincxpress BV)1.



Предварительные требования
Я пытался написать эту книгу для увлеченных учеников с минимальной
формальной подготовкой. И тем не менее ничего никогда не изучается по-
настоящему с нуля.


Математика
Вы должны чувствовать себя комфортно в математике уровня средней шко-
лы. Просто основы алгебры и геометрии, и ничего особенного.
   Для этой книги требуется абсолютно нулевой уровень в исчислении (хотя
дифференциальное исчисление имеет большую важность для приложений,
в которых линейная алгебра используется часто, таких как глубокое обучение
и оптимизация).
   Но самое главное – вам нужно чувствовать себя комфортно, думая о ма-
тематике, рассматривая уравнения и графики и не боясь противостоять
интеллектуальным трудностям, которые возникают при изучении мате-
матики.


Отношение
Линейная алгебра – это раздел математики, и, следовательно, данная книга
посвящена математике. Изучение математики, в особенности во взрослом
возрасте, требует определенного терпения, целеустремленности и напорис­
тости. Выпейте чашку кофе, сделайте глубокий вдох, уберите свой телефон
в другую комнату и погрузитесь в работу.
   В глубине вашей головы будет звучать голос, говорящий о том, что вы
слишком стары или слишком глупы, чтобы изучать продвинутую математи-
ку. Иногда этот голос будет звучать громче, а иногда мягче, но он всегда будет
присутствовать. И это не только у вас – он есть у всех. Этот голос невозможно
подавить или уничтожить; даже не пытайтесь. Просто примите, что некото-
рая неуверенность в себе – это часть человеческого бытия. Всякий раз, когда
этот голос заговаривает, вам приходится доказывать, что он неправ.



1
    Приношу извинения за бесстыдную саморекламу; обещаю, что в данной книге это
    единственный случай, когда я позволяю себе подобное послабление.
20  Введение


Программирование
Данная книга посвящена линейно-алгебраическим приложениям в рабочем
коде. Я написал эту книгу, ориентируясь на Python, потому что Python в на-
стоящее время является наиболее широко используемым языком в науке
о данных, машинном обучении и смежных областях. Если вы предпочитаете
другие языки, такие как MATLAB, R, C или Julia, то я надеюсь, что вам будет
легко переложить рабочий код Python на эти языки.
   Я постарался сделать код Python как можно проще, оставляя его при этом
релевантным для приложений. Глава 16 содержит базовое введение в про-
граммирование на Python. Стоит ли вам просматривать эту главу? Все за-
висит от вашего уровня владения языком Python:

Средний/продвинутый уровень (опыт программирования > 1 года)
  Полностью пропустите главу 16 либо, по возможности, пролистайте ее,
  чтобы получить представление о типе рабочего кода, который появится
  в остальной части книги.

Некоторые знания (опыт < 1 года)
  Рекомендую проработать эту главу на случай, если в ней есть новый мате-
  риал или вам нужно его освежить. Но вы должны быть в состоянии пройти
  ее довольно быстро.

Полный новичок
  Подробно ознакомьтесь с этой главой. Поймите, что данная книга не яв-
  ляется полным руководством по Python, поэтому если вы обнаружите, что
  вам трудно разобраться с рабочим кодом в главах книги, то, возможно,
  вы захотите отложить эту книгу, поработать со специальным курсом или
  книгой по Python, а затем вернуться к этой книге.



Математические доказательства в противовес
интуитивному пониманиюна основе
программирования
Цель изучения математики в общем и целом состоит в том, чтобы понять
математику. Как понять математику? Давайте посчитаем способы:

Строгие доказательства
  Доказательство в математике – это последовательность утверждений, де-
  монстрирующая то, что набор допущений приводит к логическому заклю-
  чению. Доказательства, несомненно, имеют большую важность в чистой
  математике.
            Математические доказательства в противовес интуитивному пониманию  21

Визуализации и примеры
  Четко написанные объяснения, диаграммы и численные примеры помо-
  гут приобретать понимание концепций и операций в линейной алгебре
  на уровне интуиции. Для удобства визуализации большинство примеров
  выполнено в 2D либо 3D, но принципы применимы и к более высоким
  измерениям.

  Разница между ними заключается в том, что формальные математиче-
ские доказательства обеспечивают строгость, но редко понимание на уровне
интуиции, в то время как визуализации и примеры обеспечивают прочное
понимание на уровне интуиции благодаря практическому опыту, но рискуют
приводить к неточностям, будучи основанными на конкретных примерах,
которые не обобщаются.
  Доказательства важных утверждений включены, но я больше концентри-
руюсь на упрочении интуитивного понимания посредством объяснений, ви-
зуализаций и примеров исходного кода.
  И это подводит меня к пониманию математики на уровне интуиции на ос-
нове программирования (тому, что я иногда называю «мягкими доказатель-
ствами»). Вот идея: вы исходите из того, что в Python (и таких библиотеках,
как NumPy и SciPy) правильно реализована низкоуровневая обработка чи-
сел, в то время как вы концентрируетесь на принципах путем обследования
многочисленных числовых примеров в исходном коде.
  Краткий пример: мы «мягко докажем» принцип коммутативности умно-
жения, который гласит, что a × b = b × a:
a = np.random.randn()
b = np.random.randn()
a * b - b * a

  Приведенный выше исходный код генерирует два случайных числа и про-
веряет гипотезу о том, что изменение порядка умножения не влияет на ре-
зультат. Если принцип коммутативности истинен, то в третьей строке будет
выведено значение 0.0. Если вы выполните этот исходный код несколько раз
и всегда будете получать 0.0, то, увидев один и тот же результат в многочис-
ленных примерах с разными числами, вы приобретете понимание коммута-
тивности на уровне интуиции.
  Для ясности: понимание на уровне интуиции на основе исходного кода
не заменяет строгого математического доказательства. Все дело в том, что
«мягкие доказательства» позволяют понимать математические концепции,
не беспокоясь о деталях абстрактного математического синтаксиса и аргу-
ментов. Это особенно выгодно для программистов, которым не хватает про-
двинутого математического образования.
  В сухом остатке все сводится к тому, что много математических концепций
могут усваиваться при помощи небольшой порции исходного кода.
22  Введение


Рабочий код в книге и предназначенный
для скачивания онлайн
Данную книгу можно читать, не глядя на исходный код и не решая упраж-
нения по программированию. Все в порядке; вы обязательно чему-нибудь
научитесь. Но не расстраивайтесь, если ваши знания будут поверхностными
и мимолетными. Если вы хотите действительно понять линейную алгебру, то
вам нужно решать задачи. Вот почему эта книга снабжена демонстрациями
исходного кода и упражнениями по каждой математической концепции.
   Важный исходный код напечатан непосредственно в книге. Я хочу, чтобы
вы читали текст и уравнения, смотрели на графики и одновременно видели
исходный код. Это позволит вам связывать концепции и уравнения с исход-
ным кодом.
   Но листинг исходного кода в книге может занимать много места, а руч-
ное его копирование на вашем компьютере будет утомительным. Поэтому
на страницах книги печатаются только ключевые строки кода; онлайновый
код содержит дополнительный исходный код, комментарии, графические
украшения и т. д. Онлайновый код также содержит решения упражнений по
программированию (всех упражнений, а не только выборочных задач!). Вам
обязательно следует скачать исходный код и ознакомиться с ним во время
работы с книгой.
   Весь исходный код можно получить из репозитория на сайте GitHub https://
github.com/mikexcohen/LinAlg4DataScience. Этот репозиторий можно клони-
ровать либо просто скачать целиком в виде ZIP-файла (вам не потребуется
регистрироваться, входить в систему либо платить за скачивание кода).
   Я написал исходный код в среде Google Colab, используя блокнот Jupyter.
Я решил использовать Jupyter, потому что это дружественная и простая
в применении среда. Тем не менее призываю вас использовать любую интег­
рированную среду разработки на Python, которую вы предпочитаете. Для
удобства онлайновый код также предоставляется в виде простых файлов .py.



Упражнения по программированию
Математика – это не зрительский вид спорта. В большинстве книг по мате-
матике есть бесчисленное множество письменных задач (и давайте будем
честны: никто не решает их все). Но данная книга целиком посвящена при-
кладной линейной алгебре, и никто не применяет линейную алгебру на бу-
маге! Вместо этого линейная алгебра применяется в рабочем коде. Поэтому
вместо ручных задач и утомительных доказательств, «оставленных читателю
в качестве упражнения» (как любят писать авторы учебников по математи-
ке), в данной книге много упражнений по программированию.
   Упражнения по программированию различаются по сложности. Если вы
в Python и линейной алгебре – новичок, то некоторые упражнения, возмож-
                Как пользоваться этой книгой (для учителей и самообучающихся)  23

но, покажутся вам действительно сложными. Если вы застряли, то вот вам
совет: кратко взгляните на мое решение, чтобы вдохновиться, затем уберите
его, чтобы не видеть мой исходный код, и продолжайте работать над своим
исходным кодом.
  Сравнивая свое решение с моим, имейте в виду, что существует много
способов решения задач на Python. Важно найти правильный ответ; пред-
принимаемые вами шаги, чтобы его получить, нередко зависят от личного
стиля программирования.



Как пользоваться этой книгой
(для учителей и самообучающихся)
Данная книга полезна в трех средах:

Самообучение
  Я постарался сделать эту книгу доступной для читателей, которые хотят
  изучать линейную алгебру самостоятельно, вне формальной классной
  комнаты. Никаких дополнительных ресурсов или онлайновых лекций не
  требуется, хотя, конечно же, существует масса других книг, веб-сайтов, ви-
  деороликов YouTube и онлайновых курсов, которые студенты могут счесть
  полезными.

Первичный учебник на занятиях по науке о данных
  Данная книга может использоваться в качестве первичного учебника
  в курсе математики, лежащей в основе науки о данных, машинного об-
  учения, искусственного интеллекта и смежных тем. Содержимое состоит
  из 14 глав (исключая это введение и дополнительный материал по Python),
  и от студентов ожидается, что они будут работать над одной-двумя глава-
  ми в неделю. Поскольку учащиеся имеют доступ к решениям всех упраж-
  нений, преподаватели, возможно, пожелают дополнить упражнения книги
  дополнительными наборами задач.

Вторичный учебник по математикоориентированному курсу линейной алгебры
  Эта книга также может использоватся в качестве дополнения к курсу ма-
  тематики с сильным акцентом на доказательствах. В этом случае лекции
  были бы сосредоточены на теории и строгих доказательствах, в то вре-
  мя как на данную книгу можно было бы ссылаться с целью переложения
  концепций в исходный код с прицелом на приложения в науке о данных
  и машинном обучении. Как я написал выше, преподаватели, возможно,
  пожелают предоставить дополнительные упражнения, поскольку решения
  ко всем упражнениям из книги доступны онлайн.
Глава          2
                               Векторы. Часть 1

Векторы обеспечивают основу, на которой построена вся линейная алгебра
(и, следовательно, остальная часть этой книги).
   К концу этой главы вы будете знать о векторах все: что они собой представ-
ляют, что они делают, как их интерпретировать и как создавать и работать
с ними на Python. Вы поймете наиболее важные операции на векторах, вклю-
чая векторную алгебру и точечное произведение. Наконец, вы узнаете о раз-
ложениях векторов, являющихся одной из главных целей линейной алгебры.



Создание и визуализация векторов в NumPy
В линейной алгебре вектор – это упорядоченный список чисел. (В абстракт-
ной линейной алгебре векторы могут содержать другие математические объ-
екты, включая функции; однако поскольку данная книга ориентирована на
приложения, мы будем рассматривать только те векторы, которые содержат
числа.)
   Векторы обладают несколькими важными характеристиками. Первые две,
с которых мы начнем, – это1:

размерность.
  Число чисел в векторе;

ориентация.
  Ориентирован ли вектор вдоль столбца (стоя в полный рост) либо вдоль
  строки (лежа ровно во всю ширь).

  Размерность нередко указывается с помощью причудливо выглядящей
записи ℝN, где ℝ обозначает действительно-значные числа (ср. c символом

1
    В книге проводится четкое различие между созвучными понятиями dimensionality
    (математическая размерность, протяженность, объемность) и dimension (геомет­
    рическое измерение, мерность как в слове двухмерный). Первый термин перево-
    дится как размерность, а второй в зависимости от контекста – как измерение либо
    мерность. – Прим. перев.
                                       Создание и визуализация векторов в NumPy  25

ℂ для комплексно-значных чисел), и N указывает на размерность. Например,
считается, что вектор с двумя элементами принадлежит ℝ2.
   Специальный символ ℝ создан с использованием кода latex, но вы также
можете написать R2, R2 или R^2.
   Уравнение 2.1 показывает несколько примеров векторов; перед чтением
следующего абзаца рекомендуется определить их размерность и ориентацию.

    Уравнение 2.1. Примеры вектора-столбца и вектора-строки




  Вот ответы: x – четырехмерный вектор-столбец, y – двумерный вектор-
столбец и z – четырехмерный вектор-строка. Также можно написать, напри-
мер, x Î ℝ2, где символ Î означает «принадлежит множеству».
  Являются ли x и z одним и тем же вектором? Технически они разные, хотя
и содержат одни и те же элементы в одном и том же порядке. Более под-
робное изложение этого вопроса смотрите во врезке «Имеет ли значение
ориентация вектора?» на стр. 26.
  Из этой книги и на протяжении всех ваших приключений, связанных с ин-
теграцией математики и программирования, вы узнаете, что существуют
различия между математикой «на меловой доске» и реализованной в ра-
бочем коде. Некоторые расхождения незначительны и несущественны, в то
время как другие вызывают путаницу и ошибки. Давайте-ка я теперь позна-
комлю вас с терминологическим разночтением между математикой и про-
граммированием.
  Ранее я писал, что размерность вектора – это число элементов в этом век-
торе. Однако в Python размерность вектора или мат­ри­цы – это число гео-
метрических измерений, используемых для распечатки числового объекта.
Например, все показанные выше векторы считаются в Python «двумерными
массивами» независимо от содержащегося в векторах числа элементов (то
есть математической размерности). Список чисел без определенной ори-
ентации в Python считается одномерным массивом независимо от числа
элементов (этот массив будет распечатан в виде строки, но, как вы увидите
позже, он обрабатывается иначе, чем вектор-строка). Математическая раз-
мерность – число элементов в векторе – в Python называется длиной, или
очертанием1, вектора.
  Бывает, что эта несогласованная, а иногда и противоречивая терминология
сбивает с толку. И действительно, терминология нередко становится сложной
проблемой на пересечении различных дисциплин (в данном случае матема-
тики и вычислительной науки). Но не волнуйтесь, вы это освоите, приобретя
немного опыта.


1
    Англ. shape; син. форма, контур. – Прим. перев.
26  Векторы. Часть 1

   При упоминании векторов обычно используются строчные жирные рим-
ские буквы, например v для обозначения «вектор v». В некоторых учебниках
используется курсив (v) или вверху выводится стрелка в качестве диакрити-
ческого знака (v→).
   По традиции в линейной алгебре исходят из допущения, что векторы
ориентированы вдоль столбца, если не указано иное. Векторы-строки за-
писываются в виде wT. Надстрочная буква T указывает на операцию транс-
понирования, о которой вы узнаете подробнее чуть позже; пока же достаточ-
но сказать, что операция транспонирования конвертирует вектор-столбец
в вектор-строку.

     Имеет ли значение ориентация вектора?
     Действительно ли нужно беспокоиться о том, ориентированы ли векторы вдоль
     столбцов либо вдоль строк или же они являются неориентированными одномерными
     массивами? Иногда – да, а иногда – нет. При использовании векторов для хранения
     данных ориентация обычно не имеет значения. Но если ориентация неправильная,
     то некоторые операции в Python могут давать ошибки либо неожиданные результаты.
     Поэтому важно понимать ориентацию векторов, поскольку, тратя 30 минут на отладку
     кода только для того, чтобы понять, что вектор-строка должен быть вектором-столб-
     цом, вы гарантированно получите головную боль.

   Векторы в Python могут быть представлены с использованием несколь-
ких типов данных. Тип «список», возможно, покажется самым элементар-
ным способом представления вектора – и это для некоторых приложений.
Но многие линейно-алгебраические операции не будут работать со списка-
ми Python. Поэтому в большинстве случаев лучше всего создавать векторы
в виде массивов NumPy. В следующем ниже исходном коде показаны четыре
способа создания вектора:
asList    =   [1, 2, 3]
asArray   =   np.array([1, 2, 3])         # одномерный массив
rowVec    =   np.array([ [1, 2, 3] ])     # строка
colVec    =   np.array([ [1], [2], [3] ]) # столбец

   Переменная asArray – это неориентированный массив, и, значит, это ни век-
тор-строка, ни вектор-столбец, а просто одномерный список чисел в NumPy.
В NumPy ориентация задается скобками: самые внешние скобки группируют
все числа вместе в один объект. Затем каждый дополнительный набор скобок
указывает строку: вектор-строка (переменная rowVec) содержит все числа
в одной строке, в то время как вектор-столбец (переменная colVec) содержит
несколько строк, причем каждая строка содержит одно число.
   С этими ориентациями можно познакомиться поближе, проинспектиро-
вав очертания переменных (в программировании проверка очертаний пере-
менных нередко бывает очень полезной):
                                           Создание и визуализация векторов в NumPy  27

print(f'asList:     {np.shape(asList)}')
print(f'asArray:    {asArray.shape}')
print(f'rowVec:     {rowVec.shape}')
print(f'colVec:     {colVec.shape}')

    Вот как выглядит результат:
asList:    (3,)
asArray:   (3,)
rowVec:    (1, 3)
colVec:    (3, 1)

  Результат показывает, что одномерный массив asArray имеет размер (3),
тогда как наделенные ориентацией векторы являются двумерными масси-
вами и хранятся как размер (1,3) либо (3,1) в зависимости от ориентации.
Размеры всегда указываются как (строки, столбцы).


Геометрия векторов
Упорядоченный список чисел – это алгебраическая интерпретация вектора;
геометрическая интерпретация вектора представляет собой прямой отрезок
с определенной длиной (также именуемой модулем вектора1) и направлени-
ем (также именуемым углом; он вычисляется относительно положительной
оси x). Две точки вектора называются хвостом (где он начинается) и головой
(где он заканчивается); голова часто имеет наконечник стрелки, чтобы устра-
нить неоднозначность с хвостом.
   Возможно, вы подумали, что в векторе кодируется геометрическая коор-
дината, но векторы и координаты – это на самом деле разные вещи. Однако
они согласуются, когда вектор начинается в начале координат. Этот случай
называется стандартным положением и проиллюстрирован на рис. 2.1.
   Концептуализация векторов в геометрическом либо алгебраическом пла-
не облегчает интуитивное понимание в различных приложениях, но это
просто две стороны одной медали. Например, геометрическая интерпре-
тация вектора широко применяется в физике и инженерии (например, для
представления физических сил), а алгебраическая интерпретация вектора –
в науке о данных (например, для хранения данных о продажах во времен-
ной динамике). Нередко линейно-алгебраические концепции усваиваются
геометрически на 2D-графиках, а затем расширяются до более высоких из-
мерений с помощью алгебры.



1
    Англ. magnitude. – Прим. перев.
28  Векторы. Часть 1




                Рис. 2.1  Все стрелки выражают один и тот же вектор.
         Вектор в стандартном положении имеет свой хвост в начале координат
              и свою голову в согласованной геометрической координате




Операции на векторах
Векторы подобны существительным; они являются персонажами нашего
рассказа о линейной алгебре. Веселье в линейной алгебре исходит от гла-
голов – действий, которые вдыхают жизнь в персонажей. Эти действия на-
зываются операциями.
   Некоторые линейно-алгебраические операции элементарны и интуитив-
но понятны и работают именно так, как вы и ожидаете (например, сложение),
в то время как другие более сложны и требуют объяснений в объеме целых
глав (например, сингулярное разложение). Давайте начнем с элементарных
операций.


Сложение двух векторов
Для того чтобы сложить два вектора, надо просто сложить каждый соответ-
ствующий элемент. Уравнение 2.2 показывает пример:
                                                  Операции на векторах  29

  Уравнение 2.2. Сложение двух векторов




  Как вы могли догадаться, сложение векторов определено только для двух
векторов, которые имеют одинаковую размерность; например, невозможно
сложить вектор в ℝ3с вектором в ℝ5.


Вычитание двух векторов
Вычитание векторов – это тоже то, что вы и ожидаете: вычесть второй вектор
из первого поэлементно.
  Уравнение 2.3 показывает пример:

  Уравнение 2.3. Вычитание двух векторов




  Сложение векторов на языке Python выполняется просто:
v = np.array([ 4, 5, 6])
w = np.array([10,20,30])
u = np.array([ 0, 3, 6, 9])
vPlusW = v+w
uPlusW = u+w # ошибка! измерения не совпадают!

  Имеет ли значение ориентация вектора для сложения? Рассмотрим урав-
нение 2.4:

  Уравнение 2.4. Можно ли сложить вектор-строку с вектором-столбцом?




  Возможно, вы подумаете, что между этим примером и тем, что был по-
казан ранее, нет никакой разницы – в конце концов, оба вектора состоят из
трех элементов. Давайте посмотрим, что сделает Python:
v = np.array([[ 4, 5, 6]])   # вектор-строка
w = np.array([[10,20,30]]).T # вектор-столбец
v+w
30  Векторы. Часть 1

>> array([[14, 15, 16],
          [24, 25, 26],
          [34, 35, 36]])

   Возможно, результат покажется запутанным и несовместимым с приве-
денным ранее определением сложения векторов. На самом деле в Python
реализована операция, именуемая транслированием. Вы узнаете о транс-
лировании больше чуть позже в этой главе, но я призываю вас потратить
немного времени на осмысление результата и подумать о том, как он воз-
ник в результате сложения вектора-строки с вектором-столбцом. Несмотря
на это, данный пример показывает, что ориентация действительно важна:
два вектора можно сложить, только если они имеют одинаковую размерность
и одинаковую ориентацию.


Геометрия сложения и вычитания векторов
Для того чтобы сложить два вектора геометрически, надо расположить век-
торы так, чтобы хвост одного вектора находился в голове другого вектора.
Суммируемый вектор проходит от хвоста первого вектора к голове второго
(график А на рис. 2.2). Эту процедуру можно расширить, чтобы суммировать
любое число векторов: надо просто уложить все векторы от хвоста к голове,
и тогда сумма будет равна отрезку, идущему от первого хвоста к итоговой
голове.

 А)                                            B)




                        Рис. 2.2  Сумма и разность двух векторов

  Геометрическое вычитание векторов немного отличается, но является
одинаково элементарным: надо выровнять два вектора так, чтобы их хвос­
ты находились в одной и той же координате (это легко достигается, если оба
вектора находятся в стандартном положении); вектор разности – это отрезок,
                                                           Операции на векторах  31

который идет от головы «отрицательного» вектора к голове «положительно-
го» вектора (график В на рис. 2.2).
  Не стоит недооценивать важность геометрии вычитания векторов: она ле-
жит в основе ортогонального разложения векторов, которое, в свою очередь,
лежит в основе метода линейных наименьших квадратов, который является
одним из наиболее важных приложений линейной алгебры в науке и технике.


Умножение вектора на скаляр
Скаляр в линейной алгебре – это число в чистом виде, не вложенное ни в век-
тор, ни в мат­ри­цу. Скаляры обычно обозначаются строчными греческими
буквами, такими как α или λ. Поэтому умножение вектора на скаляр обо-
значается, например, как βu.
  Умножение вектора на скаляр выполняется очень просто: надо умножить
каждый элемент вектора на скаляр. Для понимания будет достаточно одного
численного примера (уравнение 2.5):

  Уравнение 2.5. Умножение вектора на скаляр (также умножение скаляра
  на вектор)




                                      Вектор нулей
  Вектор, состоящий из одних нулей, также именуемый вектором нулей, или нуль-вектором,
  обозначается жирным шрифтом, 0, и в линейной алгебре является специальным векто-
  ром. Нередко использование вектора нулей для решения задачи фактически принято на-
  зывать тривиальным решением и исключать. Линейная алгебра полна утверждений типа
  «найти ненулевой вектор, который может решить ...» или «найти нетривиальное решение
  для ...».


  Ранее я писал, что тип данных хранящей вектор переменной иногда ва-
жен, а иногда и не важен. Умножение вектора на скаляр является примером
случая, когда тип данных имеет значение:
s = 2
a = [3, 4, 5]   # в виде списка
b = np.array(a) # в виде np-массива
print(a*s)
print(b*s)

>> [ 3, 4, 5, 3, 4, 5 ]
>> [ 6 8 10 ]
32  Векторы. Часть 1

   Приведенный выше исходный код создает скаляр (переменную s) и вектор
в виде списка (переменную a), затем конвертирует их в массив NumPy (пере-
менную b). В Python звездочка перегружена, то есть ее поведение зависит
от типа переменной: умножение списка на скаляр повторяет список s раз
(в данном случае два раза), что определенно не является линейно-алгебраи-
ческой операцией умножения скаляра на вектор. Однако когда вектор хра-
нится в виде массива NumPy, звездочка интерпретируется как поэлементное
умножение. (Вот для вас небольшое упражнение: что произойдет, если задать
s = 2.0, и почему1?) Обе эти операции (повторение списка и умножение век-
тора на скаляр) используются в реальном программировании, поэтому об
указанном различии следует помнить.


Сложение скаляра с вектором
Сложение скаляра с вектором формально в линейной алгебре не определе-
но: это два отдельных вида математических объектов, которые невозможно
объединить. Однако программы числовой обработки, такие как Python, по-
зволяют складывать скаляры с векторами, и указанная операция сравнима
с умножением скаляра на вектор: скаляр прибавляется к каждому элементу
вектора. Следующий ниже исходный код иллюстрирует эту идею:
s = 2
v = np.array([3, 6])
s+v

>> [5 8]



Геометрия умножения вектора на скаляр
Почему скаляры называются «скалярами»? Это вытекает из геометрической
интерпретации. Скаляры шкалируют векторы, не меняя их направления.
Существует четыре эффекта умножения вектора на скаляр, которые зависят
от того, является ли скаляр больше 1, между 0 и 1, в точности 0 либо отрица-
тельным. Рисунок 2.3 иллюстрирует эту идею.
   Ранее я писал, что скаляры не меняют направление вектора. Но на рисунке
показано, что направление вектора меняется, когда скаляр отрицательный
(то есть его угол поворачивается на 180°). Возможно, это покажется противо-
речием, но существует интерпретация векторов, в которой они указыва-
ют вдоль бесконечно длинной линии, проходящей через начало координат
и уходящей в бесконечность в обоих направлениях (в следующей главе я буду
называть такую интерпретацию «одномерным подпространством»). В этом
смысле «повернутый» вектор по-прежнему указывает вдоль той же самой


1
    Выражение a*s выдаст ошибку, потому что повторять список можно только с ис-
    пользованием целых чисел; невозможно повторить список 2.72 раза!
                                                          Операции на векторах  33

бесконечной линии, и, следовательно, отрицательный скаляр не меняет на-
правления. Указанная интерпретация важна для пространств мат­риц, соб-
ственных векторов и сингулярных векторов, все из которых представлены
в последующих главах.




                  Рис. 2.3  Один и тот же вектор (черная стрелка),
    умноженный на разные скаляры σ (серый отрезок; для наглядности слегка сдвинут)

   Умножение вектора на скаляр в сочетании со сложением векторов приво-
дит непосредственно к усреднению векторов. Усреднение векторов – это то же
самое, что усреднение чисел: просуммировать и поделить на количество чи-
сел. Таким образом, для того чтобы усреднить два вектора, надо их сложить,
а затем скалярно умножить на .5. В общем случае, для того чтобы усреднить
N векторов, надо их просуммировать и скалярно умножить результат на 1/N.


Транспонирование
Вы уже узнали об операции транспонирования: она конвертирует векто-
ры-столбцы в векторы-строки и наоборот. Тут стоит дать несколько более
формальное определение, которое будет обобщено на транспонирование
мат­риц (тема в главе 5).
  Матрица состоит из строк и столбцов; следовательно, каждый элемент
мат­ри­цы имеет индекс в формате (строка, столбец). Операция транспони-
рования просто меняет местами эти индексы. Она формализуется в урав-
нении 2.6:

  Уравнение 2.6. Операция транспонирования

  mTi, j = mj,i.

   Векторы имеют либо одну строку, либо один столбец, в зависимости от их
ориентации. Например, шестимерный вектор-строка имеет i = 1 и индексы
j от 1 до 6, тогда как шестимерный вектор-столбец имеет индексы i от 1 до 6
и j = 1. Таким образом, перемена мест индексов i, j меняет местами строки
и столбцы.
   Вот важное правило: двойное транспонирование возвращает вектор в из-
начальную ориентацию. Другими словами, vTT = v. Возможно, указанное
правило покажется очевидным и тривиальным, но оно является краеуголь-
ным камнем нескольких важных доказательств в науке о данных и машин-
34  Векторы. Часть 1

ном обучении, включая создание симметричных мат­риц ковариаций при
умножении мат­ри­цы данных на ее транспонированную версию (что, в свою
очередь, является причиной того, что анализ главных компонент есть орто-
гональный поворот пространства данных... не волнуйтесь, это предложение
обретет смысл в данной книге чуть позже!).


Транслирование векторов в Python
Транслирование – это операция, которая существует только в современной
компьютерной линейной алгебре; это не та процедура, которую вы бы нашли
в традиционном учебнике по линейной алгебре.
   Транслирование, по существу, означает многократное повторение опера-
ции между одним вектором и каждым элементом другого вектора. Рассмот­
рим следующую ниже серию уравнений1:

    [1 1] + [10 20];
    [2 2] + [10 20];
    [3 3] + [10 20].

  Обратите внимание на закономерности в векторах. Приведенный выше
набор уравнений можно компактно реализовать, сжав указанные закономер-
ности в векторы [1 2 3] и [10 20], а затем оттранслировав сложение. Вот как
это выглядит на Python:
v = np.array([[1, 2, 3]]).T # вектор-столбец
w = np.array([[10, 20]])    # вектор-строка
v + w # сложение при помощи транслирования

>> array([[11, 21],
          [12, 22],
          [13, 23]])

  Здесь вы снова заметите важность ориентации в линейно-алгебраических
операциях: попробуйте выполнить приведенный выше исходный код, из-
менив v на вектор-строку и w – на вектор-столбец2.
  Поскольку транслирование позволяет проводить эффективные и компакт-
ные вычисления, оно встречается в числовом программировании очень час­
то. В данной книге вы увидите несколько примеров транслирования, в том
числе в разделе, посвященном кластеризации методом k-средних (глава 4).


1
    Для ясности вот как данный термин трактуется в документации NumPy: термин
    «транслирование» (англ. broadcasting) относится к тому, как NumPy обрабатыва-
    ет массивы с разным размером во время арифметических операций. С учетом
    определенных ограничений меньший массив «заполняется» по большему массиву,
    чтобы они имели совместимые очертания. – Прим. перев.
2
    Python по-прежнему выполнит транслирование, но в результате вместо мат­ри­цы
    2×3 получится мат­ри­ца 3×2.
                                              Модуль вектора и единичные векторы  35


Модуль вектора и единичные векторы
Модуль вектора, также именуемый геометрической длиной, или нормой1, пред-
ставляет собой расстояние от хвоста до головы вектора и вычисляется с ис-
пользованием стандартной формулы евклидова расстояния: квадратный
корень из суммы квадратов элементов вектора (см. уравнение 2.7). Модуль
вектора указывается с помощью двойных вертикальных полос вокруг век-
тора: ||v||.

    Уравнение 2.7. Норма вектора




  В некоторых приложениях используются квадраты модулей векторов (за-
писываемые как ||v||2), и в этом случае член квадратного корня в правой
части выпадает.
  Прежде чем показать исходный код Python, следует объяснить еще не-
сколько терминологических разночтений между линейной алгеброй «на ме-
ловой доске» и линейной алгеброй на Python. В математике размерность
вектора – это число элементов в этом векторе, тогда как длина – это гео-
метрическое расстояние; на языке Python функция len() (где len – это со-
кращение от англ. length, длина) возвращает размерность массива, тогда как
функция np.norm() возвращает геометрическую длину (модуль вектора). Во
избежание путаницы в данной книге вместо термина длина я буду использо-
вать термин модуль вектора (либо геометрическая длина):
v = np.array([1, 2, 3, 7, 8, 9])
v_dim = len(v) # математическая размерность
v_mag = np.linalg.norm(v) # матем. модуль, длина или норма вектора

  Существуют приложения, в которых нужен вектор, геометрическая длина
которого равна единице, и такой вектор называется единичным вектором.
Примеры приложений включают ортогональные мат­ри­цы, мат­ри­цы пово-
рота, собственные векторы и сингулярные векторы.
  Единичный вектор определяется как ||v|| = 1.
  Излишне говорить, что многие векторы не являются единичными вектора-
ми. (У меня возникает соблазн написать «большинство векторов не являются
единичными векторами», но существует бесконечное число единичных век-
торов и неединичных векторов, хотя множество бесконечных неединичных
векторов больше, чем множество бесконечных единичных векторов.) К сча-
стью, любой неединичный вектор имеет ассоциированный с ним единичный
вектор. Это означает, что мы можем создать единичный вектор в том же
направлении, что и неединичный вектор. Создать ассоциированный еди-
ничный вектор несложно; надо просто умножить на скаляр модуля вектора,
взаимообратный его норме (уравнение 2.8):

1
    Англ. magnitude; син. величина вектора. – Прим. перев.
36  Векторы. Часть 1

    Уравнение 2.8. Создание единичного вектора




  На рис. 2.4 показано общепринятое правило обозначения единичных век-
торов (v̂) в том же направлении, что и их родительский вектор v. Данный
рисунок иллюстрирует эти случаи.




                      Рис. 2.4  Единичный вектор (серая стрелка)
                можно создать из неединичного вектора (черная стрелка);
                 оба вектора имеют одинаковый угол, но разные модули

  На самом деле утверждение о том, что «любой неединичный вектор имеет
ассоциированный единичный вектор», не совсем верно. Существует вектор,
который имеет неединичную длину и в то же время не имеет ассоциирован-
ного единичного вектора. Сможете ли вы угадать, какой это вектор1?
  Здесь я не показываю исходный код Python создания единичных векторов,
потому что это будет одним из упражнений в конце данной главы.



Точечное произведение векторов
Точечное произведение (также иногда именуемое внутренним произведением)
является одной из наиболее важных операций во всей линейной алгебре. Это
базовый вычислительный строительный блок, на основе которого строятся
многие операции и алгоритмы, включая свертку, корреляцию, результаты
преобразования Фурье, матричное умножение, извлечение линейных при-
знаков, фильтрацию сигналов и т. д.
  Точечное произведение между двумя векторами можно указать несколь-
кими способами. Я буду использовать в основном общепринятую нотацию
aTb по причинам, которые станут ясны после изучения матричного умноже-
ния. В других контекстах вы могли бы увидеть a · b или �a, b�.

1
    Вектор нулей имеет длину 0, но не ассоциирован с единичным вектором, поскольку
    у него нет направления и поскольку невозможно прошкалировать вектор нулей
    так, чтобы он имел ненулевую длину.
                                                   Точечное произведение векторов  37

  Точечное произведение – это одно число, которое предоставляет инфор-
мацию о взаимосвязи между двумя векторами. Давайте сначала сосредото-
чимся на алгоритме вычисления точечного произведения, а затем я расска-
жу, как его интерпретировать.
  Для вычисления точечного произведения надо перемножить соответству-
ющие элементы двух векторов, а затем просуммировать все отдельные про-
изведения. Другими словами, это поэлементное умножение и сумма. В урав-
нении 2.9 a и b являются векторами, а ai указывает на i-й элемент вектора a.

  Уравнение 2.9. Формула точечного произведения




  По формуле можно заключить, что точечное произведение допустимо
только между двумя векторами одинаковой размерности. Уравнение 2.10
показывает численный пример:

  Уравнение 2.10. Пример вычисления точечного произведения

  [1 2 3 4] · [5 6 7 8] = 1×5 + 2×6 + 3×7 + 4×8
                         = 5 + 12 + 21 + 32
                         = 70.

     Расстройства от индексации
     Стандартная математическая система счисления и некоторые математикоориентирован-
     ные программы обработки чисел, такие как MATLAB и Julia, начинают индексацию с 1
     и останавливаются на N, тогда как некоторые языки программирования, такие как Python
     и Java, начинают индексацию с 0 и останавливаются на N – 1. Нам незачем обсуждать до-
     стоинства и ограничения каждого соглашения, хотя иногда я все-таки задумываюсь обо
     всех тех ошибках, которое данное несоответствие внесло в человеческую цивилизацию,
     но об этом различии важно помнить при переложении формул в исходный код Python.

  Есть несколько способов реализации точечного произведения на Python;
самый элементарный состоит в использовании функции np.dot():
v = np.array([1, 2, 3, 4])
w = np.array([5, 6, 7, 8])
np.dot(v,w)

     Примечание к функции np.dot()
     На самом деле точечное произведение векторов в функции np.dot() не реализовано;
     в ней реализовано умножение мат­риц, которое представляет собой набор точечных
     произведений. Это будет иметь больше смысла после ознакомления с правилами и ме-
     ханизмами умножения мат­риц (глава 5). Если вы хотите обследовать их сейчас, то мо-
     жете изменить приведенный выше исходный код, чтобы придать обоим векторам ори-
     ентацию (вдоль строки в противовес ориентации вдоль столбца). И вы обнаружите, что
     результат будет точечным произведением только тогда, когда первый аргумент на вхо-
     де в функцию представляет собой вектор-строку, а второй аргумент – вектор-столбец.
38  Векторы. Часть 1

  Вот интересное свойство точечного произведения: умножение одного век-
тора на скаляр шкалирует точечное произведение на то же число. Указанное
свойство можно обследовать, расширив приведенный ранее исходный код:
s = 10
np.dot(s*v, w)

   Точечное произведение векторов v и w равно 70, а точечное произведение
с использованием s*v (которое в математических обозначениях было бы за-
писано как σvTw) равно 700. Теперь попробуйте этот пример с отрицатель-
ным скаляром, например s = –1. И вы увидите, что величина точечного про-
изведения сохраняется, но знак меняется на противоположный. Разумеется,
при s = 0 точечное произведение равно нулю.
   Теперь вы знаете, как вычислять точечное произведение. Каков смысл
точечного произведения и как его интерпретировать?
   Точечное произведение можно интерпретировать как меру сходства, или
соотнесенности, между двумя векторами. Представьте, что вы собрали дан-
ные о росте и весе 20 человек и сохранили эти данные в двух векторах. Вы,
конечно же, ожидаете, что эти переменные будут друг с другом связаны (бо-
лее высокие люди, как правило, весят больше), и поэтому можно ожидать,
что точечное произведение между этими двумя векторами будет большим.
С другой стороны, величина точечного произведения зависит от шкалы дан-
ных, и, стало быть, точечное произведение между данными, измеренными
в граммах и сантиметрах, будет больше, чем точечное произведение между
данными, измеренными в фунтах и футах. Однако указанное произвольное
шкалирование можно устранить с помощью коэффициента нормализации.
Нормализованное точечное произведение между двумя переменными на
самом деле называется коэффициентом корреляции Пирсона, и оно является
одним из наиболее важных методов анализа в науке о данных. Подробнее об
этом будет в главе 4!


Точечное произведение является
дистрибутивным
Дистрибутивное свойство математики выглядит так: a(b + c) = ab + ac. В пере-
ложении на векторы и точечное произведение векторов это означает, что:

  aT(b + c) = aTb + aTc.

  На словах вы бы сказали, что точечное произведение суммы векторов
равно сумме точечных произведений векторов.
  Следующий ниже исходный код Python иллюстрирует свойство дистрибу-
тивности:
a = np.array([ 0, 1, 2 ])
b = np.array([ 3, 5, 8 ])
                                                     Точечное произведение векторов  39

c = np.array([ 13,21,34 ])

# точечное произведение дистрибутивно
res1 = np.dot( a, b+c )
res2 = np.dot( a,b ) + np.dot( a,c )

  Два результата res1 и res2 одинаковы (с этими векторами ответ равен 110),
иллюстрируя дистрибутивность точечного произведения. Обратите внима-
ние, как математическая формула переводится в исходный код на языке
Python; в математикоориентированном программировании переложение
формул в исходный код является важным навыком.


Геометрия точечного произведения
Существует также геометрическое определение точечного произведения,
которое представляет собой произведение модулей двух векторов, шкали-
рованных на косинус угла между ними (уравнение 2.11).

  Уравнение 2.11. Геометрическое определение точечного произведения
  векторов

  α = cos(θv,w)||v|| ||w||.

  Уравнение 2.9 и уравнение 2.11 математически эквивалентны, но выра-
жены в разных формах. Доказательство их эквивалентности является инте-
ресным упражнением в математическом анализе, но займет около страницы
текста и основывается сперва на доказательстве других принципов, включая
закон косинусов. Указанное доказательство не имеет отношения к данной
книге и поэтому опущено.
  Обратите внимание, что модули векторов являются строго положительны-
ми числами (за исключением вектора нулей, который имеет ||0|| = 0), тогда
как косинус угла может находиться в диапазоне от –1 до +1. Это означает,
что знак точечного произведения полностью определяется геометрической
взаимосвязью между двумя векторами. На рис. 2.5 показаны пять случаев
знака точечного произведения в зависимости от угла между двумя вектора-
ми (в 2D-визуализации, но данный принцип соблюдается для более высоких
измерений).

             Острый           Тупой     Ортогональный Коллинеарный Коллинеарный




             Рис. 2.5  Знак точечного произведения между двумя векторами
             показывает геометрическую взаимосвязь между этими векторами
40  Векторы. Часть 1

      Запомните: ортогональные векторы имеют нулевое точечное произведение
      Некоторые учителя математики настаивают на том, что вы не должны запоминать
      формулы и члены, а вместо этого должны понимать процедуры и доказательства. Но
      давайте будем честны: запоминание – важная и неизбежная часть изучения мате-
      матики. К счастью, линейная алгебра не требует чрезмерного запоминания, но есть
      несколько вещей, которые вам просто нужно запомнить.
      Вот одна из них: ортогональные векторы имеют точечное произведение, равное нулю
      (это утверждение соблюдается в обоих направлениях – когда точечное произведение
      равно нулю, то два вектора ортогональны). Таким образом, следующие утверждения
      являются эквивалентными: два вектора ортогональны; два вектора имеют точечное
      произведение, равное нулю; два вектора пересекаются под углом 90°. Повторяйте эту
      эквивалентность до тех пор, пока она не отпечатается в вашем мозгу навечно.



Другие умножения векторов
Точечное произведение, пожалуй, является наиболее важным и наиболее
часто используемым способом умножения векторов. Но есть и несколько
других способов умножения векторов.


Адамарово умножение
Это просто причудливый термин для поэлементного умножения. Для реа-
лизации адамарова умножения надо перемножить каждый соответствующий
элемент в двух векторах. Произведение представляет собой вектор той же
размерности, что и у двух сомножителей. Например:




  В Python звездочка указывает на поэлементное умножение двух векторов
или мат­риц:
a = np.array([5, 4, 8, 2])
b = np.array([1, 0,.5])
a*b

   Попробуйте выполнить приведенный выше исходный код Python и... о-хо-
хо! Python выдаст сообщение об ошибке. Отыщите и исправьте ошибку. Что
вы узнали об адамаровом умножении из этой ошибки? Обратитесь к сноске
с ответом1.

1
    Ошибка в том, что два вектора имеют разные размерности, указывая на то, что
    адамарово умножение определено только для двух векторов одинаковой размер-
    ности. Проблема устраняется за счет удаления одного числа из a либо добавления
    одного числа в b.
                                                     Другие умножения векторов  41

  Адамарово умножение является удобным способом организовывать мно-
гочисленные умножения на скаляр. Например, представьте, что у вас есть
данные о числе виджетов, продаваемых в разных магазинах, и цене за вид-
жет в каждом магазине. Вы могли бы представить каждую переменную в виде
вектора, а затем перемножать эти векторы по Адамару, чтобы вычислять
доход виджета по каждому магазину (он отличается от суммарного дохода по
всем магазинам, который будет вычислен как точечное произведение).


Внешнее произведение
Внешнее произведение – это способ создания мат­ри­цы из вектора-столбца
и вектора-строки. Каждая строка в мат­ри­це внешнего произведения пред-
ставляет собой скаляр вектора-строки, умноженный на соответствующий
элемент в векторе-столбце. Можно было бы также сказать, что каждый стол-
бец в мат­ри­це произведения является скаляром вектора-столбца, умно-
женным на соответствующий элемент в векторе-строке. В главе 6 я назову
эту мат­ри­цу «матрицей ранга 1», а пока об этом термине не беспокойтесь
и вместо этого сосредоточьтесь на закономерности, проиллюстрированной
в следующем ниже примере:




                     Использование букв в линейной алгебре
 На уроках алгебры в средней школе вы узнали, что использование букв в качестве абст­
 рактных местозаполнителей вместо чисел позволяет понимать математику на более
 глубоком уровне, чем арифметика. Та же концепция используется в линейной алгебре:
 иногда, когда это облегчает понимание, внутри мат­риц учителя используют буквы вместо
 чисел. Буквы можно рассматривать как переменные.


  Внешнее произведение сильно отличается от точечного произведения:
вместо скаляра оно производит мат­ри­цу, а два вектора во внешнем произ-
ведении могут иметь разные размерности, тогда как два вектора в точечном
произведении должны иметь одинаковую размерность.
  Внешнее произведение обозначается как vwT (вспомните, что мы исходим
из того, что векторы ориентированы вдоль столбца; следовательно, внешнее
произведение предусматривает умножение столбца на строку). Обратите
внимание на тонкое, но важное различие между обозначениями точечного
произведения (vTw) и внешнего произведения (vwT). Возможно, сейчас это
покажется странным и сбивающим с толку, но обещаю, это станет совершен-
но понятным после того, как вы узнаете о матричном произведении в главе 5.
  Внешнее произведение похоже на транслирование, но это не одно и то же:
транслирование – это общая операция программирования, которая исполь-
42  Векторы. Часть 1

зуется для расширения векторов в арифметических операциях, таких как
сложение, умножение и деление; внешнее произведение – это специфическая
математическая процедура умножения двух векторов.
  NumPy может вычислять внешнее произведение с помощью функции
np.outer() либо функции np.dot(), при условии что два входных вектора ори-
ентированы соответственно вдоль столбца и строки.


Перекрестное и тройное произведения
Существует несколько других способов умножения векторов, таких как пере-
крестное произведение и тройное произведение. Эти методы используются
в геометрии и физике, но не так часто встречаются в приложениях, связан-
ных с технологиями, чтобы тратить на них время в этой книге. Я упоминаю
их здесь только для того, чтобы вы были мимолетно знакомы с названиями.



Ортогональное разложение векторов
«Разложить» вектор или мат­ри­цу означает разбить эту мат­ри­цу на несколько
более элементарных частей. Разложение используется для выявления ин-
формации, которая «скрыта» в мат­ри­це, с целью облегчения работы с матри-
цей либо с целью сжатия данных. Не будет преуменьшением написать, что
бóльшая часть линейной алгебры (абстрактной и практической) предусмат­
ривает матричные разложения. Матричные разложения – серьезная штука.
  Ниже я представлю концепцию разложения, используя два простых при-
мера со скалярами:
    число 42.01 можно разложить на две части: 42 и .01. Возможно, .01 –
      это шум, который следует игнорировать, либо, вероятно, цель состоит
      в том, чтобы сжать данные (целое число 42 требует меньше памяти, чем
      42.01 с плавающей точкой1). Независимо от мотивации, разложение
      предусматривает представление одного математического объекта как
      суммы более элементарных объектов (42 = 42 + .01);
    число 42 можно разложить на произведение простых чисел 2, 3 и 7.
      Такое разложение называется разложением на простые множители
      и имеет много применений в числовой обработке и криптографии.
      В данном примере вместо сумм предусматривается использование
      произведений, но суть – та же самая: разложить один математический
      объект на более мелкие и более элементарные части.
  В этом разделе мы начнем обследовать элементарное, но важное разло-
жение, которое заключается в разбиении вектора на два отдельных вектора,


1
    В переводе оставлена форма записи чисел, принятая в оригинале книги, где для
    отделения дробной части принято использовать точку, а для отделения групп по
    три числа многозначного числа принято использовать запятую. – Прим. перев.
                                           Ортогональное разложение векторов  43

один из которых ортогонален опорному вектору, а другой параллелен этому
опорному вектору. Ортогональное разложение векторов непосредственно
приводит к процедуре Грама–Шмидта и QR-разложению, которое часто ис-
пользуется в решении обратных задач в статистике.
  Давайте начнем с рисунка, чтобы представить цель разложения визуально.
Рисунок 2.6 иллюстрирует ситуацию: даны два вектора a и b в стандартном
положении, и наша цель – найти точку на векторе a, которая находится как
можно ближе к голове вектора b. Указанную задачу также можно было бы вы-
разить как задачу оптимизации: спроецировать вектор b на вектор a таким
образом, чтобы проекционное расстояние было минимизировано. Разумеет-
ся, эта точка на a будет шкалированной версией вектора a; другими словами,
βa. Таким образом, теперь наша цель состоит в том, чтобы найти скаляр β.
(Связь с ортогональным разложением векторов вскоре станет ясна.)




            Рис. 2.6  Для того чтобы спроецировать точку в голове вектора b
  на вектор a с минимальным расстоянием, нужна формула вычисления β таким образом,
             чтобы длина проекционного вектора (b – βa) была минимальной

  Важно отметить, что для определения отрезка от b до βa можно использо-
вать вычитание векторов. Мы могли бы дать этому отрезку отдельную букву,
например вектор c, но для отыскания решения необходимо вычитание.
  Ключевой для понимания момент, который приводит к решению этой за-
дачи, заключается в том, что точка на a, ближайшая к голове b, отыскивается
путем проведения отрезка от b, которая пересекается с a под прямым углом.
Интуитивное понимание здесь достигается, если вообразить треугольник,
образованный началом координат, вершиной b и βa; длина отрезка от b до
βa становится больше по мере того, как угол ∡βa становится меньше 90° или
больше 90°.
  Собрав все это вместе, мы пришли к выводу, что (b – βa) является ортого-
нальным βa, а это то же самое, что сказать, что данные векторы перпендику-
лярны. И стало быть, точечное произведение между ними должно быть равно
нулю. Давайте трансформируем приведенные выше слова в уравнение:

  aT(b – βa) = 0.
44  Векторы. Часть 1

  Отсюда можно применить немного алгебры, чтобы отыскать β (обратите
внимание на применение дистрибутивного свойства точечных произведе-
ний), которое показано в уравнении 2.12:

  Уравнение 2.12. Решение задачи ортогональной проекции

  aTb – βaTa = 0;
        βaTa = aTb;




   Получилось довольно красиво: мы начали с простой геометрической
картинки, обследовали последствия геометрии, выразили эти последствия
в виде формулы, а затем применили немного алгебры. И в результате обна-
ружили формулу проецирования точки на отрезок с минимальным расстоя-
нием. Указанная проекция называется ортогональной проекцией и является
основой для многих приложений в статистике и машинном обучении, вклю-
чая знаменитую формулу наименьших квадратов для решения линейных
моделей (вы увидите ортогональные проекции в главах 9, 10 и 11).
   Могу себе представить, что вы весьма заинтригованы посмотреть, как
будет выглядеть исходный код Python с реализацией этой формулы. Но вам
придется написать этот код самостоятельно в упражнении 2.8 в конце данной
главы. Если вы не хотите ждать конца главы, то смело решайте это упражне-
ние сейчас, а затем продолжайте изучать ортогональное разложение.
   Возможно, вам интересно, как это связано с ортогональным разложением
векторов, то есть с названием данного раздела. Проекция с минимальным
расстоянием была необходимой подготовительной работой, и теперь вы го-
товы усвоить разложение.
   Как обычно, мы начнем с постановки и цели анализа. Вначале даны два
вектора, которые я назову «целевым вектором» и «опорным вектором». Цель
состоит в том, чтобы разложить целевой вектор на два других вектора таким
образом, чтобы:
   1) эти два вектора в сумме составляли целевой вектор и
   2)	один вектор был ортогонален опорному вектору, в то время как другой
       был параллелен опорному вектору.
   Указанная ситуация проиллюстрирована на рис. 2.7.
   Прежде чем приступать к математике, давайте проясним наши термины:
я обозначу целевой вектор через t и опорный вектор – через r. Далее два
вектора, сформированные из целевого вектора, будут называться перпенди-
кулярной компонентой, обозначенной через t�r, и параллельной компонентой,
обозначенной через t||r.
   Мы начнем с определения параллельной компоненты. Каким будет вектор,
параллельный вектору r? Дело в том, что любая шкалированная версия век-
тора r очевидным образом является параллельной вектору r. И поэтому мы
находим t||r, просто применяя формулу ортогональной проекции, которую
мы только что обнаружили (уравнение 2.13):
                                             Ортогональное разложение векторов  45




               Рис. 2.7  Иллюстрация ортогонального разложения векторов:
                      разложить вектор t на сумму двух других векторов,
                       которые ортогональны и параллельны вектору r


  Уравнение 2.13. Вычисление параллельной компоненты вектора t
  относительно вектора r




  Обратите внимание на тонкое отличие от уравнения 2.12: там мы вычисля-
ли только скаляр β; здесь же мы хотим вычислить шкалированный вектор βr.
  Это и есть параллельная компонента. Как найти перпендикулярную ком-
поненту? Ее найти проще, потому что мы уже знаем, что две компоненты
вектора должны в сумме составлять изначальный целевой вектор. Таким
образом:

    t = t�r + t||r;
  t�r = t – t||r.

  Другими словами, мы вычитаем параллельную компоненту из изначаль-
ного вектора, и остатком будет наша перпендикулярная компонента.
  Но действительно ли эта перпендикулярная компонента ортогональна
опорному вектору? Да, это так! В целях доказательства вы показываете, что
точечное произведение между перпендикулярной компонентой и опорным
вектором равно нулю:

         (t�r)Tr = 0;




  Работа с алгеброй указанного доказательства прямолинейна, но утоми-
тельна, поэтому я ее опустил. Вместо этого вы будете работать над укрепле-
нием интуитивного понимания в упражнениях, используя исходный код
Python.
  Надеюсь, вам понравилось изучать ортогональное разложение векторов.
Еще раз обратите внимание на общий принцип: мы разбиваем один матема-
тический объект на комбинацию других объектов. Детали разложения зави-
сят от наших ограничений (в данном случае ортогонального и параллельного
46  Векторы. Часть 1

опорному вектору), означая, что разные ограничения (то есть разные цели
анализа) могут приводить к разным разложениям одного и того же вектора.



Резюме
Прелесть линейной алгебры заключается в том, что даже самые сложные
и вычислительно емкие операции на матрицах состоят из элементарных
операций, большинство из которых можно понять на уровне интуиции с по-
мощью геометрии. Не стоит недооценивать важность изучения элементар-
ных операций на векторах, потому что то, что вы узнали в этой главе, ляжет
в основу остальной части книги – и остальной части вашей карьеры приклад-
ного линейного алгебраиста (кем вы на самом деле и являетесь, если занимае­
тесь наукой о данных, машинным обучением, искусственным интеллектом,
глубоким обучением, обработкой изображений, компьютерным зрением,
статистикой и всем таким прочим).
   Вот наиболее важные выводы, которые следует вынести из этой главы.
     Вектор – это упорядоченный список чисел, который помещается в стол-
       бец либо строку. Число элементов в векторе называется его размерно-
       стью, и в геометрическом пространстве вектор можно представить
       в виде отрезка с числом осей, равным размерности.
     Несколько арифметических операций (сложение, вычитание и адама-
       рово умножение) на векторах выполняется поэлементно.
     Точечное произведение – это одно число, в котором кодируется взаи-
       мосвязь между двумя векторами одинаковой размерности и которое
       вычисляется как поэлементное умножение и сумма.
     Точечное произведение равно нулю для ортогональных векторов, что
       геометрически означает, что векторы пересекаются под прямым углом.
     Ортогональное разложение векторов предусматривает разбиение век-
       тора на сумму двух других векторов, которые ортогональны и парал-
       лельны опорному вектору. Формулу данного разложения можно снова
       извлечь из геометрии, но вы должны помнить фразу «отображение над
       модулем» как концепцию, выражаемую этой формулой.



Упражнения по программированию
Надеюсь, вы не воспринимаете эти упражнения как утомительную работу, ко-
торую вам нужно проделать. Как раз наоборот, данные упражнения дают воз-
можность отточить ваши математические навыки и навыки программирова-
ния, а также убедиться, что вы действительно понимаете материал этой главы.
  Хотелось бы также, чтобы вы рассматривали эти упражнения как трам-
плин, позволяющий вам продолжить обследовать линейную алгебру с ис-
пользованием Python. Вносите изменения в исходный код, используя разные
числа, разные размерности, разные ориентации и т. д. Пишите свой исход-
                                        Упражнения по программированию  47

ный код тестирования других упомянутых в этой главе концепций. Самое
главное: получайте удовольствие и учитесь использовать учебный опыт.
  В качестве напоминания: решения ко всем упражнениям можно просмот­
реть либо скачать по ссылке https://github.com/mikexcohen/LA4DataScience.

Упражнение 2.1
В онлайновом репозитории исходного кода «отсутствует» код создания
рис. 2.2. (На самом деле он не отсутствует – я перенес его в решение данного
упражнения.) Итак, здесь ваша цель – написать свой исходный код для соз-
дания рис. 2.2.

Упражнение 2.2
Напишите алгоритм, который вычисляет норму вектора путем переложения
уравнения 2.7 в исходный код. Подтвердите, используя случайные векторы
с разными размерностями и ориентациями, что вы получаете тот же резуль-
тат, что и np.linalg.norm(). Данное упражнение предназначено для того, чтобы
дать вам больше опыта в индексации массивов NumPy и переложении формул
в исходный код; на практике нередко проще использовать np.linalg.norm().

Упражнение 2.3
Создайте функцию Python, которая на входе будет принимать вектор, а на
выходе – выдавать единичный вектор в том же направлении. Что происходит
при вводе вектора нулей?

Упражнение 2.4
Вы знаете, как создавать единичные векторы; что, если вы хотите создавать
вектор любого произвольного модуля? Напишите функцию Python, которая
на входе будет принимать вектор и желаемый модуль вектора и возвращать
вектор в том же направлении, но с модулем, соответствующим второму вход-
ному аргументу.

Упражнение 2.5
Напишите цикл for для транспонирования вектора-строки в вектор-столбец
без использования встроенной функции или метода, такого как np.transpose()
или v.T. Данное упражнение поможет вам создавать и индексировать векто-
ры с наделенной ориентацией.

Упражнение 2.6
Вот интересный факт: квадратная норма вектора может вычисляться как
точечное произведение этого вектора на себя. Вернитесь к уравнению 2.8,
чтобы убедиться в данной эквивалентности. Затем подтвердите ее с помо-
щью Python.

Упражнение 2.7
Напишите исходный код, чтобы продемонстрировать, что точечное произ-
ведение является коммутативным. Коммутативное свойство означает a × b =
b × a, что для точечного произведения векторов означает aTb = bTa. Проде-
48  Векторы. Часть 1

монстрировав это в исходном коде, примените уравнение 2.9, чтобы понять,
почему точечное произведение является коммутативным.

Упражнение 2.8
Напишите исходный код создания рис. 2.6. (Обратите внимание, что если
присутствуют ключевые элементы, то ваше решение не обязательно должно
выглядеть в точности так, как показано на рисунке.)

Упражнение 2.9
Реализуйте ортогональное разложение векторов. Начните с двух векторов
случайных чисел t и r и воспроизведите рис. 2.8 (обратите внимание, что
ваш график будет выглядеть несколько иначе из-за случайных чисел). Затем
подтвердите, что сумма двух компонент равна t и что t�r и t||r ортогональны.




                           Рис. 2.8  Упражнение 9


Упражнение 2.10
Важным навыком программирования является отыскание ошибок. Допус­
тим, в вашем исходном коде есть дефект, из-за которого знаменатель в про-
екционном скаляре уравнения 2.13 равен tTt вместо rTr (исходя из личного
опыта написания этой главы, легко допустить ошибку!). Внедрите эту ошиб-
ку, чтобы проверить, действительно ли она уводит в сторону от точного ис-
ходного кода. Что можно сделать, чтобы проверить, является ли результат
правильным либо неправильным? (В программировании подтверждение
исходного кода известными результатами называется проверкой на исправ-
ность.)
Глава         3
                           Векторы. Часть 2

Предыдущая глава заложила основу для понимания векторов и базовых
операций на векторах. Теперь вы расширите горизонты своих знаний в об-
ласти линейной алгебры, познакомившись с набором взаимосвязанных
понятий, включая линейную независимость, подпространства и базисы.
Каждая из этих тем имеет решающее значение для понимания операций
на матрицах.
  Возможно, некоторые из представленных здесь тем покажутся абстракт-
ными и оторванными от приложений, но между ними существует очень ко-
роткий путь, например векторные подпространства и подгонка статисти-
ческих моделей к данным. Приложения в области науки о данных появятся
позже, поэтому, пожалуйста, продолжайте концентрировать свое внимание
на основах, чтобы продвинутые темы было легче понять.



Множества векторов
Мы начнем главу с чего-нибудь простого: коллекция векторов называет-
ся множеством. Вообразите, как вы кладете себе в рюкзак пучок векторов,
чтобы носить его с собой. Векторные множества обозначаются курсивом
и заглавными буквами, такими как S или V. Математические множества опи-
сываются следующим образом:

  V = {v1, …, vn}.

  Представьте, например, набор данных о числе случаев заражения Кови-
дом-19, госпитализациях и смертях из ста стран; эти данные из каждой стра-
ны можно было бы сохранить в трехэлементном векторе и создать множество
векторов, содержащее сто векторов.
  Множество векторов может содержать конечное или бесконечное число
векторов. Возможно, множества векторов с бесконечным числом векторов
будет звучать как бесполезно глупая абстракция, но векторные подпростран-
ства – это бесконечные множества векторов, и они имеют большое значение
для подгонки статистических моделей к данным.
50  Векторы. Часть 2

   Множества векторов также могут быть пустыми и обозначаются как V = {}.
Вы столкнетесь с пустыми множествами векторов, когда познакомитесь
с пространствами мат­риц.



Линейно-взвешенная комбинация
Линейно-взвешенная комбинация – это способ смешивания информации из
нескольких переменных, при этом некоторые переменные вносят больший
вклад, чем другие. Указанную фундаментальную операцию также иногда
называют линейной смесью, или взвешенной комбинацией (часть со словом
линейная подразумевается). Иногда вместо слова вес используется термин
«коэффициент».
  Линейно-взвешенная комбинация просто означает умножение векторов на
скаляр и сложение: взять некоторое множество векторов, умножить каждый
вектор на скаляр и сложить их, чтобы получить один вектор (уравнение 3.1).

    Уравнение 3.1. Линейно-взвешенная комбинация

    w = λ1v 1 + λ2v 2 + … + λnv n.

   Подразумевается, что все векторы vi имеют одинаковую размерность;
в противном случае сложение недопустимо. Скаляры λ могут быть любым
действительно-значным числом, включая ноль.
   Технически уравнение 3.1 можно было бы переписать для вычитания век-
торов, но поскольку можно вычитать, задавая отрицательное значение числа
λi, проще обсуждать линейно-взвешенные комбинации в терминах сумми-
рования.
   Уравнение 3.2 показывает пример, помогающий сделать все это конкретнее:

    Уравнение 3.2. Линейно-взвешенная комбинация




  Линейно-взвешенные комбинации реализуются в исходном коде очень
просто, как показано ниже. В Python важен тип данных; проверьте, что про-
изойдет, когда векторы будут списками, а не массивами NumPy1:

1
     Как показано в главах 2 и 16, умножение списка на целое число повторяет список
    заданное число раз вместо его умножения на скаляр.
                                               Линейная независимость  51

l1 = 1
l2 = 2
l3 = -3
v1 = np.array([ 4, 5, 1])
v2 = np.array([-4, 0,-4])
v3 = np.array([ 1, 3, 2])
l1*v1 + l2*v2 + l3*v3

   Организовывать хранение каждого вектора и каждого коэффициента
в виде отдельных переменных утомительно, и такой подход не масштаби-
руется под решение более крупных задач. Поэтому на практике линейно-
взвешенные комбинации реализуются с помощью компактного и масшта-
бируемого метода умножения мат­риц на векторы, о котором вы узнаете
в главе 5; пока же основное внимание будет сконцентрировано на концепции
и реализации в исходном коде.
   Линейно-взвешенные комбинации имеют несколько применений. Три из
них таковы:
     предсказанные данные на выходе из статистической модели создают-
       ся путем применения линейно-взвешенной комбинации регрессоров
       (предсказательных переменных) и коэффициентов (скаляров), кото-
       рые вычисляются с помощью алгоритма наименьших квадратов, о ко-
       тором вы узнаете в главах 11 и 12;
     в процедурах уменьшения размерности, таких как анализ главных ком-
       понент, каждая компонента (иногда именуемая фактором, или модой)
       извлекается как линейно-взвешенная комбинация каналов данных,
       при этом веса (коэффициенты) отбираются таким образом, чтобы мак-
       симизировать дисперсию компоненты (наряду с некоторыми другими
       ограничениями, о которых вы узнаете в главе 15);
     искусственные нейронные сети (архитектура и алгоритм, приводящие
       в действие технологию глубокого обучения) предусматривают две опе-
       рации: линейно-взвешенную комбинацию входных данных с последу-
       ющим нелинейным преобразованием. Веса усваиваются алгоритмом
       путем минимизации стоимостной функции, которая обычно пред-
       ставляет собой разницу между модельным предсказанием и реальной
       целевой переменной.
   Концепция линейно-взвешенной комбинации является механизмом соз-
дания подпространств векторов и пространств мат­риц и занимает централь-
ное место в линейной независимости. И действительно, линейно-взвешенная
комбинация и точечное произведение являются двумя наиболее важными
элементарными строительными блоками, из которых строятся многие про-
двинутые линейно-алгебраические вычисления.



Линейная независимость
Множество векторов является линейно зависимым, если по меньшей мере
один вектор в множестве можно выразить как линейно-взвешенную ком-
52  Векторы. Часть 2

бинацию других векторов в этом множестве. И следовательно, множество
векторов является линейно независимым, если ни один вектор невозможно
выразить как линейно-взвешенную комбинацию других векторов в этом
множестве.
  Ниже приведены два множества векторов. Прежде чем читать текст, по-
пробуйте определить, является ли каждое множество зависимым либо не-
зависимым. (Когда слово линейная подразумевается, термин линейная неза-
висимость иногда сокращается до одного слова независимость.)




  Векторное множество V является линейно независимым: невозможно
выразить один вектор в множестве как линейное кратное другого вектора
в множестве. То есть если обозначить векторы в множестве через v1 и v2, то
не существует возможного скаляра λ, для которого v1 = λv2.
  Что скажете насчет множества S? Оно является зависимым, потому что мы
можем применить линейно-взвешенные комбинации некоторых векторов
в множестве и получить другие векторы в множестве. Существует бесконеч-
ное число таких комбинаций, две из которых – s1 = .5*s*2 и s2 = 2*s*1.
  Давайте попробуем еще один пример. Опять же, вопрос заключается
в том, является ли множество T линейно независимым либо линейно за-
висимым:




  А вот тут разобраться намного сложнее, чем в двух предыдущих приме-
рах. Оказывается, что это линейно зависимое множество (например, сумма
первых трех векторов равна удвоенному четвертому вектору). Но я и не жду,
что вы сможете понять это в результате визуального осмотра.
  Так как же определять линейную независимость на практике? Линейная
независимость определяется путем создания мат­ри­цы из множества векто-
ров, вычисления ранга мат­ри­цы и сравнения ранга с наименьшим числом
из числа строк или столбцов. Возможно, это предложение сейчас не будет
иметь для вас смысла, потому что вы еще не познакомились с рангом мат­
ри­цы. Поэтому сейчас сосредоточьте свое внимание на концепции, что мно-
жество векторов является линейно зависимым, если по меньшей мере один
вектор в указанном множестве можно выразить как линейно-взвешенную
комбинацию других векторов в данном множестве, и множество векторов
является линейно независимым, если ни один вектор невозможно выразить
как комбинацию других векторов.
                                                         Линейная независимость  53


                               Независимые множества
    Независимость – это свойство множества векторов. То есть множество векторов может
    быть линейно независимым либо линейно зависимым; независимость не является свой-
    ством отдельного вектора внутри множества.




Математика линейной независимости
Теперь, когда вы понимаете эту концепцию, я хочу убедиться, что вы также
понимаете формальное математическое определение линейной зависимо-
сти, которое выражено в уравнении 3.3.

    Уравнение 3.3. Линейная зависимость1

    0 = λ1v1 + λ2v2 + … + λnvn, λ Î ℝ.

   Это уравнение говорит, что линейная зависимость означает существова-
ние возможности определить некоторую линейно-взвешенную комбинацию
векторов в множестве, такую чтобы получить вектор нулей. Если есть воз-
можность найти несколько скаляров λ, которые делают уравнение истинным,
то множество векторов является линейно зависимым. И наоборот, если нет
возможного способа линейно скомбинировать векторы так, чтобы получить
векторы нулей, то множество является линейно независимым.
   Вероятно, поначалу это покажется не поддающимся интуитивному по-
ниманию. Почему нас волнует вектор нулей, когда рассматривается вопрос
о возможности выразить хотя бы один вектор в множестве через взвешенную
комбинацию других векторов в этом множестве? Вероятно, вы бы предпочли
переписать определение линейной зависимости следующим образом:

    λ1v1 = λ2v2 + … + λnvn, λ Î ℝ.

  Почему бы не начать именно с этого уравнения, а не помещать вектор
нулей в левую часть? Приравнивание уравнения к нулю помогает укрепить
принцип, согласно которому все множество целиком является зависимым
либо независимым; ни один отдельный вектор не имеет привилегирован-
ного положения в качестве «зависимого вектора» (см. «Независимые мно-
жества» на данной странице). Другими словами, когда речь заходит о неза-
висимости, множества векторов становятся исключительно эгалитарными.
  Но погодите. Тщательный анализ уравнения 3.3 показывает тривиальное
решение: приравнять все скаляры λ к нулю, и уравнение будет выглядеть
как 0 = 0, независимо от векторов в множестве. Однако, как я написал в гла-
ве 2, в линейной алгебре содержащие нули тривиальные решения нередко
игнорируются. Таким образом, мы добавляем ограничение, что по меньшей
мере один λ ≠ 0.

1
    Это уравнение представляет собой применение линейно-взвешенной комбинации!
54  Векторы. Часть 2

  Это ограничение можно встроить в уравнение путем деления на один из
скаляров; имейте в виду, что v1 и λ1 могут относиться к любой векторно-ска-
лярной паре в множестве:

                           λ Î ℝ, λ1 ≠ 0.



Независимость и вектор нулей
Говоря по-простому, любое множество векторов, включающее вектор нулей,
автоматически является линейно зависимым множеством. И вот почему:
любое скалярное кратное вектору нулей по-прежнему остается вектором
нулей, поэтому определение линейной зависимости всегда соблюдается. Это
можно увидеть в следующем ниже уравнении:

  λ00 = 0v1 + 0v2 + 0vn.

  До тех пор, пока λ0 ≠ 0, мы имеем нетривиальное решение, и множество
соответствует определению линейной зависимости.

    Как насчет нелинейной независимости?
    – Но, Майк, – я представляю, как вы протестуете, – разве жизнь, Вселенная и все
    остальное не нелинейны?
    Полагаю, было бы интересным упражнением подсчитать суммарное число линейных
    взаимодействий во Вселенной относительно числа нелинейных и посмотреть, какая
    сумма больше. Но линейная алгебра всецело касается, ну, вы поняли, линейных опера-
    ций. Если один вектор можно выразить как нелинейную (а не линейную) комбинацию
    других векторов, то эти векторы все равно будут формировать линейно независимое
    множество. Причина ограничения линейности заключается в том, что мы хотим выра-
    жать преобразования как умножение мат­риц, которое является линейной операцией.
    Это говорится не для того, чтобы бросить тень на нелинейные операции – в моей во-
    ображаемой беседе вы красноречиво заявили, что чисто линейная Вселенная была
    бы довольно скучной и предсказуемой. Но нам вовсе не нужно объяснять всю Все-
    ленную с помощью линейной алгебры; линейная алгебра нам нужна только для ли-
    нейных частей. (Тут также стоит упомянуть, что многие нелинейные системы хорошо
    аппроксимируются с помощью линейных функций.)




Подпространство и охват
Когда я вводил понятие линейно-взвешенных комбинаций, я приводил при-
меры с конкретными числовыми значениями весов (например, λ1 = 1, λ3 =
–3). Подпространство – это та же идея, но с использованием бесконечности
возможных способов линейного комбинирования векторов в множестве.
  То есть для некоторого (конечного) множества векторов бесконечное чис-
ло способов их линейного комбинирования – с использованием одних и тех
                                                       Подпространство и охват  55

же векторов, но разных числовых значений весов – создает подпространство
векторов. А механизм комбинирования всех возможных линейно-взвешен-
ных комбинаций называется охватом множества векторов1. Давайте разбе-
рем несколько примеров. Мы начнем с элементарного примера множества
векторов, содержащего один вектор:




  Охватом данного множества векторов является бесконечность векторов,
которые могут быть созданы как линейные комбинации векторов в мно-
жестве. Для множества с одним вектором это просто означает все возмож-
ные шкалированные версии данного вектора. На рис. 3.1 показаны вектор
и подпространство, которое он охватывает. Учтите, что любой вектор в серой
пунктирной линии можно сформировать как некую шкалированную версию
вектора.




                 Рис. 3.1  Вектор (черным цветом) и подпространство,
                          которое он охватывает (серым цветом)

    Нашим следующим примером является множество из двух векторов в ℝ3:




  Векторы находятся в ℝ3, поэтому они графически представлены на трех-
мерной оси. Но подпространство, которое они охватывают, представляет со-
бой двумерную плоскость в этом трехмерном пространстве (рис. 3.2). Указан-
ная плоскость проходит через начало координат, потому что шкалирование
обоих векторов на ноль дает вектор нулей.



1
    Также называется линейной оболочкой (англ. linear hull), однако используемый
    в книге термин охват (англ. span) является более релевантным по причинам, ко-
    торые будут ясны далее в этой главе. – Прим. перев.
56  Векторы. Часть 2




             Рис. 3.2  Два вектора (черным цветом) и подпространство,
                       которое они охватывают (серым цветом)

  В первом примере был один вектор, и его охватом было одномерное под-
пространство, а во втором примере было два вектора, и их охватом было
двумерное подпространство. Кажется, вырисовывается некая закономер-
ность, но внешность бывает обманчивой. Рассмотрим следующий пример:




  Два вектора в ℝ3, но подпространство, которое они охватывают, по-
прежнему является всего лишь одномерным подпространством – отрезком
(рис. 3.3). Почему так? А все потому, что один вектор в множестве уже на-
ходится в зоне охвата другим вектором. И поэтому, с точки зрения охвата,
один из двух векторов является избыточным.




               Рис. 3.3  Одномерное подпространство (серым цветом),
                    охватываемое двумя векторами (черным цветом)

  Так какова же взаимосвязь между размерностью охватываемого подпро-
странства и числом векторов в множестве? Вероятно, вы уже догадались, что
это как-то связано с линейной независимостью.
                                                                               Базис  57

   Размерность подпространства, охватываемого множеством векторов, – это
наименьшее число векторов, образующих линейно независимое множество.
Если множество векторов является линейно независимым, то размерность
подпространства, охватываемого векторами в этом множестве, равна числу
векторов в этом множестве. Если множество является зависимым, то размер-
ность подпространства, охватываемого этими векторами, с необходимостью
меньше числа векторов в этом множестве. Насколько именно меньше, это
другой вопрос – для того чтобы знать взаимосвязь между числом векторов
в множестве и размерностью охватываемого ими подпространства, вам нуж-
но понимать ранг мат­ри­цы, о котором вы узнаете в главе 6.
   Формальное определение подпространства векторов таково: это подмно-
жество, которое замкнуто при сложении и умножении на скаляр и включает
начало пространства1. Это означает, что любая линейно-взвешенная комби-
нация векторов в подпространстве также должна находиться в одном и том
же подпространстве, включая установку всех весов равными нулю, чтобы
произвести вектор нулей в начале пространства.
   Пожалуйста, не теряйте сон, размышляя о том, что значит быть «замкну-
тым при сложении и умножении на скаляр»; просто запомните, что подпро-
странство векторов создается из всех возможных линейных комбинаций
множества векторов.


                 В чем разница между подпространством и охватом?
    Многих студентов смущает разница между охватом и подпространством. И это понятно,
    потому что данные понятия тесно связаны и нередко относятся к одному и тому же. Я объ-
    ясню разницу между ними, но не придавайте значения тонкостям – охват и подпростран-
    ство так часто относятся к идентичным математическим объектам, что использовать эти
    термины взаимозаменяемо, как правило, совершенно правильно.
    Я нахожу, что термин охват лучше использовать в глагольной форме, а термин подпро-
    странство – разумеется, как существительное, и это помогает понять их различие: мно­
    жест­во векторов охватывает, и результатом охвата является подпространство. Теперь
    учти­те, что подпространство может быть меньшей частью большего пространства, как вы
    увидели на рис. 3.3. Подытоживая все сказанное: охват – это механизм создания подпро-
    странства. (С другой стороны, при использовании охвата в качестве существительного ох-
    ват и подпространство относятся к одному и тому же бесконечному множеству векторов.)




Базис
Как далеко друг от друга находятся Амстердам и Тенерифе? Примерно 2000.
Что означает «2000»? Это число имеет смысл только в том случае, если доба-
вить базисную единицу. Базис подобен линейке для измерения пространства.
  В приведенном выше примере единицей измерения является миля. Таким
образом, наше базисное измерение расстояния между Голландией и Испани-

1
    Син. начало координат пространства. – Прим. перев.
58  Векторы. Часть 2

ей составляет 1 милю. Конечно же, мы могли бы использовать и другие еди-
ницы измерения, такие как нанометры или световые годы, но думаю, можно
согласиться с тем, что миля является удобным базисом для определения рас-
стояния по этой шкале. Как насчет длины, которую ваш ноготь отрастает за
один день, – должны ли мы по-прежнему использовать мили? В техническом
плане это возможно, но, думаю, можно согласиться с тем, что миллиметр
будет более удобной базисной единицей. Для ясности: величина, на которую
вырос ваш ноготь за последние 24 часа, будет одинаковой, независимо от
того, в чем вы ее измеряете: в нанометрах, милях или световых годах. Но
разные единицы измерения более или менее удобны для разных задач.
  Вернемся к линейной алгебре: базис – это множество линеек, которые
вы используете для описания информации в мат­ри­це (например, мат­ри­це
данных). Как и в приведенных выше примерах, одни и те же данные можно
описывать, используя разные линейки, но некоторые линейки более удобны
для решения определенных задач, чем другие.
  Наиболее распространенным базисным множеством является декартова
ось: знакомая плоскость XY, которую вы используете с начальной школы.
Базисные множества для двумерного и трехмерного декартовых графиков
можно записать следующим образом:




  Обратите внимание, что декартовы базисные множества содержат векто-
ры, которые являются взаимно ортогональными и имеют единичную длину.
Это замечательные свойства, и именно по этой причине декартовы базисные
множества столь распространены (и действительно, они называются стан-
дартным базисным множеством).
  Но это не единственные базисные множества. Следующее ниже множество
является другим базисным множеством для ℝ2:




  Базисные множества S2 и T оба охватывают одно и то же подпространство
(все из ℝ2). Почему вы бы предпочли T, а не S? Представим, что мы хотим
описать точки данных p и q на рис. 3.4. Указанные точки данных можно
описать как их отношение к началу координат – то есть их координаты, – ис-
пользуя базис S либо базис T.
  В базисе S эти две координаты равны p = (3, 1) и q = (–6, 2). В линейной
алгебре мы говорим, что точки выражаются как линейные комбинации ба-
зисных векторов. В данном случае эта комбинация равна 3s1 + 1s2 для точки p
и –6s1 + 2s2 для точки q.
  Теперь давайте опишем эти точки в базисе T. В качестве координат мы
имеем p = (1, 0) и q = (0, 2). И в терминах базисных векторов мы имеем 1t1 + 0t2
для точки p и 0t1 + 2t2 для точки q (другими словами, p = t1 и q = 2t2). Опять
                                                                        Базис  59

же, точки данных p и q одинаковы независимо от базисного множества, но T
обеспечил компактное и ортогональное описание.
  Базисы имеют чрезвычайную важность в науке о данных и машинном
обуче­нии. Собственно говоря, многие задачи прикладной линейной алгебры
концептуализируются как отыскание наилучшего множества базисных век-
торов с целью описания некоторого подпространства. Вы, вероятно, слышали
о следующих терминах: уменьшение размерности, извлечение признаков,
анализ главных компонент, анализ независимых компонент, факторный ана-
лиз, сингулярное разложение, линейный дискриминантный анализ, аппрок-
симация изображений, сжатие данных. Хотите верьте, хотите нет, но все эти
методы анализа, по сути, являются способами определения оптимальных
базисных векторов для конкретной задачи.




                           Рис. 3.4  Те же самые точки (p и q)
           могут быть описаны базисным множеством S (черные сплошные линии)
                            либо T (черные пунктирные линии)

   Рассмотрим рис. 3.5: это набор данных, состоящий из двух переменных
(каждая точка представляет точку данных). На рисунке фактически показаны
три четко различимых базиса: «стандартное базисное множество», соот-
ветствующее отрезкам x = 0 и y = 0, и базисные множества, определенные
с помощью анализа главных компонент (PCA1; левый график) и с помощью
анализа независимых компонент (ICA2; правый график). Какое из этих базис-
ных множеств обеспечивает «наилучший» способ описания данных? У вас,
возможно, возникнет соблазн сказать, что базисные векторы, вычисленные

1
    Англ. Principal Components Analysis. – Прим. перев.
2
    Англ. Independent Components Analysis. – Прим. перев.
60  Векторы. Часть 2

с помощью ICA, будут наилучшими. Истина же будет посложнее (как это
обычно и бывает): ни одно базисное множество не является, по сути, лучше
или хуже; для конкретных задач разные базисные множества могут быть
более или менее полезными в зависимости от целей анализа, особенностей
данных, налагаемых анализом ограничений и т. д.

           Базисные векторы PCA                           Базисные векторы ICA




                          Рис. 3.5  Двумерный набор данных
               с использованием разных базисных векторов (черные линии)



Определение базиса
Разобравшись со смыслом понятий базиса и базисного множества, формаль-
ное определение станет простым. В сущности, базис – это просто комбинация
охвата и независимости: множество векторов может быть базисом для не-
которого подпространства, если оно
   1) охватывает это подпространство и
   2) является независимым множеством векторов.
   Базис должен охватывать подпространство, чтобы его использовать в качест­
ве базиса для этого подпространства, потому что невозможно описать то, что не-
возможно измерить1. На рис. 3.6 показан пример точки за пределами одномер-
ного подпространства. Базисный вектор для этого подпространства не может
измерить точку r. Черный вектор по-прежнему является допустимым базисным
вектором для подпространства, которое он охватывает, но он не формирует
базиса для любого подпространства за пределами того, что он охватывает.
   Таким образом, базис должен охватывать пространство, для которого он
используется. Это понятно. Но почему базисное множество требует линейной
независимости? Причина в том, что любой данный вектор в подпространстве
должен иметь уникальную координату, используя этот базис. Давайте пред-

1
    В науке это общепризнанная азбучная истина.
                                                                         Резюме  61

ставим, что мы описываем точку p из рис. 3.4, используя следующее ниже
множество векторов:




               Рис. 3.6  Базисное множество может измерять только то,
                           что содержится внутри его охвата

   U – это совершенно допустимое множество векторов, но оно определенно
не является базисным множеством. Почему1?
   Какая линейно-взвешенная комбинация описывает точку p в множест­
ве U? Дело в том, что коэффициентами линейно-взвешенной комбинации
трех векторов в U могут быть (3, 0, 1) либо (0, 1,5, 1), либо ... триллионы других
возможностей. Такая ситуация сбивает с толку, и поэтому математики ре-
шили, что в пределах базисного множества вектор должен иметь уникальные
координаты. А линейная независимость гарантирует такую уникальность.
   Для ясности, точку p (или любую другую точку) можно описать с исполь-
зованием бесконечного числа базисных множеств. Таким образом, ре-
зультат измерения не будет уникальным с точки зрения громадного числа
возможных базисных множеств. Но в пределах базисного множества точка
определяется ровно одной линейно-взвешенной комбинацией. То же самое
и с моей аналогией по поводу расстояния в начале этого раздела: расстояние
от Амстердама до Тенерифе можно измерить, используя много разных еди-
ниц измерения, но это расстояние имеет только одно значение в расчете на
одну единицу измерения. Расстояние не составляет одновременно 3200 миль
и 2000 миль, но оно составляет одновременно 3200 километров и 2000 миль.
(Примечание для ботаников: я здесь аппроксимирую, хорошо?)



Резюме
Поздравляю с завершением еще одной главы! (Ну, почти завершением: еще
нужно решить несколько упражнений по программированию.) Смысл этой

1
    Потому что оно является линейно зависимым множеством.
62  Векторы. Часть 2

главы состоял в том, чтобы вывести ваши фундаментальные знания о векторах
на новый уровень. Ниже приведен список ключевых выводов, но, пожалуйста,
помните, что в основе всех этих выводов лежит очень мало элементарных
принципов, в первую очередь линейно-взвешенных комбинаций векторов.
    Множество векторов – это коллекция векторов. В множестве может
      быть конечное либо бесконечное число векторов.
    Линейно-взвешенная комбинация означает умножение на скаляр
      и сложение векторов в множестве. Линейно-взвешенная комбинация
      является одним из наиболее важных понятий в линейной алгебре.
    Множество векторов является линейно зависимым, если вектор в мно-
      жестве можно выразить как линейно-взвешенную комбинацию других
      векторов в множестве. И множество является линейно независимым,
      если такой линейно-взвешенной комбинации не существует.
    Подпространство – это бесконечное множество всех возможных линей-
      но-взвешенных комбинаций множества векторов.
    Базис – это линейка для измерения пространства. Множество векторов
      может быть базисом для подпространства, если оно
      1) охватывает это подпространство и
      2) является линейно независимым.
  Одной из главнейших целей в науке о данных является отыскание наи-
лучшего базисного множества, чтобы описывать наборы данных или решать
задачи.



Упражнения по программированию
   Упражнение 3.1
   Перепишите исходный код линейно-взвешенной комбинации, но поместите
скаляры в список, а векторы – в качестве элементов в списке (таким образом,
у вас будет два списка, один из скаляров и один из массивов NumPy). Затем при-
мените цикл for, чтобы реализовать операцию линейно-взвешенной комбина-
ции. Инициализируйте выходной вектор функцией np.zeros(). Подтвердите,
что вы получаете тот же результат, что и в приведенном ранее исходном коде.

  Упражнение 3.2
  Хотя метод прокручивания списков из предыдущего упражнения в цикле
не так эффективен, как умножение мат­риц на векторы, он более масштаби-
руем, чем без цикла for. Этот факт можно обследовать, добавив в качестве
элементов списков дополнительные скаляры и векторы. Что произойдет,
если новый добавленный вектор находится в ℝ4, а не в ℝ3? А что произойдет,
если у вас будет больше скаляров, чем векторов?

  Упражнение 3.3
  В этом упражнении вы будете извлекать случайные точки в подпростран-
ствах. Благодаря этому упражнению вы укрепите идею о том, что подпро-
странства содержат любую линейно-взвешенную комбинацию охватываю-
                                                  Упражнения по программированию  63

щих векторов. Определите множество векторов, содержащее один вектор [1,
3]. Затем создайте 100 чисел, выбранных случайным образом из равномер-
ного распределения между –4 и +4. Это ваши случайные скаляры. Умножьте
случайные скаляры на базисный вектор, чтобы создать 100 случайных точек
в подпространстве. Нанесите эти точки на график.
   Затем повторите процедуру, но используя два вектора в ℝ3: [3, 5, 1] и [0, 2, 2].
Обратите внимание, что для 100 точек и двух векторов вам нужно 100×2 слу-
чайных скаляров. Результирующие случайные точки будут находиться на
плоскости. На рис. 3.7 показано, как будут выглядеть результаты (по рисунку
не ясно, что точки лежат на плоскости, но вы увидите это, когда будете пере-
таскивать график на экране).
   Рекомендую для рисования точек использовать библиотеку plotly, чтобы
иметь возможность кликать и перетаскивать трехмерную ось по всему гра-
фику. Вот подсказка, как сделать так, чтобы все работало1:
import plotly.graph_objects as go
fig = go.Figure( data=[go.Scatter3d(
                     x=points[:,0],
                     y=points[:,1],
                     z=points[:,2],
                     mode='markers' )])
fig.show()

  Наконец, повторите случай ℝ3, но задайте второй вектор как 1/2 от пер-
вого.

А)                                          В)




                                 Рис. 3.7  Упражнение 3.3


1
     Указанный выше исходный код реализован в рамках среды Colab. При работе
     в блокноте Jupyter локально в самом начале следует добавить две следующие стро-
     ки кода:
     import plotly.io as pio
      pio.renderers.default = 'iframe' – Прим. перев.
Глава       4
            Применения векторов

Работая с предыдущими двумя главами, вы, возможно, ощущали, что часть
материала была эзотерической и абстрактной. Вероятно, вы чувствовали, что
задача изучения линейной алгебры не окупится пониманием приложений,
реально существующих в области науки о данных и машинного обучения.
  Надеюсь, что данная глава эти сомнения у вас развеет. В этой главе вы
узнае­те, как векторы и векторные операции используются в методах анализа
в рамках науки о данных. И вы сможете расширить эти знания, выполнив
упражнения.



Корреляция и косинусное сходство
Корреляция представляет собой один из наиболее фундаментальных и важ-
ных методов анализа в статистике и машинном обучении. Коэффициент
корреляции – это одно число, которое количественно описывает линейную
взаимосвязь между двумя переменными. Коэффициенты корреляции варьи­
руются от –1 до +1, причем –1 указывает на идеальную отрицательную взаи­
мосвязь, +1 – на идеальную положительную взаимосвязь, а 0 указывает на
отсутствие линейной взаимосвязи. На рис. 4.1 показано несколько примеров
пар переменных и их коэффициентов корреляции.
   В главе 2 я упоминал, что точечное произведение участвует в коэффициен-
те корреляции и что величина точечного произведения связана с величиной
числовых значений в данных (вспомните обсуждение темы использования
граммов вместо фунтов для измерения веса). Следовательно, коэффициент
корреляции требует некоторой нормализации, чтобы он находился в ожида-
емом диапазоне от –1 до +1. Эти две нормализации таковы:

Центрировать каждую переменную по среднему значению
  Центрирование по среднему значению означает вычитание среднего зна-
  чения из каждого значения данных.

Разделить точечное произведение на произведение векторных норм
  Это делящая нормализация, которая отменяет единицы измерения и шка-
  лирует максимально возможную величину корреляции в |1|.
                                            Корреляция и косинусное сходство  65




                           Рис. 4.1  Примеры данных,
                  демонстрирующих положительную корреляцию,
                отрицательную корреляцию и нулевую корреляцию.
    Нижняя правая панель иллюстрирует, что корреляция является линейной мерой;
         нелинейные взаимосвязи между переменными могут существовать,
                       даже если их корреляция равна нулю

  Уравнение 4.1 показывает полную формулу коэффициента корреляции
Пирсона.

  Уравнение 4.1. Формула коэффициента корреляции Пирсона




  Возможно, тут не так очевидно, что корреляция представляет собой не
что иное, как три точечных произведения. Уравнение 4.2 показывает эту же
формулу, переписанную с использованием обозначения линейно-алгебраи-
ческого точечного произведения. В этом уравнении —x является среднецент­
рированной версией x (то есть переменной x с примененной нормализацией
№ 1).
66  Применения векторов

  Уравнение 4.2. Корреляция Пирсона, выраженная на языке линейной
  алгебры




  Так что вот так: знаменитый и широко используемый коэффициент корре-
ляции Пирсона – это просто точечное произведение между двумя перемен-
ными, нормированное векторными модулями переменных. (Кстати, по этой
формуле также можно заметить, что если переменные единично нормиро-
ваны таким образом, что ||x|| = ||y|| = 1, то их корреляция равна их точечному
произведению.
  (Вспомним из упражнения 2.6, что           )
  Корреляция – это не единственный способ оценивать сходство между
двумя переменными. Еще один метод называется косинусным сходством.
Формула косинусного сходства представляет собой просто геометрическую
формулу точечного произведения (уравнение 2.11), решаемую для косинус-
ного члена:




где α – это точечное произведение между x и y.
   Может показаться, что корреляция и косинусное сходство – это совер-
шенно одна и та же формула. Однако следует запомнить, что уравнение 4.1
является полной формулой, тогда как уравнение 4.2 является упрощением
в рамках допущения, что переменные уже были центрированы по среднему
значению. Отсюда косинусное сходство не содержит первый фактор норма-
лизации.


               Корреляция по сравнению с косинусным сходством
 Что означает «связанность» двух переменных между собой? Корреляция Пирсона и ко-
 синусное сходство могут давать разные результаты для одних и тех же данных, поскольку
 они исходят из разных допущений. По мнению Пирсона, переменные [0, 1, 2, 3] и [100, 101,
 102, 103] идеально коррелируют (ρ = 1), поскольку изменения в одной переменной точно
 отражаются в другой переменной, и не имеет значения, что одна переменная имеет более
 крупные числовые значения. Однако косинусное сходство между этими переменными
 равно .808 – они не находятся на одной числовой шкале и, следовательно, связаны не
 идеально. Ни одна мера не является ни неправильной, ни наилучшей, чем другая; просто
 разные статистические методы принимают разные допущения о данных, и эти допущения
 влияют на результаты – и на правильную интерпретацию. У вас будет возможность обсле-
 довать этот момент в упражнении 4.2.


  Из данного раздела можно понять, почему корреляция Пирсона и косинус-
ное сходство отражают линейную зависимость между двумя переменными:
они основаны на точечном произведении, а точечное произведение является
линейной операцией.
                      Фильтрация временных рядов и обнаружение признаков  67

   С этим разделом связаны четыре упражнения по программированию, ко-
торые приведены в конце главы. Вы можете выбрать, когда их решать: перед
чтением следующего раздела либо продолжать чтение остальной части гла-
вы, а затем проработать упражнения. (Моя личная рекомендация относится
к первому, но вы являетесь хозяином своей судьбы в линейной алгебре!)



Фильтрация временных рядов
и обнаружение признаков
Точечное произведение также используется в фильтрации временных ря-
дов. Фильтрация – это, по сути, метод обнаружения признаков, при котором
шаблон – именуемый на языке фильтрации вычислительным ядром – сопо-
ставляется с частями сигнала временного ряда, и результатом фильтрации
является еще один временной ряд, который показывает, насколько харак-
теристики сигнала соответствуют характеристикам ядра. Ядра тщательно
конструируются, чтобы оптимизировать те или иные критерии, такие как
плавные колебания, острые пики, определенные контуры волновых форм
и т. д.
   Механизм фильтрации заключается в вычислении точечного произведе-
ния между ядром и сигналом временного ряда. Но фильтрация обычно тре-
бует локального обнаружения признаков, и ядро обычно намного короче, чем
весь временной ряд. Поэтому мы вычисляем точечное произведение между
ядром и коротким фрагментом данных той же длины, что и ядро. Такая про-
цедура создает одну временную точку в отфильтрованном сигнале (рис. 4.2),




                    Изначальный сигнал
                                                      Ядро




                     Отфильтрованный сигнал




              Рис. 4.2  Иллюстрация фильтрации временного ряда
68  Применения векторов

а затем ядро перемещается на один временной шаг вправо, чтобы вычислить
точечное произведение с другим (накладывающимся) сегментом сигнала.
Формально эта процедура называется сверткой и предусматривает несколь-
ко дополнительных шагов, которые я опускаю, чтобы сосредоточиться на
применении точечного произведения в обработке сигналов.
   Темпоральная фильтрация является важной темой в науке и технике.
И действительно, без темпоральной фильтрации не было бы музыки, радио,
телекоммуникаций, спутников и т. д. И все же математическим сердцем,
которое заставляет вашу музыку пульсировать, является точечное произ-
ведение векторов.
   В упражнениях в конце главы вы узнаете, как точечные произведения ис-
пользуются для обнаружения признаков (резких изменений) и сглаживания
данных временных рядов.



Кластеризация методом k-средних
Кластеризация методом k-средних – это неконтролируемый метод класси-
фицирования многопеременных данных на относительно малое число групп,
или категорий, основываясь на минимизации расстояния до центра группы.
  Кластеризация методом k-средних является важным методом анализа
в машинном обучении, и существуют самые изощренные варианты класте-
ризации методом -средних. Здесь мы реализуем простую версию алгоритма
-средних с целью увидеть, как понятия о векторах (в частности, векторах,
векторных нормах и транслировании) используются в алгоритме k-средних.
  Вот краткое описание алгоритма, который мы напишем.
  1.	Инициализировать k центроидов как случайные точки в простран-
      стве данных. Каждый центроид является классом или категорией,
      и на следующих шагах каждое наблюдение данных будет относиться
      к каждому классу. (Центроид – это центр, обобщенный на любое число
      измерений1.)
  2.	Вычислить евклидово расстояние между каждым наблюдением данных
      и каждым центроидом2.
  3.	Отнести каждое наблюдение данных к группе с ближайшим цент­
      роидом.
  4.	Обновить каждый центроид как среднее значение всех наблюдений
      данных, назначенных этому центроиду.
  5.	Повторять шаги 2–4 до тех пор, пока не будет удовлетворен критерий
      схождения либо в течение N итераций.
  Если вам удобно программировать на Python и вы хотели бы реализовать
этот алгоритм, то рекомендую это сделать сейчас, прежде чем продолжать.


1
    Центроид также иногда именуется центром тяжести. – Прим. перев.
2
    Напоминание: евклидово расстояние – это квадратный корень из суммы квадратов
    расстояний от точки наблюдения данных до центроида.
                                                   Кластеризация методом k-средних  69

Далее мы проработаем математику и исходный код по каждому указанному
шагу, уделяя особое внимание использованию векторов и транслированию
в NumPy. Мы также протестируем указанный алгоритм, используя случайно
сгенерированные двумерные данные, чтобы подтвердить правильность ис-
ходного кода.
   Давайте начнем с шага 1: инициализировать центроиды k случайных
кластеров. k – это параметр кластеризации методом k-средних; в реальных
данных определить оптимальный параметр k довольно трудно, но здесь мы
зададим k = 3. Инициализировать центроиды случайных кластеров можно
несколькими способами; в целях упрощения задачи в качестве центроидов
я случайно возьму k выборок данных. Данные содержатся в переменной data
(эта переменная имеет размер 150×2, что соответствует 150 наблюдениям по
2 признака) и визуализируются на верхней левой панели рис. 4.3 (онлайно-
вый исходный код показывает, как эти данные генерируются):
k=3
ridx = np.random.choice(range(len(data)),k,replace=False)
centroids = data[ridx,:] # мат­ри­ца данных содержит образцы по признакам

  Теперь перейдем к шагу 2: вычислить расстояние между каждым наблюде-
нием данных и центроидом каждого кластера. Здесь мы используем линей-
но-алгебраические концепции, которые вы усвоили в предыдущих главах.
Для одного наблюдения данных и центроида евклидово расстояние вычис-
ляется следующим образом:




где δi,j указывает на расстояние от наблюдения данных i до центроида j,
dix – это признак x-го наблюдения данных, а cjx – координата по оси x для
центроида j.
   Возможно, вы думаете, что этот шаг должен быть реализован с использо-
ванием двойного цикла for: один цикл по k центроидам и второй цикл по
N наблюдениям данных (вероятно, вы даже подумали о третьем цикле for
по признакам данных). Однако тут можно применить векторы и транслиро-
вание, сделав эту операцию компактной и эффективной. Это пример того,
как линейная алгебра нередко выглядит иначе в уравнениях по сравнению
с исходным кодом:
dists = np.zeros((data.shape[0], k))
for ci in range(k):
    dists[:,ci] = np.sum((data-centroids[ci,:])**2,
                         axis=1)

  Давайте подумаем о размерах этих переменных: data имеет размер 150×2
(наблюдения по числу признаков), а размер centroids[ci,:] равен 1×2 (клас­
тер ci по числу признаков). В формальном плане вычесть эти два вектора
невозможно. Однако в Python реализована операция транслирования, кото-
рая будет работать путем повтора центроидов кластеров 150 раз и, следова-
70  Применения векторов

тельно, вычитая центроид из каждого наблюдения данных. Операция воз-
ведения в степень ** применяется поэлементно, и входной аргумент axis=1
говорит Python о том, что нужно суммировать по столбцам (отдельно по
каждой строке). Таким образом, результатом функции np.sum() будет массив
размером 150×1, в котором кодируется евклидово расстояние каждой точки
до центроида ci.
  Найдите минутку, чтобы сравнить этот исходный код с формулой расстоя­
ния. Действительно ли они одинаковые? На самом деле это не так: квад­
ратный корень из евклидова расстояния в коде отсутствует. И что, значит,
исходный код неправильный? Сделайте небольшую паузу и подумайте об
этом; я дам подробный ответ чуть позже.
  Шаг 3 состоит в отнесении каждого наблюдения данных к группе с мини-
мальным расстоянием. Этот шаг в Python довольно компактен и реализуется
одной функцией:
groupidx = np.argmin(dists, axis=1)

   Обратите внимание на разницу между функцией np.min, которая возвра-
щает минимальное значение, и функцией np.argmin, которая возвращает ин-
декс, при котором происходит минимум.
   Теперь можно вернуться к несоответствию между формулой расстояния
и ее реализацией в исходном коде. В данном алгоритме k-средних использу-
ется расстояние, чтобы относить каждую точку данных к ближайшему к ней
центроиду. Расстояние и квадрат расстояния монотонно связаны, поэтому
обе метрики дают один и тот же ответ. Добавление операции квадратного
корня увеличивает сложность исходного кода и время вычислений без влия­
ния на результаты, поэтому ее можно просто опустить.
   Шаг 4 заключается в перевычислении центроидов как среднего значе-
ния всех точек данных внутри класса. Здесь можно прокрутить k кластеров
в цикле и использовать индексацию Python, чтобы найти все точки данных,
отнесенные к каждому кластеру:
for ki in range(k):
    centroids[ki,:] = [ np.mean(data[groupidx==ki, 0]),
                        np.mean(data[groupidx==ki, 1]) ]

  Наконец, шаг 5 заключается в размещении приведенных выше шагов
в цикле, который повторяется до тех пор, пока не будет получено хорошее
решение. В алгоритмах k-средних производственного уровня итерации про-
должаются до тех пор, пока не будет достигнут критерий останова, например
когда центроиды кластеров больше не перемещаются. Для простоты здесь
мы выполним три итерации (данное число выбрано произвольно, чтобы
сделать график визуально сбалансированным).
  Четыре панели на рис. 4.3 показывают центроиды изначально случайных
кластеров (итерация 0) и их обновленные местоположения после каждой из
трех итераций.
  Если вы изучаете алгоритмы кластеризации, то вы узнаете изощренные
методы инициализации центроидов и критерии останова, а также количест­
венные методы отбора подходящего параметра k. Тем не менее все методы
                                           Упражнения по программированию  71

k-средних, по существу, являются расширениями вышеупомянутого алгорит-
ма, и в основе их реализаций лежит линейная алгебра.




                  Рис. 4.3  Результат работы метода k-средних




Упражнения по программированию
Упражнения по корреляции
   Упражнение 4.1
   Напишите функцию Python, которая на входе принимает два вектора и на
выходе выдает два числа: коэффициент корреляции Пирсона и значение
косинусного сходства. Напишите исходный код, который следует форму-
лам, представленным в данной главе; не используйте вызовы встроенной
в NumPy функции np.corrcoef и встроенной в SciPy функции spatial.distance.
cosine. Убедитесь, что два значения на выходе идентичны, когда переменные
уже центрированы по среднему значению, и различны, когда переменные не
центрированы по среднему значению.

  Упражнение 4.2
  Давайте продолжим обследовать разницу между корреляцией и косинус-
ным сходством. Создайте переменную, содержащую целые числа от 0 до 3,
и вторую переменную, равную первой переменной плюс некоторое смеще-
72  Применения векторов

ние. Затем создайте симуляцию, в которой вы систематически варьируете
это смещение между –50 и +50 (то есть на первой итерации симуляции вторая
переменная будет равна [–50, –49, –48, –47]). В цикле for вычислите корре-
ляцию и косинусное сходство между двумя переменными и сохраните эти
результаты. Затем постройте линейный график, показывающий, как среднее
смещение влияет на корреляцию и косинусное сходство. Вы должны суметь
воспроизвести рис. 4.4.




                     Рис. 4.4  Результаты упражнения 4.2



  Упражнение 4.3
  В Python есть несколько функций, которые вычисляют коэффициент кор-
реляции Пирсона. Одна из них называется pearsonr и находится в модуле
stats библиотеки SciPy. Откройте исходный код этого файла (подсказка:
??functionname) и убедитесь, что вы понимаете, как реализация на Python со-
относится с формулами, представленными в данной главе.

   Упражнение 4.4
   Зачем вообще нужно программировать свои конкретно-прикладные
функции, если они уже существуют в Python? Отчасти причина состоит
в том, что написание своих конкретно-прикладных функций имеет огром-
ную образовательную ценность, потому что вы видите, что (в данном случае)
корреляция – это простое вычисление, а не какой-то невероятно сложный
черно-ящичный алгоритм, который под силу понять только кандидату в об-
ласти вычислительной науки. Но еще одна причина заключается в том, что
встроенные функции иногда работают медленнее из-за громадного числа
проверок входных данных, работы с дополнительными входными парамет­
рами, преобразованиями типов данных и т. п. Все это повышает удобство
использования, но за счет времени вычислений.
                                           Упражнения по программированию  73

  В данном упражнении ваша цель – посмотреть, будет ли ваша сокращен-
ная функция корреляции работать быстрее, чем функция NumPy corrcoef.
Модифицируйте функцию из упражнения 4.2, чтобы вычислить только ко-
эффициент корреляции. Затем, в цикле for по 1000 итерациям, сгенерируйте
две переменные из 500 случайных чисел и вычислите корреляцию между
ними. Засеките время исполнения цикла for. Затем повторите, но используя
функцию np.corrcoef. В моих тестах конкретно-прикладная функция была
примерно на 33 % быстрее, чем np.corrcoef. В этих игрушечных примерах
различия измеряются в миллисекундах, но если вы выполняете миллиарды
корреляций с крупными наборами данных, то эти миллисекунды склады-
ваются, давая действительно большой прирост в производительности! (Об-
ратите внимание, что написание своих конкретно-прикладных функций без
проверок входных данных сопряжено с риском ошибок на входе, которые
были бы обнаружены функцией np.corrcoef.) (Также обратите внимание, что
преимущество в скорости уменьшается с более крупными векторами. По-
пробуйте сами!)


Упражнения по фильтрации и обнаружению
признаков
   Упражнение 4.5
   Давайте построим детектор резких изменений1. Ядро детектора резких
изменений будет очень простым: [–1 +1]. Точечное произведение этого ядра
на фрагмент сигнала временного ряда с постоянным значением (например,
[10 10]) равно 0. Но это точечное произведение будет крупным, когда сигнал
имеет резкое изменение (например, [1 10] даст точечное произведение, рав-
ное 9). Мы будем работать с сигналом, который будет представлен функцией
плато. Графики A и B на рис. 4.5 показывают ядро и сигнал. На первом шаге
в данном упражнении пишется исходный код, который создает эти два вре-
менных ряда.
   Далее напишите цикл for для временных точек в сигнале. В каждый мо-
мент времени вычисляйте точечное произведение между ядром и сегментом
данных временного ряда, который имеет ту же длину, что и ядро. Вы должны
создать график, который выглядит как график C на рис. 4.5. (Сосредоточьтесь
больше на результате, чем на эстетике.) Обратите внимание, что наш детек-
тор резких изменений вернул 0, когда сигнал был ровным, +1, когда сигнал
подскочил вверх, и –1, когда сигнал прыгнул вниз.
   Смело продолжайте обследовать этот исходный код. Например, изменится
ли что-нибудь, если дополнить ядро нулями ([0 –1 1 0])? А что, если пере-
вернуть ядро так, чтобы оно было [1 –1]? Как насчет того, если ядро будет
асимметричным ([–1 2])?

1
    В обработке временных рядов обнаружение резких изменений также называется
    обнаружением скачков, всплесков или отклонений от среднего уровня временного
    ряда или сигнала. – Прим. перев.
74  Применения векторов

А)                                              B)




                      C)




                           Рис. 4.5  Результаты упражнения 4.5

  Упражнение 4.6
  Теперь мы повторим ту же процедуру, но с другим сигналом и ядром. Цель
будет состоять в том, чтобы сгладить неровный временной ряд. Временной
ряд будет состоять из 100 случайных чисел, сгенерированных из гауссова рас-
пределения (также именуемого нормальным распределением). Ядро будет
представлять собой функцию в форме колокола, которая аппроксимирует
гауссову функцию, определенную как числа [0, .1, .3, .8, 1, .8, .3, .1, 0], но шка-
лированную так, чтобы сумма по ядру составляла 1. Ваше ядро должно соот-
ветствовать графику A на рис. 4.6, хотя из-за случайных чисел ваш сигнал не
будет выглядеть точно так же, как график B.
  Скопируйте и адаптируйте исходный код из предыдущего упражнения
под вычисление скользящего временного ряда точечных произведений –
сигнала, отфильтрованного гауссовым ядром. Предупреждение: будьте вни-
мательны к индексации в цикле for. График C на рис. 4.6 показывает при-
мерный результат. Хорошо видно, что отфильтрованный сигнал является
сглаженной версией изначального сигнала. Такая процедура также называ-
ется низкочастотной фильтрацией.

  Упражнение 4.7
  Замените 1 в центре ядра на –1 и усредните центр ядра. Затем выполните
исходный код фильтрации и построения графика повторно. Каким будет
результат? Он действительно подчеркивает резкие признаки! По сути дела,
                                             Упражнения по программированию  75

данное ядро теперь является высокочастотным фильтром, а значит, оно при-
глушает плавные (низкочастотные) признаки и выделяет быстро меняющие­
ся (высокочастотные) признаки.

A)                                           В)




                   С)




                        Рис. 4.6  Результаты упражнения 4.6



Упражнения по алгоритму k-средних
  Упражнение 4.8
  Оптимальное значение k можно определить путем повторения кластери-
зации несколько раз (всякий раз с использованием случайно инициализи-
рованных центроидов кластеров) и оценивания того, является ли итоговая
кластеризация одинаковой либо другой. Не генерируя новых данных, повто-
рите исходный код алгоритма -средних несколько раз, используя k = 3, что-
бы увидеть похожесть/непохожесть результирующих кластеров (это качест­
венная оценка, основанная на визуальном осмотре). Выглядят ли итоговые
отнесения к кластерам в целом похожими, даже если центроиды выбраны
случайным образом?

  Упражнение 4.9
  Повторите кластеризации несколько раз, используя k = 2 и k = 4. Что вы
думаете об этих результатах?
Глава       5
                         Матрицы. Часть 1

Матрица – это вектор, перенесенный на следующий уровень. Матрицы как
математические объекты очень разноплановы. В них могут храниться набо-
ры уравнений, геометрические преобразования, положения частиц во време-
ни, финансовые отчеты и громадное число других вещей. В науке о данных
мат­ри­цы иногда называют таблицами данных, в которых строки соответ-
ствуют наблюдениям (например, клиентам), а столбцы – признакам (напри-
мер, покупкам).
  Данная и следующие две главы выведут ваши знания о линейной алгебре
на новый уровень. Выпейте чашечку кофе и наденьте свою мыслительную
тюбетейку. К концу главы ваш мозг станет больше.



Создание и визуализация матриц в NumPy
В зависимости от контекста мат­ри­цы концептуализируются в уме как мно-
жество векторов-столбцов, расположенных бок о бок (например, как таблицы
в формате «наблюдения по признакам»), как множество уложенных стопкой
векторов-строк (например, в виде мультисенсорных данных, в которых каж-
дая строка – это временной ряд из другого канала) или как упорядоченный
набор отдельных матричных элементов (например, в виде изображения,
в каждом матричном элементе которого закодировано значение интенсив-
ности пиксела).


Визуализация, индексация и нарезка матриц
Малые мат­ри­цы можно легко распечатывать полностью, как в следующих
ниже примерах:
                                       Создание и визуализация матриц в NumPy  77

   Но это не масштабируется, и мат­ри­цы, с которыми вы работаете на прак-
тике, могут быть большими, возможно, содержащими миллиарды элементов.
Поэтому более крупные мат­ри­цы можно визуализировать в виде изображе-
ний. Числовое значение каждого элемента матричной карты соотносится
с цветом изображения. В большинстве случаев такие карты1 псевдоцветны,
поскольку соотнесенность числового значения с цветом является произволь-
ной. На рис. 5.1 показано несколько примеров мат­риц, визуализированных
в виде изображений с использованием библиотеки Python matplotlib.




            Рис. 5.1  Три мат­ри­цы, визуализированные в виде изображений

   Матрицы обозначаются заглавными буквами жирным шрифтом, напри-
мер мат­ри­ца A или M. Размер мат­ри­цы указывается традиционным образом
в формате (строка, столбец). Например, следующая ниже мат­ри­ца имеет раз-
мер 3×5, поскольку в ней три строки и пять столбцов:




   На конкретные элементы мат­ри­цы можно ссылаться, обращаясь по индек-
су строки и столбца: элемент в 3-й строке и 4-м столбце мат­ри­цы A обознача-
ется как a3.4 (в предыдущем примере мат­ри­цы a3.4 = 8). Важное напоминание:
в математике используется индексация с отсчетом от единицы, в то время
как в Python используется индексация с отсчетом от нуля. Таким образом,
в Python элемент a3.4 индексируется как A[2,3].
   Подмножество строк или столбцов мат­ри­цы извлекается с помощью опе-
рации нарезки. Если вы в Python новичок, то обратитесь к главе 16, чтобы
ознакомиться с нарезкой списков и массивов NumPy. Для того чтобы извлечь
срез из мат­ри­цы, надо указать начальные и конечные строки и столбцы,
а также шаг нарезки, равный 1. Онлайновый исходный код проведет вас по
всей процедуре, а следующий ниже исходный код показывает пример из-
влечения подматрицы из строк 2–4 и столбцов 1–5 более крупной мат­ри­цы:
A = np.arange(60).reshape(6, 10)
sub = A[1:4:1,0:5:1]

1
    Син. растровые изображения. – Прим. перев.
78  Матрицы. Часть 1

    Ниже приведены полная мат­ри­ца и подматрица:
Изначальная мат­ри­ца:
[[ 0 1 2 3 4 5 6 7 8 9]
 [10 11 12 13 14 15 16 17   18   19]
 [20 21 22 23 24 25 26 27   28   29]
 [30 31 32 33 34 35 36 37   38   39]
 [40 41 42 43 44 45 46 47   48   49]
 [50 51 52 53 54 55 56 57   58   59]]

Подматрица:
[[10 11 12 13 14]
 [20 21 22 23 24]
 [30 31 32 33 34]]



Специальные матрицы
Существует бесконечное число мат­риц, потому что существует бесконечное
число способов организации чисел в мат­ри­цу. Но мат­ри­цы можно описывать
с использованием относительно малого числа характеристик, в результате
создавая «семейства», или категории мат­риц. Указанные категории важно
знать, потому что они появляются в определенных операциях либо обладают
определенными полезными свойствами.
  Некоторые категории мат­риц используются так часто, что для их создания
есть специальные функции NumPy. Ниже приведен список нескольких рас-
пространенных специальных мат­риц и исходный код Python для их созда-
ния1; вы их увидите на рис. 5.2.

Матрица случайных чисел
 Это мат­ри­ца, которая содержит числа, берущиеся случайно из некоторого
 распределения, обычно гауссова (также именуемого нормальным). Матри-
 цы случайных чисел отлично подходят для обследования линейной алгеб­
 ры в исходном коде, потому что они быстро и легко создаются с любым
 размером и рангом (понятие ранга мат­ри­цы вы узнаете в главе 16).
 В NumPy имеется несколько способов создания случайных мат­риц в за-
 висимости от того, из какого распределения вы хотите извлекать числа.
 В этой книге мы будем использовать числа, в основном распределенные
 по Гауссу:
    Mrows = 4 # очертание 0
    Ncols = 6 # очертание 1
    A = np.random.randn(Mrows, Ncols)

Квадратная и неквадратная
  Квадратная мат­ри­ца имеет такое же число строк, что и столбцов; други-
  ми словами, мат­ри­ца имеет размер ℝN×N. Неквадратная мат­ри­ца, также

1
    Существуют и другие специальные мат­ри­цы, о которых вы узнаете в книге позже,
    но этого списка для начала будет достаточно.
                                        Создание и визуализация матриц в NumPy  79

    иногда именуемая прямоугольной матрицей, имеет отличающееся число
    строк и столбцов. Квадратные и прямоугольные мат­ри­цы можно создавать
    из случайных чисел, настроив параметры очертания в приведенном выше
    исходном коде.
    Прямоугольные мат­ри­цы называются высокими, если в них больше строк,
    чем столбцов, и широкими, если в них больше столбцов, чем строк.

Диагональная
  Диагональ мат­ри­цы – это элементы, начинающиеся в верхнем левом углу
  и спускающиеся в нижний правый. Диагональная мат­ри­ца имеет нули во
  всех внедиагональных элементах; диагональные элементы тоже могут
  содержать нули, но они являются единственными элементами, которые
  могут содержать ненулевые значения.
  Функция NumPy np.diag() имеет два вида поведения в зависимости от
  входных данных: при вводе мат­ри­цы функция np.diag возвращает диа-
  гональные элементы в виде вектора; при вводе вектора функция np.diag
  возвращает мат­ри­цу с этими векторными элементами на диагонали. (При-
  мечание: извлечение диагональных элементов мат­ри­цы не называется
  «диагонализацией мат­ри­цы»; это отдельная операция, представленная
  в главе 13.)

Треугольная
  Треугольная мат­ри­ца содержит одни нули выше либо ниже главной диаго-
  нали. Матрица называется верхней треугольной, если ненулевые элементы
  находятся выше диагонали, и нижней треугольной, если ненулевые элемен-
  ты находятся ниже диагонали.
  В NumPy имеются специальные функции для извлечения верхнего (np.
  triu()) либо нижнего (np.tril()) треугольника мат­ри­цы.

Единичная
  Единичная мат­ри­ца1 является одной из наиболее важных специальных
  мат­риц. Она эквивалентна числу 1 в том смысле, что любая мат­ри­ца или
  вектор, умноженные на единичную мат­ри­цу, будут той же самой матрицей
  или вектором. Единичная мат­ри­ца – это квадратная диагональная мат­ри­
  ца, все диагональные элементы которой имеют значение 1. Она обознача-
  ется буквой I. Иногда вместе с буквой можно увидеть индекс, указываю-
  щий на ее размер (например, I5 – это единичная мат­ри­ца размером 5×5);
  если индекса нет, то ее размер можно определить из контекста (например,
  чтобы уравнение было совместимым).
  В Python единичная мат­ри­ца создается функцией np.eye().

Нулей
  Матрица нулей сравнима с вектором нулей: это мат­ри­ца, состоящая из
  одних нулей. Как и вектор нулей, она обозначается отмеченным жирным
  шрифтом нулем: 0. Возможно, то, что один и тот же символ обозначает
  как вектор, так и мат­ри­цу, будет немного сбивать с толку, но в математи-

1
    Англ. identity matrix; син. мат­ри­ца тождественного преобразования. – Прим. перев.
80  Матрицы. Часть 1

  ческой и естественно-научной нотации такого рода перегрузка довольно
  распространена.
  Матрица нулей создается функцией np.zeros().




                    Рис. 5.2  Несколько специальных мат­риц.
                        Числа и значения в оттенках серого
                  указывают значение каждого элемента мат­ри­цы




Матричная математика: сложение, умножение
на скаляр, адамарово умножение
Математические операции на матрицах делятся на две категории: понятные
и непонятные на интуитивном уровне. В общем и целом интуитивно по-
нятные операции можно выражать в виде пошаговых процедур, тогда как
объяснение интуитивно непонятных операций требует больше времени, а их
понимание – немного практики. Давайте начнем с интуитивно понятных
операций.


Сложение и вычитание
Две мат­ри­цы складываются путем сложения соответствующих элементов
мат­риц. Вот пример:
         Матричная математика: сложение, умножение на скаляр, адамарово умножение  81

  Как можно было догадаться из примера, сложение мат­риц определяется
только между двумя мат­ри­ца­ми одинакового размера.


«Сдвиг» матрицы
Как и в случае с векторами, прибавить скаляр к мат­ри­це, как в λ + A, невоз-
можно формально. Python же такую операцию допускает (например, 3+np.
eye(2)), которая предусматривает транслирование скаляра в каждый элемент
мат­ри­цы. Это удобное вычисление, но формально оно не является линейно-
алгебраической операцией.
  Однако существует линейно-алгебраический способ прибавления скаляра
к квадратной мат­ри­це, и он называется сдвигом мат­ри­цы и работает путем
прибавления постоянного значения к диагонали, то есть реализуется посред-
ством прибавления умноженной на скаляр единичной мат­ри­цы:

    A + λI.

    Вот численный пример:




    На языке Python сдиг выполняется просто:
A   =   np.array([ [4,5,1], [0,1,11], [4,9,7] ])
S   =   6
A   +   s # НЕ сдвигается!
A   +   s*np.eye(len(A)) # сдвигается

  Обратите внимание, что меняются только диагональные элементы;
остальная часть мат­ри­цы сдвигом не искажается. На практике сдвигают на
относительно малое число, чтобы в мат­ри­це сохранить как можно больше
информации, при этом извлекая выгоду из эффектов сдвига, включая повы-
шение численной стабильности мат­ри­цы (позже в книге вы узнаете, почему
происходит нестабильность).
  На сколько именно нужно сдвигать, зависит от текущих исследований во
многих областях машинного обучения, статистики, глубокого обучения, раз-
работки систем управления и т. д. Например, сдвиг на λ = 6 – это мало или
много? Как насчет λ = .001? Очевидно, что эти числа являются «большими»
либо «малыми» по отношению к числовым значениям в мат­ри­це. Поэтому
на практике лямбда λ обычно задается как некоторая доля определенного
матрицей элемента, такого как норма или среднее значение собственных
чисел. Вы сможете обследовать эту тему в последующих главах.
  «Сдвиг» мат­ри­цы имеет два первичных (чрезвычайно важных!) приме-
нения: это механизм отыскания собственных чисел мат­ри­цы и механизм
регуляризации мат­риц при подгонке моделей к данным.
82  Матрицы. Часть 1


Умножение на скаляр и адамарово умножение
Указанные два типа умножения работают для мат­риц так же, как и для век-
торов, то есть поэлементно.
  Умножение мат­ри­цы на скаляр означает умножение каждого элемента
мат­ри­цы на один и тот же скаляр. Вот пример использования мат­ри­цы, со-
держащей буквы вместо чисел:




  Адамарово умножение аналогичным образом предусматривает поэле-
ментное умножение двух мат­риц (отсюда и альтернативная терминология
поэлементное умножение). Вот пример:




  В NumPy адамарово умножение можно реализовать с помощью функции
np.multiply(). Но нередко синтаксически его проще реализовать, используя
звездочку между двумя мат­р и­ц а­ми: A*B. Возможно, это вызовет некоторую
путаницу, поскольку стандартное умножение мат­р иц (следующий раздел)
обозначается символом @. И здесь есть тонкое, но важное различие! (Дан-
ный факт будет особенно сбивать с толку тех читателей, кто переходит на
Python из MATLAB, в котором умножение мат­р иц обозначается симво-
лом *.)
A = np.random.randn(3, 4)
B = np.random.randn(3, 4)

A*B # адамарово умножение
np.multiply(A, B) # тоже адамарово
A@B # НЕ адамарово!

   Адамарово умножение действительно имеет несколько применений в ли-
нейной алгебре, например при вычислении обратной мат­ри­цы. Однако чаще
всего оно применяется в приложениях как удобный способ хранения большо-
го числа отдельных умножений. Это похоже на то, как нередко применяется
адамарово умножение векторов, как обсуждалось в главе 2.



Стандартное умножение матриц
Теперь мы переходим к интуитивно непонятному способу умножения мат­
риц. Для ясности стандартное умножение мат­риц не особенно сложное; оно
просто отличается от того, что можно было бы ожидать. Вместо того чтобы
оперировать поэлементно, стандартное умножение мат­риц оперирует по-
                                              Стандартное умножение матриц  83

строчно/постолбцово. Собственно говоря, стандартное умножение мат­риц
сводится к систематической коллекции точечных произведений между стро-
ками одной мат­ри­цы и столбцами другой мат­ри­цы. (Эта форма умножения
формально называется просто умножением мат­риц; я добавил прилагатель-
ное стандартное, чтобы помочь устранить неоднозначность в отношении
адамарова умножения и умножения на скаляр.)
  Но прежде чем перейти к деталям процедуры умножения двух мат­риц,
сначала объясню, как определять, можно умножать две мат­ри­цы или нет.
Как вы узнаете, две мат­ри­цы можно умножить только в том случае, если их
размеры согласуются.


Правила допустимости умножения матриц
Вы знаете, что размеры мат­ри­цы записываются как M×N – строки по столб-
цам. Две умножающие друг друга мат­ри­цы могут иметь разные размеры,
поэтому давайте обозначим размер второй мат­ри­цы как N×K. При записи
двух мат­риц-сомножителей с их размерами внизу можно ссылаться на «внут­
ренние» мерности N и «внешние» мерности M и K.
  Вот важный момент: умножать мат­ри­цы допустимо только тогда, когда
«внутренние» мерности совпадают, а размер мат­ри­цы произведения опреде-
ляется «внешними» мерностями. Смотрите рис. 5.3.




             Рис. 5.3  Визуализация допустимости умножения мат­риц.
                             Запомните эту картинку

  Выражаясь формальнее, умножение мат­риц допустимо, когда число столб-
цов в левой мат­ри­це равно числу строк в правой мат­ри­це, а размер мат­
ри­цы произведения определяется числом строк в левой мат­ри­це и числом
столбцов в правой мат­ри­це. Я нахожу, что правило «внутренние/внешние»
запоминается легче.
  Уже можно видеть, что умножение мат­риц не подчиняется коммутативно-
му закону: AB может быть допустимым, тогда как BA – недопустимым. Даже
если оба умножения верны (например, если обе мат­ри­цы – квадратные), они
могут давать разные результаты. То есть если C = AB и D = BA, тогда в общем
случае C ≠ D (в некоторых особых случаях они равны, но в целом допускать
равенство невозможно).
84  Матрицы. Часть 1

  Обратите внимание на обозначения: адамарово умножение обозначается
кругом с точкой (A ⊙ B), тогда как умножение мат­риц обозначается как две
мат­ри­цы бок о бок без какого-либо символа между ними (AB).
  Теперь самое время узнать о механике и интерпретации умножения мат­
риц.


Умножение матриц
Причина, по которой умножение мат­риц допустимо только в том случае, если
число столбцов в левой мат­ри­це соответствует числу строк в правой мат­ри­
це, заключается в том, что (i, j)-й элемент в мат­ри­це произведения является
точечным произведением между i-й строкой левой мат­ри­цы и j-м столбцом
в правой мат­ри­це.
   Уравнение 5.1 показывает пример умножения мат­риц с использованием
тех же двух мат­риц, которые мы использовали для адамарова умножения.
Убедитесь, что вы понимаете, как каждый элемент в мат­ри­це произведения
вычисляется в виде точечных произведений соответствующих строк и столб-
цов мат­риц левой части.

  Уравнение 5.1. Пример умножения мат­риц. Круглые скобки добавлены,
  чтобы облегчить визуальное группирование




   Если вы прилагаете усилия, чтобы запомнить принцип работы умножения
мат­риц, но у вас едва получается, то на рис. 5.4 показан мнемонический трюк
с выведением умножения при помощи пальцев.




                Рис. 5.4  Движения пальцев для умножения мат­риц

  Как интерпретировать умножение мат­риц? Вспомните, что точечное про-
изведение – это число, в котором кодируется взаимосвязь между двумя век-
торами. И таким образом, результатом умножения мат­риц является мат­ри­ца,
в которой хранятся все попарные линейные взаимосвязи между строками
левой мат­ри­цы и столбцами правой мат­ри­цы. Это прекрасная вещь, и она
лежит в основе вычисления мат­риц ковариаций и корреляций, общей ли-
                                             Стандартное умножение матриц  85

нейной модели (используемой в статистическом анализе, включая модели
ANOVA и регрессии), сингулярного разложения и бесчисленного количества
других применений.


Умножение матрицы на вектор
В чисто механическом смысле умножение мат­ри­цы на вектор не представ-
ляет собой ничего особенного и не заслуживает отдельного подраздела: вза-
имное умножение мат­ри­цы и вектора – это просто умножение мат­ри­цы,
в котором одна «мат­ри­ца» является вектором.
   Однако умножение мат­ри­цы и вектора между собой имеет много при-
менений в науке о данных, машинном обучении и компьютерной графике,
поэтому на него стоит потратить немного времени. Давайте начнем с основ.
     Матрицу можно умножить на вектор-столбец, расположенный справа,
       но не на вектор-строку, и ее можно умножить на вектор-строку, рас-
       положенную слева, но не на вектор-столбец. Другими словами, Av и vTA
       допустимы, но AvT и vA не допустимы.
         Это ясно из осмотра размеров мат­ри­цы: мат­ри­цу M×N можно пред-
       позиционно умножить на мат­ри­цу 1×M (также именуемую вектором-
       строкой) либо постпозиционно умножить на мат­ри­цу N×1 (также име-
       нуемую вектором-столбцом).
     Результатом умножения мат­ри­цы на вектор всегда является вектор,
       и ориентация этого вектора зависит от ориентации вектора-сомножи-
       теля: предпозиционное умножение мат­ри­цы на вектор-строку создает
       еще один вектор-строку, тогда как постпозиционное умножение мат­ри­
       цы на вектор-столбец производит еще один вектор-столбец. Опять же,
       это очевидно, если думать о размерах мат­ри­цы, но на это стоит указать.
   Умножение мат­риц на векторы имеет несколько применений. В статистике
предсказываемые моделью значения данных получаются путем умножения
расчетной мат­ри­цы на коэффициенты регрессии, что записывается как Bβ.
В анализе главных компонент выявляется вектор весововых коэффициентов
«важностей признаков», в котором максимизирована дисперсия в наборе дан-
ных Y, и записывается как (YTY)v (вектор важностей признаков v называется
собственным вектором). В многопеременной обработке сигналов размерно-
редуцированная компонента получается путем применения пространствен-
ного фильтра к данным многоканального временного ряда S и записывается
как wTS. В геометрии и компьютерной графике множество координат изобра-
жения можно преобразовывать с использованием мат­ри­цы математического
преобразования, и преобразование записывается как Tp, где T – это мат­ри­ца
преобразования, а p – множество геометрических координат.
   В прикладной линейной алгебре существует еще очень много примеров
применения умножения мат­ри­цы на вектор, и позже в данной книге вы
увидите несколько таких примеров. Умножение мат­ри­цы на вектор также
является основой для пространств мат­риц; с этой важной темой вы позна-
комитесь позже в следующей главе.
86  Матрицы. Часть 1

  А пока мне хотелось бы сосредоточиться на двух конкретных интерпрета-
циях умножения мат­ри­цы на вектор: как средства реализации линейно-взве-
шенных комбинаций векторов и как механизма реализации геометрических
преобразований.

Линейно-взвешенные комбинации
В предыдущей главе мы рассчитывали линейно-взвешенные комбинации,
используя отдельные скаляры и векторы, а затем перемножали их по отдель-
ности. Но теперь вы стали умнее, чем когда начинали предыдущую главу,
и поэтому готовы усвоить более оптимальный, более компактный и масшта-
бируемый метод вычисления линейно-взвешенных комбинаций: помещать
отдельные векторы в мат­ри­цу, а веса – в соответствующие элементы вектора.
И затем умножать. Вот численный пример:




  Пожалуйста, найдите минутку, чтобы проработать умножение, и убеди-
тесь, что понимаете принцип реализации линейно-взвешенной комбинации
двух векторов в виде умножения мат­ри­цы на вектор. Ключевой момент за-
ключается в том, что каждый элемент в векторе умножает соответствующий
столбец в мат­ри­це на скаляр, а затем взвешенные векторы-столбцы сумми-
руются, чтобы получить произведение.
  Данный пример предусматривает линейно-взвешенные комбинации век-
торов-столбцов; что бы вы изменили для вычисления линейно-взвешенных
комбинаций векторов-строк1?

Результаты геометрических преобразований
Когда мы думаем о векторе как о геометрическом отрезке, то умножение
мат­ри­цы на вектор становится способом поворота и шкалирования этого
вектора (вспомните, что умножение скаляра на вектор может шкалировать,
но не поворачивать).
  Ради удобства визуализации давайте начнем с двумерного примера. Вот
наша мат­ри­ца и векторы:
M = np.array([ [2,3], [2,1] ])
x = np.array([ [1, 1.5] ]).T
Mx = M@x

  Обратите внимание, что я создал x как вектор-строку, а затем транспо-
нировал его в вектор-столбец; за счет этого сократилось число квадратных
скобок при наборе исходного кода.

1
    Поместить коэффициенты в вектор-строку и предпозиционно умножить на этот
    вектор.
                                                  Стандартное умножение матриц  87

   График А на рис. 5.5 создает визуализацию этих двух векторов. Хорошо
видно, что мат­ри­ца M одновременно повернула и растянула изначальный
вектор. Давайте попробуем другой вектор с той же матрицей. На самом деле,
просто ради развлечения, давайте использовать те же векторные элементы,
но с переставленными позициями (то есть вектор v = [1.5,1]).
   Теперь на графике B (рис. 5.5) происходит странная вещь: произведение
мат­ри­цы и вектора больше не поворачивается в другом направлении. Матри-
ца по-прежнему прошкалировала вектор, но его направление сохранилось.
Другими словами, умножение мат­ри­цы на вектор действовало так, как если
бы это было умножение вектора на скаляр. И это не случайное событие: на
самом деле вектор v является собственным вектором мат­ри­цы M, а число, на
которое M растянула v, является собственным числом данной мат­ри­цы1. Это
настолько невероятно важное явление, что оно заслуживает отдельной главы
(глава 13), но я просто не мог удержаться, чтобы не познакомить вас с этой
концепцией сейчас.

А)                                           B)




                    Рис. 5.5  Примеры умножения мат­риц на векторы

   Переходя к более сложным темам, основной смысл этих демонстраций –
в том, что одна из функций умножения мат­ри­цы на вектор заключается в том,
что мат­ри­ца содержит преобразование, которое при применении к вектору
может поворачивать и растягивать этот вектор.




1
     Нередко используются синонимичные термины: характеристический вектор и ха-
     рактеристическое число. В английском языке во всех подобных терминах исполь-
     зуется заимствованный из немецкого языка корень eigen, со значением «собствен-
     ный», «характеристический». – Прим. перев.
88  Матрицы. Часть 1


Матричные операции: транспонирование
Вы узнали об операции транспонирования векторов в главе 2. Данный прин-
цип соблюдается и с мат­ри­ца­ми: поменять местами строки и столбцы. И точ-
но так же, как с векторами, транспонирование обозначается надстрочной
буквой T (таким образом, CT – это транспонированная версия мат­ри­цы C).
А двойное транспонирование мат­ри­цы возвращает изначальную мат­ри­цу
(CTT = C).
  Формальное математическое определение операции транспонирования
приведено в уравнении 5.2 (и, по существу, повторяется из предыдущей гла-
вы), но, полагаю, одинаково легко запомнить, что транспонирование меняет
местами строки и столбцы.

    Уравнение 5.2. Определение операции транспонирования

    aTi, j = aj,i.

    Вот пример:




  В Python существует несколько способов транспонирования мат­риц с ис-
пользованием функций и методов, работающих на массивах NumPy:
A = np.array([ [3,4,5], [1,2,3] ])
A_T1 = A.T             # в качестве метода
A_T2 = np.transpose(A) # в качестве функции

   В данном примере в мат­ри­це используется двумерный массив NumPy; что,
по вашему мнению, произойдет, если применить метод транспонирования
к вектору, запрограммированному в виде одномерного массива? Попробуй-
те – и узнаете1!


Обозначение точечного и внешнего
произведений
Теперь, когда вы знакомы с операцией транспонирования и правилами до-
пустимости умножения мат­риц, можно вернуться к обозначению точечного
произведения векторов. Для двух векторов-столбцов M×1 транспонирование
первого вектора, а не второго, дает две «мат­ри­цы» размером 1×M и M×1.
«Внутренние» мерности совпадают, а «внешние» мерности говорят о том, что

1
    Ничего. NumPy вернет тот же одномерный массив, не изменяя его и не выдавая
    предупреждения либо ошибки.
                                                   Симметричные матрицы  89

произведением будет 1×1, то есть скаляр. Это и есть та причина, по которой
точечное произведение указывается в виде aTb.
  То же самое рассуждение будет и для внешнего произведения: умножение
вектора-столбца на вектор-строку имеет размеры M×1 и 1×N. «Внутренние»
мерности совпадают, и размером результата будет M×N.



Матричные операции: LIVE EVIL
(порядок следования операций)
LIVE EVIL – это палиндром1 и симпатичная мнемоника для запоминания
того, как транспонирование влияет на порядок действий при умножении
мат­риц. В сущности, правило заключается в том, что результат транспони-
рования умноженных мат­риц будет одинаков, что и транспонирование и ум-
ножение отдельных мат­риц, но в обратном порядке. В уравнении 5.3 L, I, V
и E являются мат­ри­ца­ми, и для того чтобы сделать умножение допустимым,
исходят из того, что их размеры совпадают.

    Уравнение 5.3. Пример правила LIVE EVIL

    (LIVE)T = ETVTITLT.

  Излишне говорить, что данное правило применимо для умножения любого
числа мат­риц, а не только четырех, и не только с этими «случайно выбран-
ными» буквами.
  Указанное правило действительно кажется странным, но это единствен-
ный способ сделать так, чтобы транспонирование умноженных мат­риц рабо-
тало. У вас будет возможность протестировать его самостоятельно в упраж-
нении 5.7 в конце данной главы. Если хотите, то, перед тем как двигаться
дальше, можете сразу перейти к этому упражнению.



Симметричные матрицы
Симметричные мат­ри­цы имеют целый ряд особых свойств, которые делают
их удобными для работы. Кроме того, они, как правило, обладают численной
стабильностью и, следовательно, удобны для вычислительных алгоритмов.
Работая с данной книгой, вы узнаете об особых свойствах симметричных
мат­риц; здесь я сосредоточусь на вопросах о том, что такое симметричные
мат­ри­цы и как их создавать из несимметричных мат­риц.
  Что значит для мат­ри­цы быть симметричной? Это означает, что соответ-
ствующие строки и столбцы равны. И это означает, что при изменении строк

1
    Палиндромом называется слово или фраза, которые пишутся одинаково в прямом
    и обратном порядке.
90  Матрицы. Часть 1

и столбцов местами с матрицей ничего не происходит. А это, в свою очередь,
означает, что симметричная мат­ри­ца равна ее транспонированной версии.
В математических терминах мат­ри­ца A является симметричной, если AT = A.
   Посмотрите на симметричную мат­ри­цу в уравнении 5.4.

  Уравнение 5.4. Симметричная мат­ри­ца; обратите внимание, что каждая
  строка равна соответствующему ей столбцу




  Может ли неквадратная мат­ри­ца быть симметричной? Нет! Причина в том,
что если мат­ри­ца имеет размер M×N, то ее транспонированная версия имеет
размер N×M. Эти две мат­ри­цы не могут быть равными, за исключением слу-
чая, когда M = N, а это означает, что мат­ри­ца является квадратной.


Создание симметричных матриц
из несимметричных
Возможно, поначалу это покажется удивительным, но умножение любой мат­
ри­цы – даже неквадратной и несимметричной – на ее транспонированную
версию будет приводить к получению квадратной симметричной мат­ри­цы.
Другими словами, ATA является квадратной симметричной, как и AAT. (Если
вам не хватает времени, терпения либо навыков работы с клавиатурой, чтобы
форматировать надстрочную T, то можете писать AtA и AAt либо A¢A и AA¢.)
   Прежде чем рассматривать пример, давайте строго докажем это утверж-
дение. С одной стороны, на самом деле не требуется доказывать отдельно,
что ATA является квадратной и симметричной, потому что из последнего вы-
текает первое. Но доказательство прямоугольности является простым и хо-
рошим упражнением в линейно-алгебраических доказательствах (которые,
как правило, короче и проще, чем, например, доказательства в дифферен-
циальном исчислении).
   Доказательство достигается просто путем рассмотрения размеров мат­
ри­цы: если A имеет размер M×N, то ATA имеет размеры (N×M)(M×N) и, сле-
довательно, мат­ри­ца произведения имеет размер N×N. Ту же логику можно
использовать и для AAT.
   Теперь переходим к доказательству симметрии. Вспомните определение
симметричной мат­ри­цы – это мат­ри­ца, равная своей транспонированной
версии. Итак, давайте транспонируем ATA, задействуем немного алгебры
и посмотрим, что получится. Внимательно проследите каждый приведенный
ниже шаг; доказательство основано на правиле LIVE EVIL:

  (ATA)T = ATATT = ATA.
                                                             Резюме  91

   Беря первый и последний члены, мы получаем (ATA)T = (ATA). Матрица
равна ее транспонированной версии, следовательно, она является симмет­
ричной.
   Теперь повторите доказательство самостоятельно, используя AAT. Внима-
ние, спойлер! Вы придете к тому же выводу. Но написание доказательства
поможет вам усвоить концепцию до мозга костей.
   Таким образом, обе мат­ри­цы, AAT и ATA, являются квадратно-симметрич-
ными. Но это не одна и та же мат­ри­ца! На самом деле если A не является
квадратной, то два матричных произведения даже не имеют одинакового
размера.
   ATA называется мультипликативным методом создания симметричных
мат­риц. Существует также аддитивный метод, который допустим, когда мат­
ри­ца является квадратной, но несимметричной. Этот метод обладает некото-
рыми интересными свойствами, но не имеет большой прикладной ценности,
поэтому я не буду на нем заострять внимания. Упражнение 5.9 познакомит
вас с алгоритмом; если вы готовы к испытанию, то, прежде чем приступать
к выполнению упражнения, можете попробовать разработать этот алгоритм
самостоятельно.



Резюме
Данная глава является первой в серии из трех глав, посвященных мат­ри­цам.
Здесь вы ознакомились с основой, на которой базируются все матричные
операции. Вкратце:
    Матрицы – это развернутые таблицы чисел. В различных приложениях
      их удобно концептуализировать в уме в виде множества векторов-
      столбцов, множества векторов-строк либо некоей упорядоченности
      отдельных значений. Как бы то ни было, визуализация мат­риц в виде
      изображений нередко бывает информативной либо, по меньшей мере,
      приятной на вид.
    Существует несколько категорий специальных мат­риц. Знакомство со
      свойствами типов мат­риц поможет вам разобраться в матричных урав-
      нениях и продвинутых приложениях.
    Некоторые арифметические операции выполняются поэлементно, та-
      кие как сложение, умножение на скаляр и адамарово умножение.
    «Сдвиг» мат­ри­цы означает добавление константы к диагональным эле-
      ментам (без изменения внедиагональных элементов). Сдвиг имеет не-
      сколько применений в машинном обучении, в первую очередь для отыс­
      кания собственных чисел и регуляризации статистических моделей.
    Умножение мат­риц предусматривает точечные произведения между
      строками левой мат­ри­цы и столбцами правой мат­ри­цы. Матрица про-
      изведения представляет собой организованный набор соотнесений
      между парами строк и столбцов. Запомните правило проверки пра-
      вильности умножения мат­риц: (M×N)(N×K) = (M×K).
92  Матрицы. Часть 1

      LIVE EVIL1,2: транспонирование умноженных мат­риц равно транспо-
        нированным и умноженным отдельным мат­ри­цам с обратным поряд-
        ком следования.
      Симметричные мат­ри­цы зеркально отражаются по диагонали, то есть
        каждая строка равна соответствующим ей столбцам, и определяются
        как A = AT. Симметричные мат­ри­цы обладают многими интересными
        и полезными свойствами, которые делают их удобными для работы
        в приложениях.
      Симметричная мат­ри­ца создается из любой мат­ри­цы, умножая эту
        мат­ри­цу на ее транспонированную версию. Результирующая мат­ри­ца
        ATA занимает центральное место в статистических моделях и сингу-
        лярном разложении.



Упражнения по программированию
  Упражнение 5.1
  Это упражнение поможет вам ознакомиться с индексацией элементов
мат­ри­цы. Создайте мат­ри­цу 3×4, используя np.arange(12).reshape(3,4). За-
тем напишите исходный код на Python, чтобы извлечь элемент во второй
строке, четвертом столбце. Используйте мягкое программирование, чтобы
выбирать разные индексы строк/столбцов. Распечатайте сообщение, подоб-
ное следую­щему ниже:
Элемент мат­ри­цы по индексу (2,4) равен 7.

  Упражнение 5.2
  Это и следующее упражнения сосредоточены на нарезке мат­риц с целью
получения подматриц. Начните с создания мат­ри­цы C на рис. 5.6 и при-
мените существующую в Python операцию нарезки, чтобы извлечь подмат­
рицу, состоящую из первых пяти строк и пяти столбцов. Давайте назовем
эту мат­ри­цу C1. Попробуйте воспроизвести рис. 5.6, но если у вас возникли
проблемы с программированием визуализации на Python, то просто сосре-
доточьтесь на правильном извлечении подматрицы.

  Упражнение 5.3
  Расширьте этот исходный код, чтобы извлечь остальные четыре блока
размером 5×5. Затем создайте новую мат­ри­цу с этими блоками, которые
переставлены местами в соответствии с рис. 5.7.



1
    LIVE EVIL – это милая мнемотехника, а не рекомендация о том, как вести себя
    в обществе!
2
    Для справки: LIVE EVIL (Зло в натуре, Зло живьем) – это концертный альбом хеви-
    метал группы Black Sabbath, выпущенный в 1982 году. – Прим. перев.
                                          Упражнения по программированию  93




                    Рис. 5.6  Визуализация упражнения 5.2




                    Рис. 5.7  Визуализация упражнения 5.3


   Упражнение 5.4
   Реализуйте поэлементное сложение мат­риц, используя два цикла for по
строкам и столбцам. Что происходит, когда вы пытаетесь сложить две мат­ри­
цы с несовпадающими размерами? Это упражнение поможет вам подумать
о разбиении мат­ри­цы на строки, столбцы и отдельные элементы.

  Упражнение 5.5
  Сложение мат­риц и умножение мат­ри­цы на скаляр подчиняются матема-
тическим законам коммутативности и дистрибутивности. Это означает, что
94  Матрицы. Часть 1

следующие ниже уравнения дают одинаковые результаты (допустим, что
мат­ри­цы A и B имеют одинаковый размер и что σ является неким скаляром):

    σ(A + B) = σA + σB = Aσ + Bσ.

  Вместо того чтобы доказывать это уравнение математически, вы проде-
монстрируете это с помощью программирования. Создайте на Python две
мат­ри­цы случайных чисел размером 3×4 и случайный скаляр. Затем реа-
лизуйте три выражения в приведенном выше уравнении. Вам нужно будет
найти способ подтвердить эквивалентность трех результатов. Имейте в виду,
что крошечные ошибки вычислительной прецизионности1 в диапазоне 10–15
следует игнорировать.

  Упражнение 5.6
  Запрограммируйте умножение мат­риц с использованием циклов for. Под-
твердите свои результаты с помощью оператора @ библиотеки NumPy. Это
упражнение поможет вам укрепить ваше понимание умножения мат­риц, но
на практике всегда лучше использовать @ вместо написания двойного цикла
for.

    Упражнение 5.7
    Подтвердите правило LIVE EVIL, выполнив следующие ниже пять шагов.
    1.	Создайте четыре мат­ри­цы случайных чисел, установив размеры L Î
        ℝ2×6, I Î ℝ6×3, V Î ℝ3×5 и E Î ℝ5×2.
    2. Умножьте четыре мат­ри­цы и транспонируйте произведение.
    3.	Транспонируйте каждую мат­ри­цу по отдельности и умножьте их, не
        меняя их порядок следования.
    4.	Транспонируйте каждую мат­ри­цу по отдельности и умножьте их в об-
        ратном порядке в соответствии с правилом LIVE EVIL. Проверьте со-
        впадение результата шага 2 с результатами шага 3 и шага 4.
    5.	Повторите приведенные выше шаги, но используя только квадратные
        мат­ри­цы.

  Упражнение 5.8
  В этом упражнении вы напишете функцию Python, которая выполняет
проверку мат­ри­цы на симметричность. На входе она должна принимать мат­
ри­цу и на выходе выводить булево значение True, если мат­ри­ца является
симметричной, либо False, если мат­ри­ца является несимметричной. Имейте
в виду, что малые ошибки вычислительного округления/прецизионности
могут создавать впечатление, что «равные» мат­ри­цы являются неравными.
Следовательно, вам нужно будет выполнять проверку на равенство с неко-
торой разумной терпимостью. Протестируйте функцию на симметричных
и несимметричных матрицах.

1
    Для справки: метрика точности (accuracy) измеряет степень отклонения от целе-
    вого показателя, по сути являясь метрикой ширины отклонения, а метрика пре-
    цизионности (precision) измеряет степень глубины (резкости) измеряемой вели-
    чины. – Прим. перев.
                                                  Упражнения по программированию  95

  Упражнение 5.9
  Я упоминал, что существует аддитивный метод создания симметрич-
ной мат­ри­цы из несимметричной квадратной мат­ри­цы. Указанный метод
довольно прост: усреднить мат­ри­цу посредством ее транспонированной
версии. Реализуйте этот алгоритм на Python и подтвердите, что результат
действительно является симметричным. (Подсказка: используйте функцию,
которую вы написали в предыдущем упражнении!)

  Упражнение 5.10
  Повторите вторую часть упражнения 3.3 (два вектора в ℝ3), но используйте
умножение мат­ри­цы на вектор вместо умножения вектора на скаляр. То есть
вычислите As вместо σ1v1 + σ2v2.

   Упражнение 5.11
   Диагональные мат­ри­цы обладают многими интересными свойствами, ко-
торые делают их полезными для работы. В данном упражнении вы узнаете
о двух из этих свойств:
     предпозиционное умножение диагональной мат­ри­цы на мат­ри­цу-
       сомножитель шкалирует строки правой мат­ри­цы на соответствующие
       диагональные элементы;
     постпозиционное умножение диагональной мат­ри­цы на мат­ри­цу-
       сомножитель шкалирует столбцы левой мат­ри­цы на соответствующие
       диагональные элементы.
   Этот факт используется в нескольких приложениях, включая вычисление
мат­риц корреляций (глава 7) и диагонализацию мат­риц (главы 13 и 14).
   Давайте обследуем последствия этих свойств. Начнем с создания трех мат­
риц 4×4: мат­ри­цы из одних единиц (подсказка: np.ones()); диагональной
мат­ри­цы, в которой диагональные элементы равны 1, 4, 9 и 16; и диагональ-
ной мат­ри­цы, равной квадратному корню из предыдущей диагональной
мат­ри­цы.
   Далее распечатайте мат­ри­цу единиц, пред- и постпозиционно умножен-
ную на первую диагональную мат­ри­цу-сомножитель. Вы получите следую-
щие ниже результаты:
# Предпозиционно умножить на диагональную мат­ри­цу:
[[ 1. 1. 1. 1.]
 [ 4. 4. 4. 4.]
 [ 9. 9. 9. 9.]
 [16. 16. 16. 16.]]

# Постпозиционно умножить на диагональную мат­ри­цу:
[[ 1. 4. 9. 16.]
 [ 1. 4. 9. 16.]
 [ 1. 4. 9. 16.]
 [ 1. 4. 9. 16.]]

  Наконец, предпозиционно и постпозиционно умножьте мат­ри­цу единиц
на мат­ри­цу квадратных корней из диагональной мат­ри­цы. Вы получите сле-
дующее ниже:
96  Матрицы. Часть 1

# Пред- и постпозиционно умножить
# на квадратично-диагональную мат­ри­цу
[[ 1. 2. 3. 4.]
 [ 2. 4. 6. 8.]
 [ 3. 6. 9. 12.]
 [ 4. 8. 12. 16.]]

  Обратите внимание, что строки и столбцы прошкалированы таким об-
разом, что (i, j)-й элемент в мат­ри­це умножается на произведение i-го и j-го
диагональных элементов. (На самом деле мы создали таблицу умножения!)

  Упражнение 5.12
  Еще один забавный факт: умножение мат­риц – это то же самое, что и ада-
марово умножение двух диагональных мат­риц. Подумайте, почему так, ис-
пользуя бумагу и карандаш с двумя диагональными мат­ри­ца­ми 3×3, а затем
проиллюстрируйте это в исходном коде на Python.
Глава       6
                         Матрицы. Часть 2

Умножение мат­риц представляет собой один из самых замечательных да-
ров, которыми наделили нас математики. Но для того чтобы перейти от
элементарной линейной алгебры к продвинутой – а затем понимать и раз-
рабатывать алгоритмы науки о данных, – нужно делать больше, чем просто
умножать мат­ри­цы.
  Мы начинаем эту главу с изложения норм мат­риц и пространств мат­риц.
Нормы мат­риц, по сути, являются расширением норм векторов, а простран-
ства мат­риц, по сути, являются расширением подпространств векторов (под-
пространства векторов, в свою очередь, являются не чем иным, как линейно-
взвешенными комбинациями). Так что у вас уже есть необходимые базовые
знания для этой главы.
  Такие понятия, как линейная независимость, ранг и определитель, позво-
лят перейти от понимания элементарных понятий, таких как транспониро-
вание и умножение, к пониманию сложных тем, таких как обратная мат­ри­ца,
собственные числа и сингулярные числа. И эти продвинутые темы раскрыва-
ют возможности линейной алгебры для применений в науке о данных. Таким
образом, эта глава является отправной точкой в вашей трансформации из
новичка в линейной алгебре в ее знатока.
  Внешне мат­ри­цы выглядят очень простыми – просто развернутой табли-
цей чисел. Но в предыдущих главах вы уже увидели, что в матрицах кроет-
ся нечто большее, чем кажется на первый взгляд. Итак, сделайте глубокий
и успокаивающий вдох и погружайтесь в тему.



Нормы матриц
Вы познакомились с нормами векторов в главе 2: норма вектора – это его
евклидова геометрическая длина, которая вычисляется как квадратный ко-
рень из суммы квадратов элементов вектора.
  Нормы мат­риц немного сложнее. Прежде всего не существует «единствен-
ной в своем роде нормы мат­ри­цы»; из мат­ри­цы можно вычислить много-
численные отличимые нормы. Нормы мат­риц в чем-то похожи на нормы
векторов – в том смысле, что каждая норма содержит одно характеризующее
98  Матрицы. Часть 2

мат­ри­цу число и что норма обозначается двойными вертикальными линия-
ми, как в норме мат­ри­цы A, которая обозначается как ||A||.
   Но разные нормы мат­риц обладают разными смыслами. Громадное число
норм мат­риц в общих чертах можно разделить на два семейства: поэле-
ментные (также иногда именуемые матричными нормами «по входам»1)
и индуцированные. Поэлементные нормы вычисляются на основе отдельных
элементов мат­ри­цы, и, следовательно, эти нормы интерпретируются как от-
ражение ими величин элементов в мат­ри­це.
   Индуцированные нормы интерпретируются следующим образом: одной
из функций мат­ри­цы является кодирование преобразования вектора; ин-
дуцированная норма мат­ри­цы является мерой того, насколько это преоб-
разование масштабирует (растягивает или сжимает) этот вектор. Данная
интерпретация будет иметь больше смысла в главе 7, когда вы узнаете о при-
менении мат­риц для геометрических преобразований, и в главе 14, когда вы
узнаете о сингулярном разложении.
   В данной главе я познакомлю вас с поэлементными нормами, а начну
с евклидовой нормы, которая на самом деле является прямым расширением
векторной нормы на мат­ри­цы. Евклидова норма также называется нормой
Фробениуса и вычисляется как квадратный корень из суммы всех элементов
мат­ри­цы в квадрате (уравнение 6.1).

    Уравнение 6.1. Норма Фробениуса




  Индексы i и j соответствуют M строкам и N столбцам. Также обратите вни-
мание на подстрочную букву F, указывающую на норму Фробениуса.
  Норма Фробениуса еще называется нормой ℓ2 (ℓ – это причудливо вы-
глядящая буква L). А норма ℓ2 получила свое название от общей формулы
поэлементных -норм (обратите внимание, что норму Фробениуса можно
получить при p = 2):




  Нормы мат­р иц имеют несколько применений в машинном обучении
и статистическом анализе. Одним из важных применений является регу-
ляризация, целью которой является улучшение подгонки моделей и по-
вышение обобщаемости моделей на данные, которые в модель ранее не
подавались (позже в книге вы увидите соответствющие примеры). Базовая
идея регуляризации состоит в добавлении матричной нормы в качестве
стоимостной функции в алгоритм минимизации. Эта норма не дает мо-
дельным параметрам становиться слишком большими (регуляризация ℓ2,
также именуемая гребневой регрессией) либо не поощряет разреженные ре-

1
    Англ. entry-wise. – Прим. перев.
                                                           Нормы матриц  99

шения (регуляризация ℓ1, также именуемая лассо-регрессией). По сути дела,
современные архитектуры глубокого обучения достигают впечатляющей
результативности в решении задач компьютерного зрения, опираясь на
матричные нормы.
   Еще одним применением нормы Фробениуса является вычисление меры
«матричного расстояния». Расстояние между матрицей и самой собой равно
0, а расстояние между двумя разными мат­ри­ца­ми увеличивается по мере
того, как числовые значения в этих матрицах становятся все более непохо-
жими. Матричное расстояние Фробениуса вычисляется просто путем замены
мат­ри­цы A на мат­ри­цу C = A – B в уравнении 6.1.
   Указанное расстояние можно использовать в качестве критерия оптими-
зации в алгоритмах машинного обучения, например чтобы уменьшать раз-
мер хранилища изображений, минимизируя расстояние Фробениуса между
уменьшенной и изначальной мат­ри­ца­ми. Упражнение 6.2 проведет вас по
простому примеру минимизации.


След матрицы и норма Фробениуса
След мат­ри­цы – это сумма ее диагональных элементов, обозначаемая как
tr(A), и существует он только для квадратных мат­риц. Обе следующие ниже
мат­ри­цы имеют одинаковый след (14):




  След обладает несколькими интересными свойствами. Например, след
мат­ри­цы равен сумме собственных чисел1 мат­ри­цы и, следовательно, яв-
ляется мерой «объема» собственного пространства2. Многие свойства следа
менее подходят для применения в науке о данных, но вот одно интересное
исключение:




  Другими словами, норму Фробениуса можно вычислять как квадратный
корень из следа мат­ри­цы, умноженной на ее транспонированную версию.
Причина, по которой это работает, состоит в том, что каждый диагональный
элемент мат­ри­цы ATA определяется точечным произведением каждой стро-
ки на саму себя.
  Упражнение 6.3 поможет вам обследовать следовой метод вычисления
нормы Фробениуса.


1
    Англ. eigenvalues. – Прим. перев.
2
    Син. характеристическое пространство. – Прим. перев.
100  Матрицы. Часть 2


Пространства матрицы (столбцовое, строчное,
нуль-пространство)
Концепция пространств мат­ри­цы занимает центральное место во многих
темах абстрактной и прикладной линейной алгебры. К счастью, пространства
мат­ри­цы обладают концептуальной простотой и, по сути, представляют со-
бой просто линейно-взвешенные комбинации разных признаков мат­ри­цы.


Столбцовое пространство
Напомню, что линейно-взвешенная комбинация векторов предусматривает
умножение на скаляр и суммирование множества векторов. Две модифика-
ции этой концепции будут расширять линейно-взвешенную комбинацию
до столбцового пространства мат­ри­цы. Во-первых, мы концептуализируем
мат­ри­цу в виде множества векторов-столбцов. Во-вторых, вместо того чтобы
работать с определенным множеством скаляров, мы рассматриваем бес-
конечность действительно-значных скаляров. Бесконечное число скаляров
дает бесконечное число способов комбинирования множества векторов. Это
результирующее бесконечное множество векторов называется столбцовым
пространством мат­ри­цы.
  Давайте конкретизируем все это с помощью нескольких числовых приме-
ров. Мы начнем с простого – с мат­ри­цы, которая имеет только один столбец
(что на самом деле то же самое, что и вектор-столбец). Ее столбцовое про-
странство – все возможные линейно-взвешенные комбинации этого столб-
ца – можно выразить следующим образом:


                 λ Î ℝ.


  C(A) обозначает столбцовое пространство мат­ри­цы A, а символ Î обозна-
чает принадлежность к множеству. В данном контексте это означает, что λ
может быть любым возможным действительно-значным числом.
  Что означает указанное математическое выражение? Оно означает, что
столбцовое пространство является множеством всех возможных шкалиро-
ванных версий вектора-столбца [1 3]. Давайте рассмотрим несколько кон-
кретных случаев. Находится ли вектор [1 3] в столцовом пространстве? Да,
потому что этот вектор можно выразить как мат­ри­цу, умноженную на λ = 1.
Как быть с [2 –6]? Тоже да, потому что этот вектор можно выразить как мат­
ри­цу, умноженную на λ = –2. А что насчет [1 4]? Здесь ответ – нет: вектор [1 4]
не находится в столбцовом пространстве мат­ри­цы, потому что просто нет
скаляра, который мог бы умножить мат­ри­цу, чтобы произвести этот вектор.
  Как выглядит столбцовое пространство? Для мат­ри­цы с одним столбцом
столбцовое пространство представляет собой линию, которая проходит че-
рез начало координат в направлении вектора-столбца и простирается до
               Пространства матрицы (столбцовое, строчное, нуль-пространство)  101

бесконечности в обоих направлениях. (В техническом плане линия не рас-
тягивается до буквальной бесконечности, потому что бесконечность не яв-
ляется вещественным числом. Но эта линия имеет сколь угодно большую
длину – намного длиннее, чем наш ограниченный человеческий разум может
представить, – поэтому во всех отношениях и с любой точки зрения об этой
линии можно говорить как о бесконечно длинной.) На рис. 6.1 показано изо-
бражение столбцового пространства этой мат­ри­цы.




    Рис. 6.1  Визуализация столбцового пространства мат­ри­цы с одним столбцом.
      Данное столбцовое пространство является одномерным подпространством

  Теперь давайте рассмотрим мат­ри­цу с бóльшим числом столбцов. Мы
оставим размерность столбца равной двум, чтобы иметь возможность визуа­
лизировать его на двумерном графике. Вот наша мат­ри­ца и ее столбцовое
пространство:


                             λ Î ℝ.


  У нас два столбца, поэтому допускаем два отдельных λ (оба являются дей-
ствительно-значными числами, но могут отличаться друг от друга). Теперь
вопрос состоит в том, каким будет множество всех векторов, которое может
быть достигнуто некоторой линейной комбинацией этих двух векторов-
столбцов.
  Ответ таков: все векторы в ℝ2. Например, вектор [–4 3] можно получить
путем шкалирования двух столбцов соответственно на 11 и –15. Как я до-
думался до этих скалярных значений? Я использовал проекцию методом
наименьших квадратов, о которой вы узнаете в главе 11. А пока вы можете
102  Матрицы. Часть 2

сосредоточиться на концепции, согласно которой эти два столбца могут быть
надлежаще взвешенны, чтобы достичь любой точки в ℝ2.
  График А на рис. 6.2 показывает два столбца мат­ри­цы. Я не нарисовал
столбцовое пространство мат­ри­цы, потому что оно будет всей осью целиком.




            Рис. 6.2  Дополнительные примеры столбцовых пространств

  Еще один пример в ℝ2. Вот новая мат­ри­ца, которую мы рассмотрим:


                            λ Î ℝ.


   Какова размерность ее столбцового пространства? Можно ли достичь лю-
бой точки в ℝ2 с помощью некоторой линейно-взвешенной комбинации двух
столбцов?
   Ответ на второй вопрос – нет. И если вы мне не верите, то попробуйте
найти линейно-взвешенную комбинацию двух столбцов, которая создает
вектор [3 5]. Это просто невозможно. Два столбца фактически являются
коллинеарными (график В на рис. 6.2), потому что один уже является шка-
лированной версией другого. Это означает, что столбцовое пространство
этой мат­р и­ц ы 2×2 по-прежнему остается просто линией – одномерным
подпространством.
   Из этого можно вынести один вывод: наличие N столбцов в мат­ри­це не га-
рантирует, что столбцовое пространство будет -мерным. Размерность столб-
цового пространства равна числу столбцов только в том случае, если столбцы
образуют линейно независимое множество. (Вспомните главу 3, в которой
говорилось, что линейная независимость означает множество векторов,
в котором ни один вектор невозможно выразить как линейно-взвешенную
комбинацию других векторов в этом множестве.)
              Пространства матрицы (столбцовое, строчное, нуль-пространство)  103

  Заключительный пример столбцовых пространств служит для того, чтобы
увидеть, что происходит, когда мы переходим в три измерения.
  Вот наша мат­ри­ца и ее столбцовое пространство:


                            λ Î ℝ.


  Теперь в ℝ3 два столбца. Эти два столбца линейно независимы, и, значит,
выразить один как шкалированную версию другого невозможно. Таким об-
разом, столбцовое пространство этой мат­ри­цы является двумерным, но оно
является двумерной плоскостью, которая вложена в ℝ3 (рис. 6.3).
  Столбцовое пространство этой мат­ри­цы представляет собой бесконечную
двумерную плоскость, но указанная плоскость является всего лишь бесконеч-
но малым срезом трех измерений. Это можно представить как бесконечно
тонкий лист бумаги, который охватывает Вселенную.




             Рис. 6.3  Двумерное столбцовое пространство мат­ри­цы,
                          встроенной в три измерения.
               Две толстые линии изображают два столбца мат­ри­цы

  На этой плоскости есть много векторов (т. е. много векторов, которые
можно получить как линейную комбинацию двух векторов-столбцов), но еще
больше векторов, которые не находятся на плоскости. Другими словами, есть
векторы в столбцовом пространстве мат­ри­цы, и есть векторы вне столбцо-
вого пространства мат­ри­цы.
  Как узнать, что вектор находится в столбцовом пространстве мат­ри­цы?
Это вовсе не тривиальный вопрос – на самом деле данный вопрос лежит
104  Матрицы. Часть 2

в основе линейного метода наименьших квадратов, и просто невозможно
передать словами важность наименьших квадратов в прикладной матема-
тике и инженерии. Итак, как определить, что вектор находится в простран-
стве столбцов? В приводимых до сих пор примерах мы просто использовали
некоторые догадки, арифметику и визуализацию. Смысл этого подхода со-
стоял в том, чтобы привить интуитивное понимание, но эти методы, оче-
видно, не масштабирутся на более высокие измерения и на более сложные
задачи.
   Количественные методы определения присутствия вектора в столбцовом
пространстве мат­ри­цы основаны на концепции ранга мат­ри­цы, о котором
вы узнаете позже в этой главе. А пока сосредоточьтесь на интуитивном пони-
мании, что столбцы мат­ри­цы образуют векторное подпространство, которое
может включать в себя все M-мерное пространство либо может быть неко-
торым подпространством меньшей мерности, и что важный вопрос состоит
в том, не находится ли какой-либо другой вектор внутри этого подпростран-
ства (это означает, что вектор можно выразить как линейно-взвешенную
комбинацию столбцов мат­ри­цы).


Строчное пространство
После того как вы разберетесь в столбцовом пространстве мат­ри­цы, понять
строчное пространство будет реально легко. Строчное пространство мат­
ри­цы – это, по сути, точно такая же концепция, однако вместо столбцов мы
рассматриваем все возможные взвешенные комбинации строк.
  Строчное пространство обозначается как R(A). И поскольку операция
транспонирования меняет местами строки и столбцы, можно также записать,
что строчное пространство мат­ри­цы является столбцовым пространством
транспонированной мат­ри­цы, другими словами, R(A) = C(AT). Между строч-
ным и столбцовым пространствами мат­ри­цы есть несколько различий; на-
пример, строчное пространство (но не столбцовое пространство) инвариант-
но к операциям приведения строк. Но это выходит за рамки данной главы.
  Поскольку строчное пространство равно столбцовому пространству транс-
понированой мат­ри­цы, эти два матричных пространства идентичны для
симметричных мат­риц.


Нуль-пространства
Нуль-пространство незначительно, но важно отличается от столбцового про-
странства. Столбцовое пространство можно кратко свести к следующему
ниже уравнению:

  Ax = b.

  Оно переводится на естественный так: «можно ли найти некоторый набор
коэффициентов в x такой, что взвешенная комбинация столбцов в A произ-
               Пространства матрицы (столбцовое, строчное, нуль-пространство)  105

водила бы вектор b?» Если ответ – да, то b Î C(A), и вектор x говорит о том,
как взвешивать столбцы A, чтобы добраться до b.
  Нуль-пространство, напротив, можно кратко свести к следующему ниже
уравнению:

  Ay = 0.

  Оно переводится на естественный так: «можно ли найти некоторый набор
коэффициентов в y такой, что взвешенная комбинация столбцов в A давала
бы вектор нулей 0?»
  Мгновенный осмотр покажет ответ, который действует для любой возмож-
ной мат­ри­цы A: установить y = 0! Очевидно, что умножение всех столбцов
на нули приведет к получению вектора нулей. Но это решение будет триви-
альным, и мы его исключаем. Следовательно, возникает вопрос: «можно ли
найти множество весов – не все из которых равны 0, – которое производит
вектор нулей?» Любой вектор y, который может удовлетворять этому урав-
нению, находится в нуль-пространстве A, которое мы записываем как N(A).
  Давайте начнем с простого примера. Прежде чем читать следующий далее
текст, посмотрим, сможете ли вы найти такой вектор y:




  Вы придумали вектор? Мой будет таким: [7.34, 7.34]. Готов держать пари
размером с Лас-Вегас, что вы не придумали тот же вектор. Возможно, вы
придумали [1, 1] или [–1, –1]. А может быть, [2, 2]?
  Думаю, вы видите, к чему это ведет – существует бесконечное число векто-
ров y, которые удовлетворяют Ay = 0 для этой конкретной мат­ри­цы A. И все
эти векторы можно выразить в виде некоторой шкалированной версии лю-
бого из этих вариантов. Таким образом, нуль-пространство данной мат­ри­цы
можно выразить как

                λ Î ℝ.


  Вот еще один пример мат­ри­цы. Опять же, попробуйте найти набор коэф-
фициентов такой, что взвешенная сумма столбцов давала бы вектор нулей
(то есть найдите y в Ay = 0):




  Готов сделать еще более высокую ставку на то, что вы не сможете найти
такой вектор. Но это не потому, что я не верю в ваши способности (я очень
высокого мнения о своих читателях!), а потому, что в этой мат­ри­це нет нуль-
пространства. Формально мы говорим, что нуль-пространство этой мат­ри­цы
является пустым множеством: N(A) = {}.
106  Матрицы. Часть 2

   Взгляните на два примера мат­риц в этом подразделе еще раз. Вы заметите,
что первая мат­ри­ца содержит столбцы, которые могут быть сформированы
как шкалированные версии других столбцов, тогда как вторая мат­ри­ца со-
держит столбцы, которые образуют независимое множество. Это вовсе не
совпадение: между размерностью нуль-пространства и линейной незави-
симостью столбцов в мат­ри­це существует тесная взаимосвязь. Точная при-
рода этой взаимосвязи задается теоремой о ранге и нульности1, о которой
вы узнаете в дополнении А к книге. Но их ключевой момент заключается
в следующем: нуль-пространство является пустым, когда столбцы мат­ри­цы
образуют линейно независимое множество.
   Рискуя показаться избыточным, повторю вот этот важный момент: полно-
ранговые мат­ри­цы и мат­ри­цы с полным столбцовым рангом имеют пустые
нуль-пространства, тогда как рангово-пониженные мат­ри­цы имеют непус­
тые (нетривиальные) нуль-пространства.
   Библиотека SciPy в Python содержит функцию, которая вычисляет нуль-
пространство мат­ри­цы. Давайте подтвердим наши результаты с помощью
исходного кода:
A = np.array([ [1,-1], [-2,2] ])
B = np.array([ [1,-1], [-2,3] ])

print( scipy.linalg.null_space(A) )
print( scipy.linalg.null_space(B) )
Вот результат:
[[0.70710678]
 [0.70710678]]

[]

   Второй результат ([]) – это пустое множество. Почему Python выбрал для
нуль-пространства A числовые значения 0.70710678? Разве не было бы лег-
че читать, если бы Python выбрал 1? Учитывая бесконечность возможных
векторов, Python вернул единичный вектор2 (норму этого вектора можно
вычислить в уме, зная, что       » .7071). Единичные векторы удобны в ра-
боте и обладают несколькими приятными свойствами, включая численную
стабильность. Поэтому компьютерные алгоритмы часто возвращают еди-
ничные векторы в качестве базисов подпространств. Вы увидите это снова
с собственными векторами и сингулярными векторами.
   Как выглядит нуль-пространство? На рис. 6.4 показаны векторы-строки
и нуль-пространство мат­ри­цы A.
   Почему я вывел на график векторы-строки вместо векторов-столбцов?
Оказывается, что строчное пространство расположено ортогонально нуль-

1
     Англ. rank-nullity theorem. См. https://en.wikipedia.org/wiki/Rank-nullity_theorem. –
     Прим. перев.
2
     То есть с векторной величиной, равной единице. – Прим. перев.
                Пространства матрицы (столбцовое, строчное, нуль-пространство)  107

пространству. Это не по какой-то причудливой эзотерической причине; как
раз наоборот, это зашито прямо в определение нуль-пространства как Ay = 0.
Переписывание этого уравнения для каждой строки мат­ри­цы (ai) приводит
к выражению ai y = 0; другими словами, точечное произведение между каж-
дой строкой и нуль-пространственным вектором равно 0.




                  Рис. 6.4  Визуализация нуль-пространства мат­ри­цы

  К чему весь этот сыр-бор по поводу нуль-пространств? Излишнее вни-
мание векторам, которые могут умножать мат­ри­цу, чтобы получать вектор
нулей, вполне может показаться странным. Однако, как вы узнаете в главе 13,
нуль-пространство является краеугольным камнем в отыскании собствен-
ных и сингулярных векторов.
  Заключительная мысль данного раздела такова: каждая мат­ри­ца имеет
четыре ассоциированных подпространства; вы узнали о трех (столбцовом,
строчном, нуль-пространстве). Четвертое подпространство называется пра-
вым нуль-пространством1 и представляет собой строчное нуль-пространство.
Оно нередко записывается как нуль-пространство транспонированной мат­
ри­цы: N(AT). Традиционная учебная программа по математике теперь потра-
тила бы несколько недель на обследование тонкостей и взаимосвязей между
четырьмя подпространствами. Матричные подпространства заслуживают
изучения из-за их завораживающей красоты и совершенства, но на такой
уровень глубины мы погружаться не будем.

1
    Существует два нуль-пространства: левое и правое. Когда пишут просто «нуль-
    пространство», то имеют в виду левое нуль-пространство. – Прим. перев.
108  Матрицы. Часть 2


Ранг
Ранг – это число, ассоциированное с матрицей. Оно связано с размерностями
матричных подпространств и имеет важные последствия для матричных
операций, включая инвертирование мат­риц и определение числа решений
системы уравнений.
   Как и в случае с другими темами в этой книге, существуют богатые и под-
робные теории ранга мат­ри­цы, но здесь я сосредоточусь на том, что вам
нужно знать для науки о данных и связанных с ней применений.
   Начну с перечисления нескольких свойств ранга. Без какого-либо опреде-
ленного порядка важности:
     ранг представляет собой неотрицательное целое число, поэтому мат­
       ри­ца может иметь ранг 0, 1, 2, ..., но не −2 или 3.14;
     каждая мат­ри­ца имеет один уникальный ранг; мат­ри­ца не может одно-
       временно иметь несколько отдельных рангов (Это также означает, что
       ранг является характеристикой мат­ри­цы, а не строк или столбцов.);
     ранг мат­ри­цы указывается с использованием r(A) или rank(A). Также
       уместно говорить, что «A является -ранговой матрицей»;
     максимально возможный ранг мат­ри­цы – это меньшее из числа ее
       строк либо столбцов. Другими словами, максимально возможный ранг
       равен min{M, N};
     мат­ри­ца с максимально возможным рангом называется «полноранго-
       вой». Матрица с рангом r < min{M, N} по-разному называется: «ранго-
       во-пониженной», «рангово-дефицитной» или «сингулярной»;
     скалярное умножение не влияет на ранг мат­ри­цы (за исключением 0,
       который преобразовывает мат­ри­цу в мат­ри­цу нулей с рангом 0).
   Существует несколько эквивалентных интерпретаций и определений ран-
га мат­ри­цы. К ним относятся:
     наибольшее число столбцов (или строк), которые образуют линейно
       независимое множество;
     размерность столбцового пространства (которая совпадает с размер-
       ностью строчного пространства);
     число измерений, содержащих информацию в мат­ри­це. Оно не со-
       впадает с суммарным числом столбцов или строк в мат­ри­це из-за воз-
       можных линейных зависимостей;
     количество ненулевых сингулярных чисел мат­ри­цы.
   Возможно, покажется удивительным, что определение ранга выглядит
одинаково для столбцов и строк: действительно ли размерность одинакова
для столбцового и строчного пространств, даже для неквадратной мат­ри­цы?
Да, так и есть. Существуют различные тому доказательства, многие из кото-
рых либо достаточно сложны, либо основаны на сингулярном разложении,
поэтому я не буду включать формальное доказательство в эту главу1. Но в ка-

1
    Если вы уже знакомы с сингулярным разложением (SVD), то короткая версия за-
    ключается в том, что SVD-разложение мат­риц A и AT меняет местами строчное
    и столбцовое пространства, но количество ненулевых сингулярных чисел остается
    прежним.
                                                                         Ранг  109

честве иллюстрации покажу пример строчного и столбцового пространств
неквадратной мат­ри­цы.
  Вот наша мат­ри­ца:




  Столбцовое пространство мат­ри­цы находится в ℝ2, тогда как строчное
пространство находится в ℝ3, поэтому эти два пространства необходимо
выводить на разных графиках (рис. 6.5). Три столбца не образуют линейно
независимого множества (любой один столбец можно описать как линейную
комбинацию двух других), но они действительно охватывают все ℝ2. Следо-
вательно, столбцовое пространство мат­ри­цы – двумерное. Две строки дей-
ствительно образуют линейно независимое множество, и подпространство,
которое они охватывают, является двумерной плоскостью в ℝ3.
  Для ясности: столбцовое и строчное пространства мат­ри­цы различны. Но
размерность этих пространств мат­ри­цы совпадает. И эта размерность явля-
ется рангом мат­ри­цы. Таким образом, данная мат­ри­ца имеет ранг 2.




                         Рис. 6.5  Столбцовое и строчное пространства
                        имеют разные охваты, но одинаковые размерности

  Ниже приведено несколько мат­риц. Хотя я еще не научил вас вычислять
ранг, попробуйте угадать ранг каждой мат­ри­цы на основе предыдущих опи-
саний. Ответы приведены в сноске1.




1
    r(A) = 1, r(B) = 1, r(C) = 2, r(D) = 3, r(E) = 1, r(F) = 0.
110  Матрицы. Часть 2




  Надеюсь, вам удалось разобраться в рангах, или, по меньшей мере, вы не
были шокированы, увидев ответы в сноске.
  Излишне говорить, что визуальный осмотр и некоторая интуиция не яв-
ляются масштабируемым методом вычисления ранга на практике. Ранг вы-
числяется несколькими способами. Например, в главе 10 вы узнаете, что
ранг можно вычислять построчно, приводя мат­ри­цу к ее ступенчатой форме
и подсчитывая число опорных элементов. Компьютерные программы, такие
как Python, вычисляют ранг путем подсчета количества ненулевых сингуляр-
ных чисел мат­ри­цы. Вы узнаете об этом в главе 14.
  А пока я хотел бы, чтобы вы сосредоточились на идее, что ранг соответ-
ствует наибольшему числу столбцов, которые могут образовывать линейно
независимое множество, что также соответствует размерности столбцового
пространства мат­ри­цы. (Слово «столбцовый» в предыдущем предложении
можно заменить на «строчный», и это будет одинаково точно.)


Ранги специальных матриц
Некоторые специальные мат­ри­цы имеют ранги, которые легко вычисляются
или о которых стоит знать.

Векторы
  Все векторы имеют ранг 1. Вся причина в том, что векторы – по определе-
  нию – содержат только один столбец (или строку) информации; подпро-
  странство, которое они охватывают, одномерное. Единственным исклю-
  чением является вектор нулей.

Матрицы нулей
 Матрица нулей любого размера (включая вектор нулей) имеет ранг 0.

Единичные мат­ри­цы
  Ранг единичной мат­ри­цы равен числу строк (которое равно числу столб-
  цов). Другими словами, r(IN) = N. На самом деле единичная мат­ри­ца явля-
  ется просто частным случаем диагональной мат­ри­цы.

Диагональные мат­ри­цы
  Ранг диагональной мат­ри­цы равен числу ненулевых диагональных эле-
  ментов. Это связано с тем, что каждая строка содержит максимум один
  ненулевой элемент, и невозможно создать ненулевое число посредством
  взвешенных комбинаций нулей. Это свойство становится полезным при
  решении систем уравнений и интерпретации сингулярного разложения.
                                                                         Ранг  111

Треугольные мат­ри­цы
  Треугольная мат­ри­ца является полноранговой только в том случае, если во
  всех диагональных элементах имеются ненулевые значения. Треугольная
  мат­ри­ца с хотя бы одним нулем в диагонали будет рангово-пониженной
  (точный ранг будет зависеть от числовых значений в мат­ри­це).

Случайные мат­ри­цы
  Ранг случайной мат­ри­цы невозможно узнать априори, поскольку он за-
  висит от распределения чисел, из которого были взяты элементы мат­ри­
  цы, и от вероятности извлечения каждого числа. Например, мат­ри­ца 2×2,
  заполненная 0 либо 1, может иметь ранг 0, если все отдельные элементы
  равны 0. Либо она может иметь ранг 2, например если единичная мат­ри­ца
  отобрана в случайном порядке.
  Но существует способ создания случайных мат­риц с гарантированным
  максимально возможным рангом. Это делается путем извлечения чисел
  с плавающей точкой в случайном порядке, например из гауссова или рав-
  номерного распределения. 64-битовый компьютер может представлять
  264 чисел. Извлечение нескольких десятков или сотен из этого множества,
  чтобы поместить в мат­ри­цу, означает, что вероятность линейных зави-
  симостей в столбцах мат­ри­цы будет астрономически малой. На самом
  деле настолько маловероятной, что это могло бы привести в действие
  двигатель бесконечной невероятности космического корабля «Золотое
  сердце»1.
  Дело в том, что мат­ри­цы, созданные, например, функцией np.random.
  randn(), будут иметь максимально возможный ранг. Это удобно при ис-
  пользовании Python для изучения линейной алгебры, потому что можно
  создавать мат­ри­цу с любым произвольным рангом (с учетом обсуждав-
  шихся ранее ограничений). Упражнение 6.5 проведет вас по всему про-
  цессу.

Одноранговые мат­ри­цы
  Одноранговая мат­ри­ца имеет – как вы уже догадались – ранг 1. Это оз-
  начает, что на самом деле в мат­ри­це содержится информация только из
  одного столбца (и как вариант: информация только из одной строки), а все
  остальные столбцы (или строки) являются просто линейными кратными.
  Ранее в этой главе вы увидели несколько примеров одноранговых мат­риц.
  Вот еще несколько:



1
    Технология двигателя бесконечной невероятности позволяет космическому ко-
    раблю «Золотое сердце» преодолевать невозможные расстояния в космосе. Если
    вы не знаете, о чем я говорю, значит, вы не читали роман «Автостопом по Галак-
    тике» Дугласа Адамса (Hitchhiker’s Guide to the Galaxy, Douglas Adams). И если вы
    не читали эту книгу, то вы реально упускаете одно из величайших, заставляющих
    задуматься и забавнейших интеллектуальных достижений XX века.
112  Матрицы. Часть 2




    Одноранговые мат­ри­цы могут быть квадратными, высокими или широ-
    кими; независимо от размера каждый столбец является шкалированной
    копией первого столбца (или каждая строка является шкалированной ко-
    пией первой строки).
    Как создавать мат­ри­цу ранга 1? На самом деле в главе 2 вы уже узнали, как
    это делается (правда, я упомянул об этом лишь вскользь; так что прощаю
    вас, что забыли). Ответ таков: путем взятия внешнего произведения между
    двумя ненулевыми векторами. Например, третья показанная выше мат­ри­
    ца является внешним произведением [4 2 3]T и [3 1 1 3 1].
    Одноранговые мат­ри­цы имеют большую важность в собственном и сингу-
    лярном разложении мат­ри­цы. Вы столкнетесь со многими одноранговыми
    мат­ри­ца­ми в последующих главах этой книги – и в своих приключениях
    по прикладной линейной алгебре.


Ранг сложенных и умноженных матриц
Если известны ранги мат­риц A и B, то можно ли автоматически узнать ранг
A + B либо AB?
  Ответ – нет, невозможно. Но ранги двух отдельных мат­риц обеспечивают
верхние границы максимально возможного ранга. Вот правила:

    rank(A + B) £ rank(A) + rank(B);
    rank(AB) £ min{rank(A), rank(B)}.

  Я не рекомендую заучивать эти правила наизусть. Но настоятельно реко-
мендую запомнить следующее:
    невозможно узнать точный ранг мат­ри­цы суммирования или мат­ри­цы
      произведения, основываясь на знании рангов отдельных мат­риц (за
      некоторыми исключениями, например мат­ри­цы нулей); вместо этого
      отдельные мат­ри­цы обеспечивают верхние границы ранга мат­ри­цы
      суммирования либо мат­ри­цы произведения;
    ранг мат­ри­цы суммирования может быть больше, чем ранги отдельных
      мат­риц;
    ранг мат­ри­цы произведения не может быть больше наибольшего ранга
      умножающих мат­риц1.
  В упражнении 6.6 у вас будет возможность проиллюстрировать эти два
правила, используя случайные мат­ри­цы различных рангов.

1
    Это правило объясняет, почему внешнее произведение всегда дает мат­ри­цу ран-
    га 1.
                                                                           Ранг  113


Ранг сдвинутых матриц
Если говорить просто, то сдвинутые мат­ри­цы имеют полный ранг. Фактиче-
ски одной из первичных целей сдвига квадратной мат­ри­цы является повы-
шение ее ранга с r < M до r = M.
  Очевидным примером является сдвиг мат­ри­цы нулей единичной матри-
цей. Ранг результирующей суммы 0 + I является полноранговой матрицей.
  Вот еще один, чуть менее очевидный пример:




   Крайняя левая мат­ри­ца имеет ранг 2; обратите внимание, что третий
столбец равен второму минус первый. Но ранг суммированной мат­ри­цы
равен 3: третий столбец больше невозможно получить некоторой линейной
комбинацией первых двух. И все же информация в мат­ри­це практически
не изменилась; и действительно, корреляция Пирсона между элементами
в изначальной и сдвинутой мат­ри­цах составляет ρ = 0.999997222233796. Этот
факт имеет существенные последствия: например, мат­ри­цу ранга 2 нельзя
инвертировать, тогда как сдвинутую мат­ри­цу можно. (Вы узнаете причину
в главе 8.)


Теория и практика
    Понимание сингулярных чисел перед пониманием сингулярных чисел
    Невозможно изучать математику в чисто монотонном ключе, то есть когда вы полно-
    стью разбираетесь в концепции a до того, как полностью усвоите концепцию b и т. д.
    Полное понимание ранга мат­ри­цы требует знания сингулярного разложения, но син-
    гулярное разложение не имеет смысла, пока вы не узнаете о ранге. Это своего рода
    уловка, из которой нет выхода из-за взаимно зависимых условий. И это часть того,
    что затрудняет изучение математики. Хорошие новости состоят в том, что страницы
    в этой книге продолжают существовать и после того, как вы их прочитали, поэтому,
    если следующее далее изложение будет не совсем разумным, вернитесь к нему после
    ознакомления с сингулярным разложением (SVD).
    Если кратко, то каждая мат­ри­ца M×N имеет множество min{M, N} неотрицательных
    сингулярных чисел, в которых закодирована «важность», или «пространственность»,
    различных направлений в столбцовом и строчном пространствах мат­ри­цы. Направ-
    ления с сингулярным числом 0 находятся в одном из нуль-пространств.

  В абстрактной линейной алгебре ранг – незыблемое понятие. Каждая мат­
ри­ца имеет ровно один ранг, и на этом история заканчивается.
  Однако на практике вычисление ранга мат­ри­цы влечет за собой некоторую
неопределенность. Возможно, компьютеры даже не вычисляют ранг, а лишь
рассчитывают его оценку с разумной степенью точности. Ранее я писал, что
ранг можно вычислить как количество ненулевых сингулярных чисел. Но
114  Матрицы. Часть 2

это не то, что делает Python. Ниже приведены две ключевые строки, взятые
из функции np.linalg.matrix_rank() (я опустил несколько аргументов, чтобы
сосредоточиться на главном):
S = svd(M)
return count_nonzero(S > tol)

  M – это рассматриваемая мат­ри­ца, S – вектор сингулярных чисел, а tol –
порог допуска. И значит, NumPy на самом деле не подсчитывает ненулевые
сингулярные числа; он подсчитывает сингулярные числа, которые превы-
шают некоторый порог. Точное пороговое значение зависит от числовых
значений в мат­ри­це, но обычно оно примерно на 10–12 порядков меньше,
чем элементы мат­ри­цы.
  Это означает, что NumPy принимает решение о том, какие числа доста-
точно малы, чтобы считаться «фактически нулевыми». Я, конечно же, не
критикую NumPy – это правильный поступок! (Другие программы числовой
обработки, такие как MATLAB и Julia, вычисляют ранг таким же образом.)
  Но зачем это делать? Почему бы просто не подсчитывать ненулевые син-
гулярные числа? Ответ в том, что допуск поглощает малые числовые неточ-
ности, которые могут возникать из-за ошибки вычислительного округ­ления.
Наличие допуска также позволяет игнорировать незначительные шумы, ко-
торыми могут, например, загрязняться датчики сбора данных. Эта идея ис-
пользуется для очистки данных, сжатия и уменьшения размерности. Рису-
нок 6.6 иллюстрирует эту концепцию.




                                                              Шум




           Рис. 6.6  Ранг представляющей двумерную плоскость мат­ри­цы 3×3
             может считаться равным 3 при наличии малого количества шума.
                Крайняя правая диаграмма показывает перспективу взгляда
                     непосредственно через поверхность плоскости




Применения ранга
Существует целый ряд применений ранга мат­ри­цы. В данном разделе я пред-
ставлю два из них.
                                                     Применения ранга  115


В столбцовом пространстве
В главе 4 вы узнали о столбцовом пространстве мат­ри­цы и о том, что важным
вопросом в линейной алгебре является принадлежность/непринадлежность
вектора к столбцовому пространству мат­ри­цы (который математически
можно записать как v Î C(A)?). Я также написал, что строгий и масштабиру-
емый ответ на данный вопрос зависит от понимания ранга мат­ри­цы.
   Прежде чем рассказать об алгоритме определения v Î C(A), мне нужно
кратко представить процедуру, именуемую расширением1 мат­ри­цы.
   Расширить мат­ри­цу означает добавить дополнительные столбцы в правую
часть мат­ри­цы. Вы начинаете с «базовой» мат­ри­цы M×N и «дополнительной»
мат­ри­цы M×K. Расширенная мат­ри­ца имеет размер M×(N + K). Расширение
двух мат­риц допустимо при условии, что они имеют одинаковое число строк
(у них может быть разное число столбцов). Вы увидите расширенные мат­ри­
цы здесь, в этом разделе, и снова в главе 10 при решении систем уравнений.
   Ниже приведен пример, иллюстрирующий данную процедуру:




   С учетом этого обязательного отклонения от темы вот алгоритм опре-
деления принадлежности/непринадлежности вектора к столбцовому про-
странству мат­ри­цы:
   1.	Раширить мат­ри­цу вектором. Обозначим изначальную мат­ри­цу че-
       рез A и раширенную мат­ри­цу – через Ã.
   2. Вычислить ранги двух мат­риц.
   3. Сравнить два ранга. Будет один из двух возможных исходов:
       a)	rank(A) = rank(Ã)	Вектор v находится в столбцовом пространстве
                              мат­ри­цы A.
       b)	rank(A) < rank(Ã)	Вектор v не находится в столбцовом простран-
                              стве мат­ри­цы A.
   Какова логика, стоящая за этой проверкой? Если v Î C(A), то v можно выра-
зить как некоторую линейно-взвешенную комбинацию столбцов мат­ри­цы A
(столбцы расширенной мат­ри­цы Ã образуют линейно зависимое множество).
С точки зрения охвата вектор v является избыточным в Ã. Следовательно,
ранг остается прежним.
   И наоборот, если v Ï C(A), то v невозможно выразить как линейно-взве-
шенную комбинацию столбцов мат­ри­цы A, и, следовательно, v добавил но-
вую информацию в Ã. А это значит, что ранг увеличится на 1.

1
    Англ. augmenting. – Прим. перев.
116  Матрицы. Часть 2

   Определение принадлежности/непринадлежности вектора к столбцовому
пространству мат­ри­цы – это не просто какое-то эзотерическое академиче-
ское упражнение; это часть внутренней логики линейного моделирования
методом наименьших квадратов, то есть математики, лежащей в основе ме-
тодов ANOVA, регрессий и общих линейных моделей. В главах 10 и 11 вы
узнаете об этом больше, но главная идея заключается в том, что мы раз-
рабатываем модель того, как устроен мир, а затем переводим эту модель
в мат­ри­цу, которая называется расчетной матрицей1. Измеряемые данные из
окружающего мира хранятся в векторе. Если этот вектор данных находится
в столбцовом пространстве расчетной мат­ри­цы, то мы смоделировали мир
идеально. Почти во всех случаях вектор данных не находится в столбцовом
пространстве, и поэтому мы определяем, достаточно ли он близок, чтобы
считаться статистически значимым. Очевидно, что в последующих главах
я расскажу гораздо подробнее, но здесь я предвосхищаю события, чтобы
поддержать ваш энтузиазм в обучении.


Линейная независимость множества векторов
Теперь, когда вы знаете о ранге мат­ри­цы, вы готовы понять алгоритм опре-
деления линейной зависимости/независимости множества векторов. Если
вы хотите разработать такой алгоритм самостоятельно, то смело делайте это
сейчас, перед тем как читать остальную часть данного раздела.
   Алгоритм незамысловат: поместить векторы в мат­ри­цу, вычислить ранг
мат­ри­цы, а затем сравнить этот ранг с максимально возможным рангом этой
мат­ри­цы (вспомните, что это min{M, N}; для удобства изложения я буду исхо-
дить из допущения, что у нас высокая мат­ри­ца). Возможными результатами
являются:
     r = M: множество векторов является линейно независимым;
     r < M: множество векторов является линейно зависимым.
   Внутренняя логика этого алгоритма должна быть ясной: если ранг меньше,
чем число столбцов, то по меньшей мере один столбец можно описать как
линейную комбинацию других столбцов, то есть он является определением
линейной зависимости. Если ранг равен числу столбцов, то каждый столбец
вносит уникальную информацию в мат­ри­цу, и, стало быть, ни один столбец
невозможно описать как линейную комбинацию других столбцов.
   Здесь я хотел бы подчеркнуть более общий момент: многие операции
и приложения в линейной алгебре на самом деле являются довольно прос­
тыми и разумными, но для их понимания требуется значительный объем
базовых знаний. И это хорошо: чем больше вы знаете линейную алгебру, тем
проще становится линейная алгебра.
   С другой стороны, это утверждение не является универсально истинным:
в линейной алгебре существуют операции, которые настолько умопомрачи-

1
    Англ. design matrix; син. модельная мат­ри­ца; мат­ри­ца плана эксперимента. – Прим.
    перев.
                                                        Определитель  117

тельно утомительны и сложны, что я не могу заставить себя описать их во
всех деталях в этой книге. Переходим к следующему разделу…



Определитель
Определитель1– это число, ассоциированное с квадратной матрицей. В абст­
рактном линейном алгоритме определитель является ключевым элементом
в нескольких операциях, включая вычисление обратной мат­ри­цы. Однако
вычисление определителя для крупных мат­риц на практике бывает численно
нестабильным из-за проблем, связанных с потерей значимости и перепол-
нением. Вы увидите иллюстрацию тому в упражнениях.
   Тем не менее получение обратной мат­ри­цы или собственного разложения
мат­ри­цы невозможно понять, не понимая определителя.
   Два наиболее важных свойства определителя – и два наиболее важных вы-
вода из этого раздела – таковы:
   1) он определен только для квадратных мат­риц и
   2) он равен нулю для сингулярных (рангово-пониженных) мат­риц.
   Определитель обозначается как det(A) или |A| (обратите внимание на оди-
ночные вертикальные линии вместо двойных вертикальных линий, которы-
ми обозначается норма мат­ри­цы). Греческая заглавная дельта Δ использует-
ся, когда не нужно ссылаться на конкретную мат­ри­цу.
   Но что такое определитель? Что он означает и как его интерпретиро-
вать? Определитель имеет геометрическую интерпретацию, которая связана
с тем, насколько мат­ри­ца растягивает векторы во время умножения мат­ри­цы
на вектор (как мы помним из предыдущей главы, умножение мат­ри­цы на
вектор представляет собой механизм применения геометрических преоб-
разований к координате, выраженной в виде вектора). Отрицательный опре-
делитель означает, что во время преобразования поворачивается одна ось.
   Однако в приложениях, связанных с наукой о данных, определитель ис-
пользуется алгебраически; эти геометрические объяснения не очень понят-
ны в части того, как использовать определитель для отыскания собственных
чисел или инвертирования мат­риц ковариаций. Поэтому достаточно сказать,
что определитель является решающим шагом в таких продвинутых темах,
как получение обратной мат­ри­цы, собственного разложения и сингулярного
разложения, и что вы можете избавить себя от стресса и бессонных ночей,
просто признав, что определитель является инструментом в нашем арсенале,
не слишком беспокоясь о том, что он значит.


Вычисление определителя
Вычисление определителя отнимает много времени и утомляет. Если я про-
живу тысячу лет и никогда не вычислю определитель мат­ри­цы 5×5 вручную,

1
    Син. детерминант. – Прим. перев.
118  Матрицы. Часть 2

то проживу богатую, полноценную и осмысленную жизнь. Тем не менее су-
ществует короткий путь вычисления определителя мат­ри­цы 2×2, который
показан в уравнении 6.2.

  Уравнение 6.2. Вычисление определителя мат­ри­цы 2×2




  Из этого уравнения можно увидеть, что определитель не ограничивается
целыми либо положительными числами. В зависимости от числовых значе-
ний в мат­ри­це определителем может быть −1 223 729 358 либо +.00000002,
или же любое другое число. (Для действительно-значной мат­ри­цы опреде-
лителем всегда будет действительное число.)
  Определитель вычисляется довольно просто, не правда ли? Всего-то про-
изведение диагонали минус произведение вне диагонали. Для многих мат­
риц определитель можно вычислять в уме. Определитель еще проще для
мат­ри­цы 1×1: это просто абсолютное значение этого числа.
  Возможно, теперь вы сомневаетесь в моем утверждении о том, что опре-
делитель имеет численную нестабильность.
  Дело в том, что сокращенный способ вычисления определителя мат­ри­цы
2×2 не масштабируется до крупных мат­риц. Есть «сокращенный способ» для
мат­риц 3×3, но на самом деле это не сокращенный способ; это визуальная
мнемоника. Я не буду ее тут показывать, но напишу главную идею:




  Как только вы доберетесь до мат­риц 4×4, вычисление определителя станет
настоящей морокой, если только в мат­ри­це не будет много тщательно рас-
ставленных нулей. Но я знаю, что вам любопытно, поэтому уравнение 6.3
показывает определитель мат­ри­цы 4×4.

  Уравнение 6.3. Желаю удачи с этим уравнением




  Я даже не собираюсь показывать полную процедуру вычисления опре-
делителя мат­ри­цы любого размера, потому что давайте будем честны: вы
читаете эту книгу, поскольку интересуетесь прикладной линейной алгеброй;
важно понимать, как использовать определитель, а не полную формулу его
вычисления.
                                                        Определитель  119

  В любом случае, суть в том, что если вам когда-нибудь понадобится вычис-
лить определитель, то следует использовать функцию np.linalg.det() либо
функцию scipy.linalg.det().


Определитель с линейными зависимостями
Второй вывод в отношении определителй заключается в том, что они равны
нулю для любой рангово-пониженной мат­ри­цы. Этот факт можно обследо-
вать с помощью мат­ри­цы 2×2. Вспомните, что любая рангово-пониженная
мат­ри­ца имеет по меньшей мере один столбец, который можно выразить как
линейную комбинацию других столбцов:




  Вот определитель сингулярной мат­ри­цы 3×3:




  Указанная концепция обобщается на более крупные мат­ри­цы. Отсюда сле-
дует, что все рангово-пониженные мат­ри­цы имеют определитель, равный 0.
Фактический ранг не имеет значения; если r < M, то Δ = 0. Все полноранговые
мат­ри­цы имеют ненулевой определитель.
  Я уже писал, что, по моему мнению, геометрические интерпретации опре-
делителя имеют ограниченную ценность для понимания причины, по кото-
рой определитель имеет важность в науке о данных. Но Δ = 0 действительно
имеет приятный геометрический смысл: мат­ри­ца с Δ = 0 – это преобразо-
вание, при котором по меньшей мере одно измерение сглаживается, чтобы
получить площадь поверхности, но не объем.
  Представьте себе, как вы сжимаете мяч до бесконечно тонкого диска. Вы
увидите наглядный пример в следующей главе («Геометрические преобра-
зования с помощью умножения мат­ри­ц на векторы» на стр. 131).


Характеристический многочлен
Уравнение определителя мат­ри­цы 2×2 содержит пять элементов: четыре эле-
мента в мат­ри­це и значение определителя. Их можно записать в уравнении
как ad – bc = D. Одна из замечательных особенностей уравнения заключается
в том, что элементы в нем можно перемещать туда-сюда и находить решения
для разных переменных. Рассмотрим уравнение 6.4; допустим, что a, b, c и D
известны, а λ – некоторый неизвестный элемент.
120  Матрицы. Часть 2

  Уравнение 6.4. Использование определителя для отыскания недостающего
  элемента мат­ри­цы




   Немного алгебры уровня средней школы позволит найти решение для λ
в терминах других элементов. Само решение не имеет важности; главное
в том, что если определитель мат­ри­цы известен, то можно найти решение для
неизвестных переменных внутри мат­ри­цы.
   Вот численный пример:




  Теперь давайте сделаем еще один шаг вперед:




  То же самое неизвестное λ находилось на диагонали, произведя полино-
миальное уравнение второго порядка, а оно, в свою очередь, привело к двум
решениям. Это вовсе не совпадение: фундаментальная теорема алгебры го-
ворит о том, что многочлен n-го порядка имеет ровно n корней (хотя неко-
торые из них могут быть комплекснозначными).
  Вот ключевой момент, к которому я веду: комбинирование сдвига мат­ри­
цы с определителем называется характеристическим многочленом мат­ри­цы,
как показано в уравнении 6.5.

  Уравнение 6.5. Характеристический многочлен мат­ри­цы

  det(A – λI) = D.

  Почему он называется многочленом? Потому что сдвинутая мат­ри­ца M×M
имеет член λM и, следовательно, имеет M решений. Вот как он выглядит для
мат­риц 2×2 и 3×3:




  Пожалуйста, не просите меня показать вам характеристический много­
член мат­ри­цы 4×4. Поверьте, в нем будет член λ4.
                                                          Определитель  121

  Давайте вернемся к случаю 2×2, на этот раз используя числа вместо букв.
Приведенная ниже мат­ри­ца является полноранговой, то есть ее определи-
тель не равен 0 (он равен λ = –8), но я собираюсь допустить, что она имеет
определитель 0 после сдвига на некоторый скаляр λ; вопрос в том, какие
значения λ сделают эту мат­ри­цу рангово-пониженной. Давайте применим
характеристический многочлен, чтобы выяснить:




  После небольших алгебраических операций (которые я рекомендую вам
проработать) два решения таковы: λ = –2, 4. Что эти числа означают? Для
того чтобы это выяснить, давайте включим их обратно в сдвинутую мат­ри­цу:




   Совершенно ясно, что обе мат­р и­ц ы являются одноранговыми. Кроме
того, обе мат­р и­ц ы имеют нетривиальные нуль-пространства, и, значит,
можно найти некий ненулевой вектор y такой, что (A – λI)y = 0. Уверен, вы
сможете найти вектор для каждого из этих λ самостоятельно! Но на всякий
случай, если вы хотите подтвердить свой правильный ответ, обратитесь
к сноске1.
   Характеристический многочлен, выражаясь технически, суперудивителен.
Прежде всего он дает замечательное представление о том, что каждую ква-
дратную мат­ри­цу можно выразить в виде уравнения. И не просто какого-то
уравнения – а уравнения, которое напрямую связывает мат­ри­цы с фунда-
ментальной теоремой алгебры. И если это недостаточно круто, то решени-
ями характеристического многочлена, заданного равным Δ = 0, являются
собственные числа мат­ри­цы (это числа λ, которые мы нашли выше). Знаю,
что вам придется подождать главы 13, чтобы понять, что такое собственные
числа, какова причина их важности, но сегодня ночью вы сможете спать спо-
койно, ибо вы раскрыли для себя важную вещь: как определять собственные
числа мат­ри­цы.




1
    Любая шкалированная версия векторов [1 –1] и [1 1].
122  Матрицы. Часть 2


Резюме
Цель этой главы была в том, чтобы расширить ваши знания о матрицах, вклю-
чив в них несколько важных понятий: нормы, пространства, ранг и опреде-
литель. Если бы это была книга об абстрактной линейной алгебре, то данная
глава легко могла бы занять более ста страниц. Но я попытался сосредоточить
изложение на том, что вам нужно знать для применения линейной алгебры
в науке о данных и искусственном интеллекте. Вы узнаете о матричных про-
странствах еще больше в последующих главах (в частности, о методе наи-
меньших квадратов в главе 11 и сингулярном разложении в главе 14). А пока
вот список наиболее важных выводов, которые следует вынести из этой главы.
     Существует много видов матричных норм, которые в широком смысле
       можно классифицировать на поэлементные и индуцированные. Первые
       отражают величины1 элементов в мат­ри­це, тогда как вторые отражают
       геометрически преобразующее влияние мат­ри­цы на векторы.
     Наиболее широко используемая поэлементная норма называется нор-
       мой Фробениуса (она же евклидова норма, или ℓ2-норма) и вычисляет-
       ся как квадратный корень из суммы квадратов элементов.
     След мат­ри­цы – это сумма диагональных элементов.
     Существует четыре пространства мат­ри­цы (столбцовое, строчное, нуль-
       пространство, правое нуль-пространство), и они определяются как мно-
       жество линейно-взвешенных комбинаций разных признаков мат­ри­цы.
     Столбцовое пространство мат­ри­цы содержит все линейно-взвешенные
       комбинации столбцов в мат­ри­це и записывается как C(A).
     Принадлежность/непринадлежность некого вектора b столбцовому
       пространству мат­ри­цы является важным вопросом в линейной алгеб­
       ре; если он ему принадлежит, то существует некий вектор x такой, что
       Ax = b. Ответ на этот вопрос формирует основу для многих алгоритмов
       подгонки статистических моделей.
     Строчное пространство мат­ри­цы – это множество линейно-взвешен-
       ных комбинаций строк мат­ри­цы и обозначается как R(A) или C(AT).
     Нуль-пространство мат­ри­цы – это множество векторов, которое ли-
       нейно комбинирует столбцы, чтобы получить вектор нулей – другими
       словами, любой вектор y, который находит решение уравнения Ay = 0.
       Тривиальное решение y = 0 исключается. Нуль-пространство, среди
       других применений, имеет большое значение для отыскания собствен-
       ных векторов.
     Ранг – это ассоциированное с матрицей неотрицательное целое число.
       Ранг отражает наибольшее число столбцов (или строк), которые могут
       образовывать линейно независимое множество. Матрицы с рангом,
       меньшим максимально возможного, называются рангово-понижен-
       ными, или сингулярными.
     Сдвиг квадратной мат­ри­цы путем добавления константы к диагонали
       обеспечивает полный ранг.

1
    Син. магнитуды. – Прим. перев.
                                        Упражнения по программированию  123

    Одно (из многих) применений ранга состоит в определении принад-
      лежности/непринадлежности вектора столбцовому пространству мат­
      ри­цы, которое производтся путем сравнения ранга мат­ри­цы с рангом
      векторно-раширенной мат­ри­цы.
    Определитель – это ассоциированное с квадратной матрицей число
      (для прямоугольных мат­риц определителя нет). Алгоритм его вычис-
      ления весьма утомителен, но самое важное, что нужно о нем знать,
      сводится к тому, что он равен нулю для всех рангово-пониженных мат­
      риц и отличен от нуля для всех полноранговых мат­риц.
    Характеристический многочлен преобразовывает сдвинутую на λ
      квад­ратную мат­ри­цу в уравнение, приравненное к определителю. Зная
      определитель, можно находить решения для λ. Я лишь кратко намекнул
      о причине, по которой так важно знать определитель, но поверьте: это
      важно. (Или же не доверяйте мне и убедитесь сами в главе 13.)



Упражнения по программированию
  Упражнение 6.1
  Норма мат­ри­цы связана со шкалой числовых значений в мат­ри­це. В дан-
ном упражнении вы проведете эксперимент, чтобы это продемонстриро-
вать. В каждой из 10 итераций эксперимента создавайте мат­ри­цу случай-
ных чисел 10×10 и вычисляйте ее фробениусову норму. Затем повторите
этот эксперимент 40 раз, всякий раз умножая мат­ри­цу на другой скаляр, ко-
торый находится в диапазоне от 0 до 50. Результатом эксперимента станет
мат­ри­ца норм размером 40×10. На рис. 6.7 показаны результирующие нор-
мы, усредненные по 10 итерациям эксперимента. Указанный эксперимент
также иллюстрирует два дополнительных свойства матричных норм: они
строго неотрицательны и могут быть равны 0 только для мат­ри­цы нулей.




                     Рис. 6.7  Решение к упражнению 6.1
124  Матрицы. Часть 2

   Упражнение 6.2
   В этом упражнении вы напишете алгоритм, который находит скаляр, сво-
дящий расстояние Фробениуса между двумя мат­ри­ца­ми к 1. Начните с на-
писания функции Python, которая на входе принимает две мат­ри­цы (одина-
кового размера) и на выходе возвращает фробениусово расстояние между
ними. Затем создайте две мат­ри­цы N×N случайных чисел (в исходном коде
решений я использовал N = 7, но вы можете использовать любой другой раз-
мер). Создайте переменную s = 1, которая скалярно умножает обе мат­ри­цы.
Вычисляйте расстояние Фробениуса между шкалированными мат­ри­ца­ми. До
тех пор, пока это расстояние остается выше 1, устанавливайте скаляр рав-
ным ему самому, умноженному на .9, и пересчитывайте расстояние между
шкалированными мат­ри­ца­ми. Это должно быть сделано в цикле while. Когда
фробениусово расстояние станет меньше 1, завершите цикл while и сообщите
число итераций (что соответствует числу раз, когда скаляр s был умножен на
.9) и скалярное значение.

  Упражнение 6.3
  Продемонстрируйте, что метод следа мат­ри­цы и евклидова формула дают
один и тот же результат (норму Фробениуса). Работает ли формула следа
только для ATA или же вы получаете тот же результат и для AAT?

  Упражнение 6.4
  Это упражнение будет забавным1, потому что вы сможете использовать
материал из данной и предыдущих глав. Вы обследуете влияние сдвига мат­
ри­цы на норму этой мат­ри­цы. Начните с создания случайной мат­ри­цы 10×10
и вычислите ее фробениусову норму. Затем запрограммируйте следующие
ниже шаги внутри цикла for:
  1) сдвиньте мат­ри­цу на долю нормы;
  2)	вычислите процентное изменение нормы по сравнению с изначаль-
       ным;
  3)	вычислите фробениусово расстояние между сдвинутой и изначальной
       мат­ри­ца­ми и
  4)	вычислите коэффициент корреляции между элементами в матрицах
       (подсказка: прокоррелируйте векторизованные мат­ри­цы с помощью
       функции np.flatten()).
  Доля нормы, на которую вы делаете сдвиг, должна находиться в диапазоне
от 0 до 1 с шагом в 30 линейно отстоящих шагов. Проверьте, чтобы на каждой
итерации цикла использовалась изначальная мат­ри­ца, а не сдвинутая мат­
ри­ца из предыдущей итерации. У вас должен получиться график, похожий
на рис. 6.8.

  Упражнение 6.5
  Теперь я покажу вам, как создавать случайные мат­ри­цы с произвольным
рангом (с учетом ограничений, касающихся размеров мат­риц и т. д.). Для

1
    Абсолютно все упражнения доставляют удовольствие, но некоторые больше, чем
    другие.
                                        Упражнения по программированию  125

создания мат­ри­цы M×N с рангом r умножьте случайную мат­ри­цу M×r на мат­
ри­цу r×N. Реализуйте это на Python и подтвердите, что ранг действительно
равен r. Что произойдет, если задать r > min{M, }, и почему это происходит?




                     Рис. 6.8  Решение к упражнению 6.4

  Упражнение 6.6
  Продемонстрируйте правило сложения ранга мат­риц (r(A + B) £ r(A) + r(B)),
создав три пары одноранговых мат­риц, просуммированных с
  1) нуль-ранговой,
  2) одноранговой и
  3) двуранговой.
  Затем повторите это упражнение, используя умножение мат­риц вместо
их сложения.

  Упражнение 6.7
  Поместите исходный код из упражнения 6.5 в функцию Python, которая на
входе принимает параметры M и r и на выходе выдает случайную мат­ри­цу
M×M ранга r. В двойном цикле for создайте пары мат­риц 20×20 с отдельными
рангами, варьирующимися от 2 до 15. Складывайте и умножайте эти мат­ри­
цы и сохраняйте ранги этих результирующих мат­риц. Указанные ранги долж-
ны быть организованы в мат­ри­цу и визуализированы как функция рангов
отдельных мат­риц (рис. 6.9).

  Упражнение 6.8
  Интересно, что все мат­ри­цы A, AT, ATA и AAT имеют одинаковый ранг. На-
пишите исходный код, чтобы это продемонстрировать, используя случайные
мат­ри­цы различных размеров, очертаний (квадратные, высокие, широкие)
и рангов.

  Упражнение 6.9
  Цель этого упражнения – ответить на вопрос: v Î C(A)? Создайте мат­
ри­цу A Î ℝ4×3 ранга 3 и вектор v Î ℝ4 с использованием чисел, взятых из
нормального распределения в случайном порядке. Следуйте описанному
ранее алгоритму определения принадлежности/непринадлежности вектора
столбцовому пространству мат­ри­цы. Повторите исходный код несколько раз,
126  Матрицы. Часть 2

чтобы попробовать найти неуклонную закономерность. Затем используйте
мат­ри­цу A Î ℝ4×4 ранга 4. Готов поспорить на один миллион биткойнов1, что
вы всегда будете находить, что v Î C(A), когда A – это случайная мат­ри­ца 4×4
(при условии отсутствия ошибок программирования). Что именно вселяет
в меня уверенность в вашем ответе2?




                         Рис. 6.9  Результаты упражнения 6.7

  В целях дополнительной сложности поместите этот исходный код в функ-
цию, которая возвращает True либо False в зависимости от результата провер-
ки и которая вызывает исключение (то есть полезное сообщение об ошибке),
если размер вектора не соответствует расширению мат­ри­цы.

  Упражнение 6.10
  Напомню, что определитель рангово-пониженной мат­ри­цы – теоретиче-
ски – равен нулю. В данном упражнении вы проверите эту теорию на прак-
тике. Выполните следующие ниже шаги.
  1. Создайте квадратную случайную мат­ри­цу.
  2.	Понизьте ранг мат­ри­цы. Ранее вы делали это путем умножения прямо­
      угольных мат­риц; здесь задайте один столбец кратным другому столбцу.
  3. Вычислите определитель и сохраните его абсолютное значение.
  Выполните эти три шага в двойном цикле for: один цикл для мат­риц раз-
мером от 3×3 до 30×30 и второй цикл, который повторяет три шага сто раз
(повторение эксперимента широко применяется при симулировании шумо-
вых данных). Наконец, выведите определитель, усредненный по ста повто-
рам, на линейный график как функцию от числа элементов в мат­ри­це. Теория
линейной алгебры предсказывает, что эта линия (то есть определители всех

1
    Nota bene. У меня не так много биткойнов, ¯\_(ツ)_/¯
2
    Потому что столбцовое пространство мат­ри­цы полного ранга охватывает все ℝM,
    и, следовательно, все векторы в ℝM обязательно находятся в пространстве столбцов­.
                                       Упражнения по программированию  127

рангово-пониженных мат­риц) равна нулю независимо от размера мат­ри­цы.
На рис. 6.10 показано иное, отражая вычислительные трудности, связанные
с точным вычислением определителя. Я подверг данные логарифмическому
преобразованию с целью повышения наглядности; вы должны проинспекти-
ровать график, используя логарифмическую и линейную шкалы.




                   Рис. 6.10  Результаты упражнения 6.10
Глава          7
                     Применения матриц

Надеюсь, что теперь, после насыщенных теорией двух последних глав, вы
чувствуете себя так, словно только что закончили интенсивную тренировку
в тренажерном зале: измотанные, но полные энергии. Данная глава долж-
на быть похожа на велосипедную прогулку по холмам в сельской местно-
сти: временами она будет требовать усилий, но при этом предлагать свежую
и вдохновляющую перспективу.
   Описанные в данной главе приложения в общих чертах основаны на при-
ложениях, описанных в главе 4. Я сделал это, чтобы иметь несколько общих
нитей изложения, которые связывают главы о векторах и о матрицах. И по-
тому, что хочу, чтобы вы увидели, что, несмотря на все более усложняющие­
ся концепции и приложения, по мере продвижения по линейной алгебре
фундамент по-прежнему строится на тех же простых принципах, таких как
линейно-взвешенные комбинации и точечное произведение.



Матрицы ковариаций многопеременных
данных
В главе 4 вы узнали, как вычислять коэффициент корреляции Пирсона в виде
векторного точечного произведения двух переменных, деленного на произ-
ведение норм векторов. Эта формула была рассчитана на две переменные
(например, рост и вес); а что, если у вас несколько переменных (например,
рост, вес, возраст, еженедельные упражнения...)?
   Вы могли бы представить, что пишете двойной цикл for для всех пере-
менных и применяете формулу двухпеременной корреляции ко всем па-
рам переменных. Но это будет громоздко и неэлегантно, а следовательно,
противоречит духу линейной алгебры. Цель этого раздела – показать, как
вычислять мат­ри­цы ковариаций и корреляций из наборов многоперемен-
ных1 данных.

1
    Англ. multivariate; син. многопараметрический; относится к нескольким независи-
    мым переменным в статистической модели или модели МО. – Прим. перев.
                                     Матрицы ковариаций многопеременных данных  129

  Давайте начнем с ковариации. Ковариация – это просто числитель уравне-
ния корреляции, другими словами, точечное произведение двух перемен-
ных, центрированных по среднему значению. Ковариация интерпретируется
так же, как корреляция (положительная, когда переменные движутся вместе;
отрицательная, когда переменные движутся порознь; нулевая, когда пере-
менные не имеют линейной зависимости), за исключением того, что в кова-
риации сохраняется шкала данных и, следовательно, не ограничена ±1.
  Ковариация также имеет коэффициент нормализации n – 1, где n – это
число точек данных. Указанная нормализация не дает ковариации увеличи-
ваться по мере суммирования большего числа значений данных (аналогично
делению на N, чтобы преобразовать сумму в среднее значение). Вот уравне-
ние ковариации:




  Как и в главе 4, если через x обозначить переменную, центрированную
по среднему значению переменной x, то ковариация будет равна xTy/(n – 1).
  Ключевым моментом в реализации указанной формулы для нескольких
переменных является то, что матричное умножение представляет собой ор-
ганизованный набор точечных произведений между строками левой мат­ри­
цы и столбцами правой мат­ри­цы.
  Итак, вот что мы делаем: создаем мат­ри­цу, в которой каждый столбец
соответствует каждой переменной (переменная – это признак данных).
Обозначим эту мат­ри­цу через X. Теперь умножение XX не имеет смысла
(и, вероятно, даже недопустимо, потому что мат­ри­цы данных, как правило,
являются высокими, следовательно, M > N). Но если бы мы транспонирова-
ли первую мат­ри­цу, то строки мат­ри­цы XT соответствовали бы столбцам
мат­ри­цы X. Следовательно, в произведении мат­риц XTX кодируются все
попарные ковариации (при условии что столбцы центрированы по среднему
значению и затем делятся на n – 1). Другими словами, (i, j)-й элемент в мат­
ри­це ковариаций является точечным произведением между признаками
данных i и j.
  Матричное уравнение для мат­р и­ц ы ковариаций выглядит элегантно
и компактно:




  Матрица C симметрична. Это вытекает из доказательства в главе 5, что
любая мат­ри­ца, умноженная на ее транспонированную версию, является
квадратно-симметричной, но это также имеет смысл статистически: кова-
риация и корреляция симметричны, и, стало быть, например, корреляция
между высотой и весом такая же, как и корреляция между весом и ростом.
  Каковы диагональные элементы C? Они содержат ковариации каждой
переменной с самой собой, которые в статистике называются дисперсией1,

1
    Англ. variance. – Прим. перев.
130  Применения матриц

и количественно определяют разбросанность вокруг среднего значения (дис-
персия – это возведенное в квадрат стандартное отклонение).


                        Зачем транспонировать левую мат­ри­цу?
    В транспонировании левой мат­ри­цы нет ничего особенного. Если ваша мат­ри­ца данных
    организована как признаки по наблюдениям, тогда ее мат­ри­ца ковариаций равна XXT.
    Если вы когда-либо будете сомневаться в том, какую мат­ри­цу следует транспонировать:
    левую либо правую, – подумайте о том, как применить правила умножения мат­риц, чтобы
    получить мат­ри­цу в формате «признаки по признакам». Матрицы ковариаций всегда ор-
    ганизованы как «признаки по признакам».


  Пример в онлайновом исходном коде создает мат­ри­цу ковариаций из
общедоступного набора данных статистики преступности. Набор данных
содержит более ста признаков социальной, экономической, образовательной
и жилищной информации в различных сообществах по всей территории
США1. Цель набора данных – использовать указанные признаки для пред-
сказывания уровня преступности, но здесь мы будем использовать его для
инспектирования мат­риц ковариации и корреляции.
  После импорта и небольшой обработки данных (описанной в онлайновом
коде) у нас будет мат­ри­ца данных под названием dataMat. Следующий ниже
исходный код показывает, как вычислять мат­ри­цу ковариаций:
datamean = np.mean(dataMat,axis=0) # вектор средних значений признаков
dataMatM = dataMat - datamean      # среднецентрировать,
                                   # используя транслирование
covMat = dataMatM.T @ dataMatM # мат­ри­ца данных, умноженная на
                                   # ее транспонированную версию
covMat /= (dataMatM.shape[0]-1) # разделить на N–1

  На рис. 7.1 показано изображение мат­ри­цы ковариаций. Прежде всего
она выглядит просто замечательно, не правда ли? На своей «повседневной
работе» я работаю с наборами многопеременных данных в качестве про-
фессора нейробиологии, и разглядывание мат­риц ковариаций никогда не
переставало вызывать улыбку на моем лице.
  В этой мат­ри­це светлые цвета указывают на переменные, которые кова-
риируют положительно (например, процент разведенных мужчин по срав-
нению с числом людей, живущих в бедности), темные цвета указывают на
переменные, которые ковариируют отрицательно (например, процент разве-
денных мужчин по сравнению со средним доходом), а серые цвета указывают
на переменные, которые друг с другом не связаны.
  Как вы узнали в главе 4, вычисление корреляции на основе ковариации
просто предусматривает шкалирование на нормы векторов. Его можно пере-

1
    Редмонд М. А. и Бавея А. Программный инструмент, управляемый данными, для
    обеспечения совместного обмена информацией между полицейскими департа-
    ментами (M. A. Redmond and A. Baveja, “A Data-Driven Software Tool for Enabling
    Cooperative Information Sharing Among Police Departments,” European Journal of
    Operational Research 141 (2002): 660–678).
     Геометрические преобразования посредством умножения матриц на векторы  131

ложить в матричное уравнение, которое позволит вычислять мат­ри­цу корре-
ляций данных без циклов for. Упражнение 7.1 и упражнение 7.2 познакомят
с процедурой. Как я писал в главе 4, прежде чем переходить к следующему
разделу, призываю вас проработать эти упражнения.


                        Матрица ковариаций данных




                     Рис. 7.1  Матрица ковариаций данных


  Заключительное примечание: в NumPy есть функции вычисления мат­
риц ковариаций и корреляций (соответственно, np.cov() и np.corrcoef()).
На практике использовать эти функции удобнее, чем писать исходный код
самостоятельно. Но – как всегда в этой книге – я хотел бы, чтобы вы поняли
математику и механизмы, которые реализованы в этих функциях. Следова-
тельно, в этих упражнениях вы должны реализовать ковариацию как прямое
переложение формул вместо вызова функций NumPy.



Геометрические преобразования посредством
умножения матриц на векторы
В главе 5 упоминалось, что одной из целей умножения мат­ри­цы на вектор
является применение геометрического преобразования к множеству коор-
динат. В данном разделе вы это увидите на двумерных статичных изобра-
жениях и в анимации. Попутно вы узнаете о матрицах чистого поворота
и о создании анимации данных на Python.
132  Применения матриц

  «Матрица чистого поворота» поворачивает вектор, сохраняя при этом его
длину. Подумайте о стрелках аналоговых часов: с течением времени стрелки
вращаются, но не меняются по длине. Двумерная мат­ри­ца поворота выра-
жается как




   Матрица чистого поворота является примером ортогональной мат­ри­цы.
Я опишу ортогональные мат­ри­цы подробнее в главе 9, но хотел бы указать,
что столбцы мат­ри­цы T ортогональны (их точечное произведение равно
cos(θ)sin(θ) – sin(θ)cos(θ)) и являются единичными векторами (вспомним
тригонометрическое тождество, что cos2(θ) + sin2(θ) = 1).
   Для того чтобы применить эту трансформационную мат­ри­цу, надо устано-
вить θ равной некоторому углу поворота по часовой стрелке, а затем умно-
жить мат­ри­цу T на мат­ри­цу геометрических точек 2×Т, где каждый столбец
в этой мат­ри­це содержит координаты (X,Y) по каждой из Т точек данных.
Например, θ = 0 не изменит местоположения точек (потому что θ = 0 означает
T = I); θ = π/2 повернет точки на 90° вокруг начала координат.
   В качестве простого примера возьмем множество точек, выровненных по
вертикальной линии, и эффект умножения этих координат на T. На рис. 7.2
я задал θ = π/5.

                                  Поворот на 36°
                                                        Изначальное
                                                        Преобразованное




                Рис. 7.2  Поворот точек вокруг начала координат
                       с помощью мат­ри­цы чистого поворота
       Геометрические преобразования посредством умножения матриц на векторы  133

   Прежде чем продолжить работу с этим разделом, пожалуйста, ознакомь-
тесь с онлайновым исходным кодом, которым генерируется этот рисунок.
Убедитесь, что понимаете, как написанная выше математика переводится
в исходный код, и найдите минутку, чтобы поэкспериментировать с разны-
ми углами поворота. Вы также можете попробовать выяснить, как сделать
поворот против часовой стрелки вместо поворота по часовой стрелке; ответ
находится в сноске1.
   Давайте сделаем обследование поворотов более захватывающим, при-
менив «нечистые» повороты (то есть растягивание и поворот, а не только
поворот) и анимировав преобразования. В частности, мы будем плавно кор-
ректировать трансформационную мат­ри­цу в каждом кадре фильма.
   Анимации на Python создаются несколькими способами; применяемый
здесь метод предусматривает определение функции Python, которая создает
содержимое рисунка для каждого кадра фильма, затем вызывает процедуру
библиотеки matplotlib, чтобы выполнять эту функцию на каждой итерации
фильма.
   Я называю этот фильм «Шаткий круг». Окружности определяются мно­
жеством точек cos(θ) и sin(θ) для вектора углов θ, которые варьируются от 0
до 2π. Трансформационная мат­ри­ца задана в следующем виде:




  Почему я выбрал именно эти значения и как интерпретировать трансфор-
мационную мат­ри­цу? В общем случае диагональные элементы шкалируют
координаты по осям x и y, тогда как внедиагональные элементы растягивают
обе оси. Точные значения в приведенной выше мат­ри­це были выбраны пу-
тем подбора чисел до тех пор, пока я не нашел что-то, что, по моему мнению,
выглядело замечательно. Позже, в упражнениях, у вас будет возможность
обследовать эффекты изменения трансформационной мат­ри­цы.
  По ходу фильма значение φ будет плавно переходить от 1 к 0 и обратно к 1,
подчиняясь формуле φ = x2, –1 £ x £ 1. Обратите внимание, что T = I при φ = 1.
  Исходный код анимации данных на Python можно подразделить на три
части. Первая часть состоит в настройке рисунка:
theta = np.linspace(0,2*np.pi,100)
points = np.vstack((np.sin(theta),np.cos(theta)))

fig,ax = plt.subplots(1,figsize=(12,6))
plth, = ax.plot(np.cos(x),np.sin(x),'ko')

   Результатом работы метода ax.plot является переменная plth, представля-
ющая собой дескриптор, или указатель на объект графика. Этот дескриптор
позволяет обновлять место расположения точек, не перерисовывая рисунок
с нуля в каждом кадре.
   Вторая часть состоит в определении функции, которая обновляет оси
в каждом кадре:

1
    Поменяйте местами знаки минус в функциях синуса.
134  Применения матриц

def aframe(ph):
    # создать и применить трансформационную мат­ри­цу
    T = np.array([ [1,1-ph],[0,1] ])
    P = T@points

    # обновить местоположение точек
    plth.set_xdata(P[0,:])
    plth.set_ydata(P[1,:])

    return plth

  Наконец, определяем параметр преобразования φ и вызываем функцию
matplotlib, которая создает анимацию:
phi = np.linspace(-1,1-1/40,40)**2
animation.FuncAnimation(fig, aframe, phi,
                        interval=100,
                        repeat=True)

   На рис. 7.3 показан один кадр фильма, и вы можете просмотреть все видео,
выполнив исходный код. По общему признанию, этот фильм вряд ли получит
какие-либо награды, но он показывает, как умножение мат­риц применяется
в анимации. Графика в CGI-фильмах и видеоиграх немного сложнее, потому
что в них используются математические объекты, именуемые кватернио-
нами, то есть векторы в ℝ4, допускающие повороты и трансляции в 3D. Но
принцип – умножение мат­ри­цы геометрических координат на трансформа-
ционную мат­ри­цу – в точности тот же.




                         Рис. 7.3  Кадр из фильма «Шаткий круг»
                                       Обнаружение признаков изображения  135

  Прежде чем приступать к выполнению упражнений данного раздела, ре-
комендую потратить немного времени на то, чтобы поиграть с исходным
кодом этого раздела. В частности, измените трансформационную мат­ри­цу,
установив одному из диагональных элементов значение .5 или 2, измените
нижний левый внедиагональный элемент вместо верхнего правого внедиа-
гонального элемента (либо в дополнение к нему), параметризуйте один из
диагональных элементов вместо внедиагонального элемента и т. д. И вот
вопрос: сможете ли вы выяснить, как заставить круг качаться влево, а не
вправо? Ответ содержится в сноске1.



Обнаружение признаков изображения
В данном разделе я познакомлю вас с фильтрацией изображений, то есть
механизмом обнаружения признаков изображения. Фильтрация изображе-
ний на самом деле является расширением фильтра временного ряда, так
что ознакомление с главой 4 пойдет вам здесь на пользу. Напомню, что для
фильтрации или обнаружения признаков в сигнале временного ряда разра-
батывается ядро, а затем создается временной ряд точечных произведений
между ядром и накладывающимися друг на друга сегментами сигнала.
   Фильтрация изображений работает таким же образом, только в 2D, а не
в 1D. Мы конструируем двумерное ядро, а затем создаем новое изображение,
содержащее «точечные произведения» между ядром и накладывающимися
окнами изображения.
   Я написал «точечные произведения» в кавычках, как бы извиняясь, потому
что здесь операция формально не совпадает с точечным произведением век-
торов. Вычисление одно и то же – поэлементное умножение и сумма, однако
операция выполняется между двумя мат­ри­ца­ми, поэтому реализация пред-
ставляет собой адамарово умножение и суммирование по всем элементам
мат­ри­цы. График А на рис. 7.4 иллюстрирует указанную процедуру. Сущест­
вуют дополнительные детали свертки, – например, дополнение изображе-
ния таким образом, чтобы результат был того же размера, – о которых вы
узнаете из книги по обработке изображений. Здесь же я хотел бы, чтобы вы
сосредоточились на аспектах линейной алгебры, в частности на идее о том,
что точечное произведение количественно определяет взаимосвязь между
двумя векторами (или мат­ри­ца­ми) и его можно применять для обнаружения
признаков и фильтрации.
   Прежде чем перейти к анализу, кратко объясню, как создается двумерное
гауссово ядро. Двумерный гауссиан задается следующим ниже уравнением:

    G = exp(–(X 2 + Y 2)/σ).

  Несколько замечаний об этом уравнении: exp обозначает натуральную
экспоненту (константа e = 2,71828…), а exp(x) используется вместо ex, когда

1
    Установите значение нижнего правого элемента равным –1.
136  Применения матриц

экспоненциальный член – длинный. X и Y – это двумерные решетки с коор-
динатами x, y, на которых вычисляется функция. Наконец, σ – это параметр
функции, который часто называют «коэффициентом масштаба», или «шири-
ной»: меньшие его значения делают гауссиан уже, тогда как бóльшие значе-
ния делают ее шире. В настоящий момент я зафиксирую данный параметр на
определенных значениях, и вы сможете обследовать последствия изменения
указанного параметра в упражнении 7.6.



                                                                       Результаты
       Изображение
                                        Ядро




            Изображение                                             Результаты



                                           Ядро




                          Рис. 7.4  Механизм свертки изображения

  Вот как эта формула переводится в исходный код:
Y, X   = np.meshgrid(np.linspace(-3,3,21),
                     np.linspace(-3,3,21))
kernel = np.exp( -(X**2+Y**2) / 20 )
kernel = kernel / np.sum(kernel) # нормализовать

  Решетки X и Y изменяются от –3 до +3 с шагом 21. Параметр ширины
жестко задан равным 20. Третья строка нормализует значения в ядре таким
                                            Обнаружение признаков изображения  137

образом, чтобы сумма по всему ядру была равна 1. Это позволяет поддержи-
вать изначальную шкалу данных. При надлежащей нормализации каждый
шаг свертки – и, следовательно, каждый пиксел в отфильтрованном изобра-
жении – становится средневзвешенным значением окружающих пикселов,
при этом веса определяются гауссианом.
  Вернемся к текущей задаче: мы сгладим мат­ри­цу случайных чисел ана-
логично тому, как мы сглаживали временной ряд случайных чисел в главе 4.
Матрица случайных чисел, гауссово ядро и результат свертки показаны на
рис. 7.4.
  Следующий ниже исходный код Python показывает реализацию свертки
изображения. И вновь вспомните свертку временного ряда в главе 4, чтобы
понять, что идея та же самая, но с дополнительным геометрическим изме-
рением, требующим дополнительного цикла for:
for rowi in range(halfKr,imgN-halfKr):     # прокрутить строки
    for coli in range(halfKr,imgN-halfKr): # прокрутить столбцы

       # вырезать кусок изображения
       pieceOfImg = imagePad[ rowi-halfKr:rowi+halfKr+1:1,
                              coli-halfKr:coli+halfKr+1:1 ]

       # точечное произведение:
       # адамарово умножение и суммирование
       dotprod = np.sum( pieceOfImg*kernel )

       # сохранить результат для этого пиксела
       convoutput[rowi,coli] = dotprod

  Реализация свертки в виде двойного цикла for на самом деле неэффектив-
на с вычислительной точки зрения. Оказывается, что свертку можно реали-
зовать быстрее и с меньшим объемом исходного кода в частотном диапазоне.
Это происходит благодаря теореме о свертке, которая гласит, что свертка во
временном (или пространственном) диапазоне равна умножению в частот-
ном диапазоне. Полное изложение теоремы о свертке выходит за рамки этой
книги; я упоминаю о ней здесь для того, чтобы обосновать рекомендацию
использовать функцию SciPy convolve2d вместо двойного цикла for, в особен-
ности в случае больших изображений.
  Давайте попробуем сгладить реальный фотоснимок. Мы будем использо-
вать фотографию музея Стеделийк в Амстердаме, которую я с любовью на-
зываю «ванна из космоса». Это изображение является трехмерной матрицей,
поскольку в нем есть строки, столбцы и глубина – глубина содержит значения
интенсивности пикселов из красного, зеленого и синего цветовых каналов.
Указанное изображение хранится в виде мат­ри­цы размером ℝ1675×3000×3. Фор-
мально она называется тензором, потому что является кубом, а не развер-
нутой таблицей чисел.
  Пока что мы сведем ее к двумерной мат­ри­це путем преобразования в от-
тенки серого. Это упрощает вычисления, хотя в этом нет необходимости.
В упражнении 7.5 вы узнаете, как сглаживать трехмерное изображение. На
рис. 7.5 показано изображение в оттенках серого до и после применения
гауссова ядра сглаживания.
138  Применения матриц




        Рис. 7.5  Фотография музея-ванны до и после сносного сглаживания

   В обоих этих примерах использовалось гауссово ядро. Сколько существует
других ядер? Бесконечное число! В упражнении 7.7 вы протестируете два
других ядра, которые применяются для выявления вертикальных и горизон-
тальных линий. Эти детекторы признаков широко применяются в обработ-
ке изображений (и используются нейронами мозга для обнаружения краев
в узорах света, попадающих на сетчатку).
   Ядра свертки изображений являются главнейшей темой в компьютерном
зрении. Собственно говоря, невероятная результативность сверточных ней-
ронных сетей (архитектуры глубокого обучения, оптимизированной под
компьютерное зрение) полностью обусловлена способностью сети искус-
но создавать оптимальные фильтерные ядра посредством автоматического
обуче­ния.



Резюме
Буду говорить просто: вся суть в том, – повторю это еще раз – что невероятно
важные и изощренные методы в науке о данных и машинном обучении по-
строены на простых линейно-алгебраических принципах.



Упражнения по программированию
Упражнения по матрицам ковариаций
и корреляций
  Упражнение 7.1
  В данном упражнении вы преобразуете мат­ри­цу ковариаций в мат­ри­цу
корреляций. Процедура предусматривает деление каждого элемента мат­ри­
цы (то есть ковариации между каждой парой переменных) на произведение
дисперсий этих двух переменных.
                                             Упражнения по программированию  139

  Указанная процедура реализуется путем пред- и постпозиционного умно­
жения мат­ри­цы ковариаций на диагональную мат­ри­цу, содержащую инвер-
тированные стандартные отклонения каждой переменной (стандартное от-
клонение – это квадратный корень из дисперсии). Стандартные отклонения
инвертируются, потому что нам нужно делить на отклонения, хотя мы будем
умножать мат­ри­цы. Причиной пред- и постпозиционного умножения на
стандартные отклонения является особое свойство пред- и постпозиционно-
го умножения на диагональную мат­ри­цу, которое было объяснено в упраж-
нении 5.11. Уравнение 7.1 показывает формулу.

  Упражнение 7.1. Корреляция из ковариации

  R = SCS.

  C – это мат­ри­ца ковариаций, а S – диагональная мат­ри­ца взаимообратных
стандартных отклонений по каждой переменной (то есть -я диагональ равна
1/σi, где σi – это стандартное отклонение переменной i).
  В данном упражнении ваша цель – вычислить мат­ри­цу корреляций из мат­
ри­цы ковариацй, переложив уравнение 7.1 в исходный код Python. И тогда
вы сможете воспроизвести рис. 7.6.

       Матрица ковариаций данных                    Матрица корреляций данных




                           Рис. 7.6  Решение упражнения 7.1

  Упражнение 7.2
  В библиотеке NumPy есть функция np.corrcoef(), которая возвращает мат­
ри­цу корреляций с учетом входной мат­ри­цы данных. Примените эту функ-
цию, чтобы воспроизвести мат­ри­цу корреляций, созданную вами в предыду-
щем упражнении. Покажите обе мат­ри­цы и их разницу на рисунке, подобном
рис. 7.7, чтобы подтвердить их одинаковость.
  Далее проинспектируйте исходный код функции np.corrcoef(), выполнив
??np.corrcoef(). В NumPy используется несколько иная реализация трансли-
руемого деления на стандартные отклонения вместо пред- и постпозицион-
140  Применения матриц

ного умножения на диагональную мат­ри­цу, состоящую из инвертированных
стандартных отклонений, но вы должны суметь понять, как их реализация
в исходном коде соответствует математике и исходному коду Python, кото-
рый вы написали в предыдущем упражнении.

   Моя мат­ри­ца корреляций        Матрица корреляций NumPy        Матрица разниц




                               Рис. 7.7  Решение упражнения 7.2.
                       Обратите внимание на разницу в шкалировании цвета



Упражнения по геометрическим
преобразованиям
  Упражнение 7.3
  Цель этого упражнения – показать точки в круге до и после применения
преобразования, аналогично тому, как я показал линию до и после поворота
на рис. 7.2. Примените следующую ниже мат­ри­цу преобразования, а затем
создайте график, который выглядит как на рис. 7.8:




  Упражнение 7.4
  Теперь перейдем к еще одному фильму. Я называю его «Сворачивающая-
ся ДНК». На рис. 7.9 показан один кадр фильма. Процедура та же, что и для
«Шаткого круга», – настроить рисунок, создать функцию Python, которая
применяет трансформационную мат­ри­цу к мат­ри­це координат, сообщить
библиотеке matplotlib, что нужно создать анимацию с использованием этой
функции. Используйте следующую ниже трансформационную мат­ри­цу:




  –1 £ φ £ 1.
                   Упражнения по программированию  141


                                Изначальный
                                Преобразованный




Рис. 7.8  Решение к упражнению 7.3




Рис. 7.9  Решение к упражнению 7.4
142  Применения матриц


Упражнения по обнаружению признаков
изображения
  Упражнение 7.5
  Сгладьте трехмерное изображение музея-ванны (если вам нужна подсказ-
ка, то обратитесь к сноске1).
  Результат работы функции convolve2d имеет тип данных float64 (вы можете
убедиться в этом сами, введя variableName.dtype). Однако plt.imshow выдаст
предупреждение об отсечении числовых значений, и изображение не будет
отображаться надлежащим образом. Следовательно, вам нужно будет преоб-
разовать результат свертки в uint8.

  Упражнение 7.6
  Для каждого цветового канала вовсе не нужно использовать одно и то же
ядро. Измените параметр ширины гауссианы по каждому каналу в соответ-
ствии со значениями, показанными на рис. 7.10. Эффект на изображении
незаметен, но разные размытия разных значений цвета придают ему не-
много трехмерный вид, как будто вы смотрите на красно-синий анаглиф
без очков.

        Ширина 0.5 (канал R)              Ширина 5 (канал G)              Ширина 50 (канал B)




                         Рис. 7.10  Ядра для каждого цветового канала,
                                  используемые в упражнении 7.6

  Упражнение 7.7
  В техническом плане сглаживание изображения – это извлечение при-
знаков, поскольку оно предусматривает извлечение сглаженных признаков
сигнала при одновременном ослаблении резких признаков. Здесь мы изме-
ним фильтерные ядра, чтобы решить другие задачи обнаружения признаков
изображения: выявление горизонтальных и вертикальных линий.
  Два ядра показаны на рис. 7.11, как и их влияние на изображение. Два
ядра можно создать вручную, основываясь на их внешнем виде; они имеют
размер 3×3 и содержат только числа –1, 0 и +1. Примените свертку с этими

1
    Подсказка: сглаживайте каждый цветовой канал по отдельности.
                                           Упражнения по программированию  143

ядрами к двумерному фотоснимку в оттенках серого, чтобы создать карты
признаков, показанные на рис. 7.11.

          Вертикальное ядро                             Горизонтальное ядро




                       Рис. 7.11  Результаты упражнения 7.7
Глава          8
                     Обратные матрицы

Мы движемся к решению матричных уравнений. Матричные уравнения по-
хожи на обычные уравнения (например, найти решение для x в 4x = 8), но...
в них есть мат­ри­цы. К этому моменту книги вы хорошо понимаете, что когда
в дело вступают мат­ри­цы, все становится сложнее. Тем не менее мы должны
принять эту сложность как само собой разумеющееся, потому что решение
матричных уравнений – огромная часть науки о данных.
   Обратная мат­ри­ца играет центральную роль в решении матричных урав-
нений в практических приложениях, включая подгонку статистических
моделей к данным (подумайте об общих линейных моделях и регрессии).
К концу этой главы вы поймете, что такое обратная мат­ри­ца, когда ее можно
и нельзя вычислить, как ее вычислять и как ее интерпретировать.



Обратная матрица
Матрица, обратная мат­ри­це A, – это еще одна мат­ри­ца A–1 (произносится как
«обратная мат­ри­ца, или инверсная мат­ри­ца, мат­ри­цы A»), которая умножает
мат­ри­цу A, производя единичную мат­ри­цу. Другими словами, A–1A = I. Имен-
но так мат­ри­ца «отменяется». Еще одна концептуализация заключается в на-
мерении преобразовать мат­ри­цу линейно в единичную мат­ри­цу; обратная
мат­ри­ца содержит это линейное преобразование, а матричное умножение
является механизмом применения этого преобразования к мат­ри­це.
  Но зачем вообще инвертировать мат­ри­цы? Нам приходится «отменять»
мат­ри­цу, чтобы решать задачи, которые можно выразить в форме Ax = b, где
A и b – это известные величины, и мы хотим найти решение для x. Решение
имеет следующую ниже общую форму:

     Ax = b;
  A–1Ax = A–1b;
      Ix = A–1b;
       x = A–1b.
                              Типы обратных матриц и условия обратимости  145

  Она выглядит очень простой, но, как вы скоро узнаете, вычислять обрат-
ную мат­ри­цу обманчиво трудно.



Типы обратных матриц и условия обратимости
«Инвертировать мат­ри­цу» звучит так, как будто это должно работать всегда.
Кому не хотелось бы инвертировать мат­ри­цу, если это удобно? К сожалению,
жизнь не всегда так проста: не все мат­ри­цы можно инвертировать.
  Есть три разных вида обратных мат­риц с разными условиями обратимо-
сти. Они представлены ниже; их подробности находятся в следующих далее
разделах:

Полная обратная мат­ри­ца
  Она означает, что A–1A = AA–1 = I. Наличие у мат­ри­цы полной обратной
  мат­ри­цы обусловливается двумя свойствами: (1) она должна быть квад­
  ратной и (2) полноранговой. Каждая квадратная полноранговая мат­ри­ца
  имеет обратную мат­ри­цу, и каждая мат­ри­ца, имеющая полную обратную
  мат­ри­цу, является квадратной и полноранговой. Между прочим, здесь
  я использую термин «полная обратная мат­ри­ца», чтобы отличить ее от
  следующих ниже двух возможностей; полная обратная мат­ри­ца обычно
  называется просто обратной матрицей.

Односторонняя обратная мат­ри­ца
  Односторонняя обратная мат­ри­ца может преобразовывать прямоуголь-
  ную мат­ри­цу в единичную мат­ри­цу, но она работает только для одного
  порядка умножения. В частности, высокая мат­ри­ца T может иметь лево-
  обратную мат­ри­цу, то есть LT = I, но TL ≠ I. А широкая мат­ри­ца W может
  иметь правообратную мат­ри­цу, то есть WR = I, но RW ≠ I.
  Неквадратная мат­ри­ца имеет одностороннюю обратную мат­ри­цу только
  в том случае, если она имеет максимально возможный ранг. То есть высо-
  кая мат­ри­ца имеет левообратную мат­ри­цу, если она имеет ранг N (полный
  столбцовый ранг), тогда как широкая мат­ри­ца имеет правообратную мат­
  ри­цу, если она имеет ранг M (полный строчный ранг).

Псевдообратная мат­ри­ца
  Каждая мат­р и­ц а имеет псевдообратную мат­р и­ц у, независимо от ее
  формы и ранга. Если мат­р и­ц а является квадратной полноранговой, то
  псевдообратная мат­р и­ц а равна полной обратной мат­р и­ц е. Точно так же
  если мат­р и­ц а является неквадратной и имеет максимально возможный
  ранг, то псевдообратная марица равна левообратной мат­р и­ц е (для вы-
  сокой мат­р и­ц ы) или правообратной мат­р и­ц е (для широкой мат­р и­ц ы).
  Но рангово-пониженная мат­р и­ц а по-прежнему имеет псевдообратную
  мат­ри­цу, и в этом случае псевдообратная мат­ри­ца преобразовывает син-
  гулярную мат­р и­ц у в еще одну мат­р и­ц у, близкую, но не равную единич-
  ной мат­р и­ц е.
146  Обратные матрицы

  Матрицы, не имеющие ни полной, ни односторонней обратной мат­ри­цы,
называются сингулярными, или необратимыми. Это то же самое, что поме-
тить мат­ри­цу как рангово-пониженную или рангово-дефицитную.



Вычисление обратной матрицы
Обратная мат­ри­ца звучит великолепно! Как ее вычислять? Давайте начнем,
подумав о том, как вычислить обратное значение скаляра: надо просто ин-
вертировать его число (взять его взаимообратную величину). Например,
число, обратное числу 3, равно 1/3, что то же самое, что 3–1. Тогда 3×3–1 = 1.
  Основываясь на этом рассуждении, можно догадаться, что взятие обрат-
ной мат­ри­цы работает точно так же: инвертировать каждый элемент мат­ри­
цы. Давай попробуем:




  К сожалению, это не произведет желаемого результата, что легко проде-
монстрировать, умножив изначальную мат­ри­цу на мат­ри­цу отдельно инвер-
тированных элементов:




  Это допустимое умножение, но оно не дает единичной мат­ри­цы, и, стало
быть, мат­ри­ца с отдельно инвертированными элементами не является об-
ратной матрицей.
  Существует алгоритм вычисления мат­ри­цы для любой обратимой мат­ри­
цы. Он долгий и утомительный (вот почему у нас есть компьютеры, которые
делают всю числодробительную работу за нас!), но имеется несколько со-
кращенных версий для специальных мат­риц.


Обратная матрица матрицы 2×2
Для того чтобы инвертировать мат­ри­цу 2×2, надо поменять местами диаго-
нальные элементы, умножить внедиагональные элементы на −1 и разделить
на определитель. Этот алгоритм создаст мат­ри­цу, которая преобразовывает
изначальную мат­ри­цу в единичную.
  Проследите за следующей ниже последовательностью уравнений:
                                                        Вычисление обратной матрицы  147




  Давайте проработаем числовой пример:




  Сработало замечательно.
  Вычислить обратую мат­ри­цу на Python очень просто:
A = np.array([ [1,4],[2,7] ])
Ainv = np.linalg.inv(A)
A@Ainv

  Вы можете подтвердить, что A@Ainv дает единичную мат­ри­цу, как и Ainv@A.
Конечно же, A*Ainv не дает единичной мат­ри­цы, потому что операция * озна­
чает адамарово (поэлементное) умножение.
  Матрица, обратная ей мат­ри­ца и их произведение представлены на рис. 8.1.

                Матрица                  Обратная матрица          Их произведение




                          Рис. 8.1  Обратная мат­ри­ца мат­ри­цы 2×2

  Давайте попробуем еще один пример:
148  Обратные матрицы

   В этом примере есть несколько проблем. Умножение мат­риц вместо I
дает 0. Но тут есть более крупная проблема – определитель равен нулю! Мате-
матики веками предупреждали о том, что на ноль делить нельзя. Так давайте
же не будем начинать это делать сейчас.
   Чем отличается второй пример? Это рангово-пониженная мат­ри­ца (ранг =
1). В нем показан числовой пример того, что рангово-пониженные мат­ри­цы
необратимы.
   Что делает Python в этом случае? Давайте выясним:
A = np.array([ [1,4],[2,8] ])
Ainv = np.linalg.inv(A)
A@Ainv

  Python даже не будет пытаться вычислить результат, как это сделал я.
Вмес­то этого он выдаст ошибку со следующим ниже сообщением1:
LinAlgError: Singular matrix

  Рангово-пониженные мат­ри­цы не имеют обратной мат­ри­цы, и такие про-
граммы, как Python, даже не будут пытаться ее вычислить. Однако эта мат­ри­
ца имеет псевдообратную мат­ри­цу. Я вернусь к ней через несколько разделов.


Обратная матрица диагональной матрицы
Существует также сокращенная версия вычисления обратной мат­ри­цы квад­
ратной диагональной мат­ри­цы. К этому сокращению ведет понимание, что
произведение двух диагональных мат­риц – это просто умножение диаго-
нальных элементов на скаляр (обнаруженное в упражнении 5.12). Рассмот­
рим приведенный ниже пример; прежде чем продолжить чтение текста,
попробуйте найти сокращенное вычисление обратной мат­ри­цы для диа-
гональной мат­ри­цы:




  Вы поняли, как вычислять обратную мат­ри­цу диагональной мат­ри­цы?
Хит­рость в том, что надо просто инвертировать каждый диагональный эле-
мент, игнорируя внедиагональные нули. Это ясно из предыдущего примера,
если задать b = 1/2, c = 1/3 и d = 1/4.
  Что произойдет, если у вас диагональная мат­ри­ца с нулем по диагонали?
Инвертировать этот элемент не получится, потому что у вас будет 1/0. Та-
ким образом, диагональная мат­ри­ца хотя бы с одним нулем по диагонали
не имеет обратной мат­ри­цы. (Также впомните из главы 6, что диагональная
мат­ри­ца является полноранговой, только если все диагональные элементы
отличны от нуля.)

1
    Линейно-алгебраическая ошибка: сингулярная мат­ри­ца. – Прим. перев.
                                             Вычисление обратной матрицы  149

  Обратная мат­ри­ца диагональной важна тем, что она напрямую приводит
к формуле вычисления псевдообратной мат­ри­цы. Подробнее об этом позже.


Инвертирование любой квадратной
полноранговой матрицы
Честно говоря, я сомневался, стоит ли включать этот раздел в главу. Полный
алгоритм инвертирования обратимой мат­ри­цы – длинный и утомительный,
и вам никогда не понадобится его использовать в приложениях (вместо этого
вы будете использовать функцию np.linalg.inv либо другие функции, вы-
зывающие inv).
   С другой стороны, реализация алгоритма на Python – это для вас отличная
возможность попрактиковаться в переложении в исходный код Python ал-
горитма, описанного в уравнениях и на естественном языке. Поэтому здесь
я объясню принцип работы алгоритма, не показывая никакого исходного
кода. Я призываю вас реализовать алгоритм программно на языке Python, по
ходу чения этого раздела, и потом вы сможете сравнить свое решение с моим
в упражнении 8.2 онлайнового исходного кода.
   Алгоритм вычисления обратной мат­ри­цы предусматривает четыре про-
межуточные мат­ри­цы, именуемые матрицей миноров, матрицей-решеткой,
матрицей кофакторов и матрицей адъюгатов.

Матрица миноров
 Эта мат­ри­ца содержит определители подматриц. Каждый элемент mi,j мат­
 ри­цы миноров является определителем подматрицы, созданной путем
 исключения i-й строки и j-го столбца. На рис. 8.2 показан общий вид про-
 цедуры для мат­ри­цы 3×3.




                    Рис. 8.2  Вычисление мат­ри­цы миноров
                 (области, окрашенные серым цветом, удаляются,
                       чтобы создать каждую подматрицу)
150  Обратные матрицы

Матрица-решетка
 Матрица-решетка – это шахматная доска с чередующимися значениями
 +1 и –1. Она рассчитывается по следующей ниже формуле:
    gi, j = –1i+j.
    При реализации этой формулы на Python будьте осмотрительны с индек-
    сацией и возведением в степень. Вы должны тщательно проверить мат­ри­
    цу, чтобы убедиться, что это шахматная доска с чередующимися знаками,
    с +1 в верхнем левом элементе.

Матрица кофакторов
 Матрица кофакторов – это адамарово умножение мат­ри­цы миноров на
 мат­ри­цу-решетку.

Матрица адъюгатов1
 Это транспонированная версия мат­ри­цы кофакторов, скалярно умножен-
 ной на взаимообратное значение определителя изначальной мат­ри­цы
 (мат­ри­цы, матричную инверсию которой вы вычисляете, а не мат­ри­цы
 кофакторов).
 Матрица адъюгатов является инверсией изначальной мат­ри­цы.

  На рис. 8.3 показаны четыре промежуточные мат­ри­цы, а также обратная
мат­ри­ца, возвращенная функцией np.linalg.inv, и единичная мат­ри­ца, по-
лученная в результате умножения изначальной мат­ри­цы на обратную ей
мат­ри­цу, вычисленную в соответствии с ранее описанной процедурой. Из-
начальная мат­ри­ца была матрицей случайных чисел.

      Матрица миноров                       Матрица-решетка                Матрица кофакторов




Матрица адъюгатов (инверсия)                  np.linalg.inv                     A@Ainv




                                  Рис. 8.3  Визуализация мат­риц,
                        продуцирующих инверсию для мат­ри­цы случайных чисел

1
    Англ. adjugate; син. примыканий, присоединений. – Прим. перев.
                                              Вычисление обратной матрицы  151


Односторонние обратные матрицы
Высокая мат­ри­ца не имеет полной обратной мат­ри­цы. То есть для мат­ри­цы T
размера M > N не существует высокой мат­ри­цы T–1 такой, что TT–1 = T–1T = I.
  Но существует мат­ри­ца L такая, что LT–1. Сейчас наша цель – найти эту
мат­ри­цу. Начнем с того, что сделаем мат­ри­цу T квадратной. Как превратить
неквадратную мат­ри­цу в квадратную? Разумеется, вы знаете ответ – ее надо
умножить на ее транспонированную версию.
  Возникает следующий вопрос: что вычислять – TTT или TTT? Обе являются
квадратными… но TTT – полноранговая, если T имеет полный столбцовый
ранг. И почему это важно? Как вы уже догадались, все квадратные полно-
ранговые мат­ри­цы имеют обратную мат­ри­цу. Прежде чем представить ма-
тематический вывод левообратной мат­ри­цы, давайте продемонстрируем на
Python, что высокая мат­ри­ца, умноженная на ее транспонированную версию,
имеет полную обратную мат­ри­цу:
T = np.random.randint(-10, 11, size=(40,4))
TtT = T.T @ T
TtT_inv = np.linalg.inv(TtT)
TtT_inv @ TtT

  В приведенном выше исходном коде можно подтвердить, что последняя
строка создает единичную мат­ри­цу (в пределах машинно зависимой ошибки
прецизионности).
  Давайте я переложу этот исходный код Python в математическое уравнение:

  (TTT)–1(TTT) = I.

   Из исходного кода и формулы видно, что поскольку TTT – это не та же мат­
ри­ца, что и T, (TTT)–1 не является обратной матрицей мат­ри­цы T.
   Но – и вот ключевой момент – мы ищем мат­ри­цу, которая умножает T
с левой стороны, чтобы получить единичную мат­ри­цу; нас на самом деле не
волнует, какие другие мат­ри­цы нужно умножить, чтобы получить эту мат­ри­
цу. Тогда давайте разобьем и перегруппируем умножения мат­риц:

    L = (TTT)–1TT = I;
  LT = I.

  Эта мат­ри­ца L является левообратной матрицей мат­ри­цы T.
  Теперь можно завершить исходный код Python вычисления левообрат-
ной мат­ри­цы и подтвердить, что он соответствует нашей спецификации.
Умножьте изначальную высокую мат­ри­цу с левой стороны, чтобы получить
единичную мат­ри­цу:
L = TtT_inv @ T.T # левообратная мат­ри­ца
L@T # производит единичную мат­ри­цу

  Вы также можете подтвердить на Python, что TL (то есть умножение на
левообратную мат­ри­цу справа) не дает единичную мат­ри­цу. Вот почему ле-
вообратная мат­ри­ца является односторонней обратной матрицей.
152  Обратные матрицы

  На рис. 8.4 показаны высокая мат­ри­ца, ее левообратная мат­ри­ца и два
способа умножения левообратной мат­ри­цы на мат­ри­цу. Обратите внимание,
что левообратная мат­ри­ца не является квадратной и что постпозиционное
умножение на левообратную мат­ри­цу дает результат, который определенно
не является единичной матрицей.

            Высокая мат­ри­ца




                                                       Левообратная мат­ри­ца




                  L@T                                            T@L




                    Рис. 8.4  Визуализация левообратной мат­ри­цы

  Левообратная мат­ри­ца имеет чрезвычайную важность. На самом деле, по-
сле того как вы узнаете о подгонке статистических моделей к данным и мето-
де наименьших квадратов, вы повсюду будете встречать левообратную мат­
ри­цу. Не будет преувеличением сказать, что левообратная мат­ри­ца – один
из самых важных вкладов линейной алгебры в современную человеческую
цивилизацию.
                                            Уникальность обратной матрицы  153

   Заключительное замечание о левообратной мат­ри­це, которое присутство-
вало в этом изложении неявно: левообратная мат­ри­ца определена только для
высоких мат­риц с полным столцовым рангом. Матрица размера M > N ранга
r < N не имеет левообратной мат­ри­цы. Почему? Ответ – в сноске1.
   Теперь вы научились вычислять левообратную мат­ри­цу. А что насчет пра-
вообратной? Я отказываюсь учить вас ее вычислять! И не потому, что скры-
ваю от вас тайное знание, и уж точно не потому, что вы мне не нравитесь.
Наоборот, я хочу, чтобы вы бросили вызов самому себе, математически вы-
ведя правообратную мат­ри­цу и продемонстрировав ее в исходном коде с ис-
пользованием Python. В упражнении 8.4 есть несколько подсказок, если они
вам понадобятся, либо вы можете попробовать разобраться с этим вопросом
сейчас, перед тем как переходить к следующему разделу.



Уникальность обратной матрицы
Обратная мат­ри­ца уникальна, а это означает, что если мат­ри­ца имеет об-
ратную мат­ри­цу, то она имеет ровно одну обратную мат­ри­цу. Не может быть
двух мат­риц B и C таких, что AB = I и AC = I и B ¹ C.
   Есть несколько доказательств этого утверждения. То, которое я покажу,
основано на технике, именуемой доказательством путем отрицания. Оно
означает, что мы пытаемся, но не можем доказать ложное утверждение, тем
самым доказывая правильное утверждение. В данном случае мы начинаем
с трех допущений:
    1) мат­ри­ца A обратима;
    2) мат­ри­цы B и C – обратные мат­ри­цы мат­ри­це A;
    3) мат­ри­цы B и C различны, то есть B ¹ C.
  Следуйте каждому выражению слева направо и обратите внимание, что
каждое последующее выражение основано на добавлении или удалении еди-
ничной мат­ри­цы, выраженной как мат­ри­ца, умноженная на обратную ей
мат­ри­цу:

    C = CI = CAB = IB = B.

  Все утверждения эквивалентны, и, значит, первое и последнее выражения
эквивалентны, и, стало быть, наше допущение о том, что B ¹ C, ложно. Вы-
вод: любые две мат­ри­цы, которые претендуют на то, чтобы быть обратными
мат­ри­ца­ми одной и той же мат­ри­цы, эквивалентны. Другими словами, об-
ратимая мат­ри­ца имеет ровно одну обратную мат­ри­цу.




1
    Потому что TTT является рангово-пониженной и, следовательно, инвертировать
    ее невозможно.
154  Обратные матрицы


Псевдообратная матрица Мура–Пенроуза
Как я писал ранее, преобразовать рангово-пониженную мат­ри­цу в единич-
ную мат­ри­цу путем умножения мат­риц просто невозможно. Это означает,
что рангово-пониженные мат­ри­цы не имеют ни полной, ни односторонней
обратной мат­ри­цы. Но вот сингулярные мат­ри­цы имеют псевдообратные
мат­ри­цы. Псевдообратные мат­ри­цы – это трансформационные мат­ри­цы,
приближающие мат­ри­цу к единичной мат­ри­це.
  Термин псевдобратные мат­ри­цы во множественном числе – это не опечат-
ка: в отличие от уникальности полной обратной мат­ри­цы, псевдообратная
мат­ри­ца не уникальна. Рангово-пониженная мат­ри­ца имеет бесконечное
число псевдообратных мат­риц. Но некоторые из них лучше, чем другие, и на
самом деле обсуждать стоит только одну псевдообратную мат­ри­цу, потому
что она, скорее всего, будет единственной, которую вы когда-либо будете
использовать.
  Она называется псевдообратной матрицей Мура–Пенроуза, иногда сокра-
щенно именуемой псевдообратной матрицей MP. Но поскольку она, безус­
ловно, является наиболее часто используемой псевдообратной матрицей,
можно всегда исходить из того, что термин псевдообратная мат­ри­ца отно-
сится к псевдообратной мат­ри­це Мура–Пенроуза.
  Следующая ниже мат­ри­ца является псевдообратной матрицей сингуляр-
ной мат­ри­цы, которую вы встречали ранее в этой главе. Первая строка по-
казывает псевдообратную мат­ри­цу мат­ри­цы, а вторая строка – произведение
мат­ри­цы и псевдообратной ей мат­ри­цы:




(Шкалирующий коэффициент 85 был извлечен, чтобы облегчить визуальный
осмотр мат­ри­цы.)
  Псевдообратная мат­ри­ца обозначается надстрочным крестиком, знаком
плюс либо звездочкой: A†, A+ или A*.
  В Python получение псевдообратной мат­ри­цы реализовано в функции
np.linalg.pinv. Следующий ниже исходый код вычисляет псевдообратную
мат­ри­цу сингулярной мат­ри­цы, для которой функция np.linalg.inv выдала
сообщение об ошибке:
A = np.array([ [1,4],[2,8] ])
Apinv = np.linalg.pinv(A)
A@Apinv

  Каков алгоритм вычисления псевдообратной мат­ри­цы? Он непостижимо
труден либо интуитивно понятен; все зависит от вашего понимания сингу-
лярного разложения. Я кратко объясню вычисление псевдообратной мат­
                                     Численная стабильность обратной матрицы  155

ри­цы, но если вы его не поймете, то, пожалуйста, не беспокойтесь: обещаю,
что к концу главы 13 данный алгоритм станет интуитивно понятным. Для
того чтобы вычислить псевдообратную мат­ри­цу, надо взять SVD мат­ри­цы,
инвертировать ненулевые сингулярные числа без изменения сингулярных
векторов и реконструировать мат­ри­цу путем умножения UΣ+VT.



Численная стабильность обратной матрицы
При вычислении обратной мат­ри­цы задействуется много FLOP’ов1 (опера-
ций с плавающей точкой), включая большое число определителей. В главе 6
вы узнали, что определитель мат­ри­цы может быть численно нестабильным,
поэтому вычисление многочисленных определителей приводит к численным
неточностям, которые могут накапливаться и вызывать значительные проб­
лемы при работе с большими мат­ри­ца­ми.
   По этой причине в низкоуровневых библиотеках, в которых реализованы
численные вычисления (таких как LAPACK), обычно, когда это возможно,
стараются избегать явного инвертирования мат­риц либо разлагают мат­
ри­ц ы на произведение других мат­р иц, которые являются более числен-
но стабильными (например, путем QR-разложения, о котором вы узнаете
в главе 9).
   Матрицы, числовые значения которых находятся примерно в одном диа-
пазоне, тяготеют к большей стабильности (хотя это и не гарантируется), по-
этому с мат­ри­ца­ми случайных чисел легко работать. Но мат­ри­цы с большим
диапазоном числовых значений имеют высокий риск численной нестабиль-
ности. «Диапазон числовых значений» более формально определяется как
кондиционное число мат­ри­цы, которое представляет собой отношение наи-
большего сингулярного числа к наименьшему. Подробнее о кондиционном
числе вы узнаете в главе 14; пока же достаточно сказать, что кондиционное
число является мерой разброса числовых значений в мат­ри­це2.
   Примером численно нестабильной мат­ри­цы является мат­ри­ца Гильберта.
Каждый элемент в мат­ри­це Гильберта определяется простой формулой, по-
казанной в уравнении 8.1.

    Уравнение 8.1. Формула создания мат­ри­цы Гильберта. i и j – это индексы
    строк и столбцов




1
    Англ. floating-point operations. – Прим. перев.
2
    Англ. condition number; син. число обусловленности мат­ри­цы, кондиция мат­ри­цы.
    В частности, кондиционное число мат­ри­цы определяет степень чувствительности
    ответа к возмущениям во входных данных и к ошибкам округления, допущенным
    в процессе решения. – Прим. перев.
156  Обратные матрицы

  Вот пример мат­ри­цы Гильберта 3×3:




  По мере того как мат­ри­ца становится все больше, диапазон числовых зна-
чений увеличивается. Как следствие рассчитываемая компьютером мат­ри­ца
Гильберта быстро становится рангово-дефицитной. Даже полноранговые
мат­ри­цы Гильберта имеют обратные мат­ри­цы в совершенно другом число-
вом диапазоне. Это проиллюстрировано на рис. 8.5, где показаны мат­ри­ца
Гильберта 5×5, обратная ей мат­ри­ца и их произведение.

                                                                 Их произведение
   Матрица Гильберта                 Inv(hilbert)




                           Рис. 8.5  Матрица Гильберта,
                       обратная ей мат­ри­ца и их произведение

   Матрица произведения, безусловно, выглядит как единичная мат­ри­ца, но
в упражнении 8.9 вы обнаружите, что ее внешний вид бывает обманчивым,
а ошибки округления резко возрастают с увеличением размера мат­ри­цы.



Геометрическая интерпретация обратной
матрицы
В главах 6 и 7 вы научились концептуализировать умножение мат­ри­цы на
вектор как геометрическое преобразование вектора или множества точек.
  Следуя в этом направлении, обратную мат­ри­цу можно трактовать как от-
кат назад геометрического преобразования, вызванного матричным умно-
жением. На рис. 8.6 показан пример, вытекающий из рис. 7.8; я просто умно­
жил преобразованные геометрические координаты на обратную мат­ри­цу
трансформационной мат­ри­цы.
  Этот геометрический эффект перестанет быть удивительным после обсле-
дования математики. В следующих ниже уравнениях P – это мат­ри­ца 2×N
изначальных геометрических координат, T – трансформационная мат­ри­ца,
                              Геометрическая интерпретация обратной матрицы  157

Q – мат­ри­ца преобразованных координат, а U – мат­ри­ца обратно преобра-
зованных координат:

    Q = TP;
    U = T–1Q;
    U = T–1TP.


                                            Изначальная
                                            Преобразованная
                                            Обратно преобразованная




                            Рис. 8.6  Обратная мат­ри­ца
                   откатывает назад геометрическое преобразование.
                         Исходный код создания этого рисунка
                           является частью упражнения 8.8

   Хотя этот результат и не удивителен, надеюсь, что он поможет сформиро-
вать некоторое геометрическое понимание предназначения обратной мат­
ри­цы на интуитивном уровне: откат назад вызванного матрицей результата
преобразования. Эта интерпретация пригодится, когда вы узнаете о диаго-
нализации мат­ри­цы посредством собственного разложения.
   Указанная геометрическая интерпретация также дает некоторое интуи-
тивное понимание причины, по которой рангово-пониженная мат­ри­ца не
имеет обратной мат­ри­цы: геометрический эффект преобразования с по-
мощью сингулярной мат­р и­ц ы заключается в том, что по меньшей мере
одно измерение сглаживается. Когда измерение сглажено, это сглаживание
нельзя отменить, точно так же, как невозможно увидеть свою спину, глядя
в зеркало1.

1
    Где-то здесь должна быть остроумная аналогия с Плосколандией, которую я не
    совсем в состоянии красноречиво высказать. Лучше уж прочтите книгу «Плоско-
    ландия» (Flatland).
158  Обратные матрицы


Резюме
Мне очень понравилось писать эту главу, и я надеюсь, что вам понравилось
учиться по ней. Вот краткое изложение ключевых выводов, которые следует
вынести из этой главы.
    Обратная мат­ри­ца – это мат­ри­ца, которая преобразовывает макси-
      мально-ранговую мат­ри­цу в единичную мат­ри­цу путем умножения
      мат­риц. Обратная мат­ри­ца имеет много предназначений, включая
      переставление мат­риц в уравнении (например, отыскание решения
      для x в Ax = b).
    Полноранговая квадратная мат­ри­ца имеет полную обратную мат­ри­
      цу, высокая мат­ри­ца полного столбцового ранга имеет левообратную
      мат­ри­цу, а широкая мат­ри­ца полного строчного ранга имеет правооб-
      ратную мат­ри­цу. Линейно преобразовать рангово-пониженные мат­ри­
      цы в единичную мат­ри­цу невозможно, но у них есть псевдообратная
      мат­ри­ца, которая преобразовывает мат­ри­цу в еще одну мат­ри­цу, более
      близкую к единичной мат­ри­це.
    Обратная мат­ри­ца является уникальной – если мат­ри­цу можно линей-
      но преобразовать в единичную, то это можно сделать только одним
      способом.
    Вычисление обратных мат­риц некоторых видов, включая мат­ри­цы 2×2
      и диагональные мат­ри­цы, предусматривает использование нескольких
      сокращенных способов. Указанные способы представляют собой упро-
      щения полной формулы вычисления обратной мат­ри­цы.
    Из-за риска ошибок численной прецизионности в алгоритмах произ-
      водственного уровня предпочитается избегать явного инвертирования
      мат­риц, либо мат­ри­ца в них разлагается на другие мат­ри­цы, которые
      можно инвертировать с большей численной стабильностью.



Упражнения по программированию
  Упражнение 8.1
  Обратная мат­ри­ца обратной мат­ри­цы будет изначальной матрицей; дру-
гими словами, (A–1)–1 = A. Это аналогично следующему: 1/1/a = a. Проил­
люстрируйте это на Python.

  Упражнение 8.2
  Реализуйте полный алгоритм, описанный в разделе «Инвертирование
любой квадратной полноранговой мат­ри­цы» на стр. 149, и воспроизведите
рис. 8.3. Разумеется, из-за случайных чисел ваши мат­ри­цы будут выглядеть
иначе, чем на рис. 8.3, хотя мат­ри­ца-решетка и единичная мат­ри­ца останутся
теми же.
                                               Упражнения по программированию  159

  Упражнение 8.3
  Реализуйте алгоритм генерирования полной обратной мат­ри­цы вручную
для мат­ри­цы 2×2, используя матричные элементы a, b, c и d. Обычно в этой
книге я не даю задания, решаемые вручную, но данное упражнение покажет,
откуда берется сокращенный вариант. Вспомните, что определителем скаля-
ра является его абсолютное значение.

  Упражнение 8.4
  Выведите правообратную мат­ри­цу для широких мат­риц математически, сле-
дуя логике, которая позволила обнаружить левообратную мат­ри­цу. Затем вос-
произведите рис. 8.4 для широкой мат­ри­цы. (Подсказка: начните с исходного
кода для левообратной мат­ри­цы и при необходимости откорректируйте его.)

   Упражнение 8.5
   Проиллюстрируйте на Python, что псевдообратная мат­ри­ца (посредством
функции np.linalg.pinv) равна полной обратной мат­ри­ц е (посредством
функции np.linalg.inv) обратимой мат­ри­цы. Затем проиллюстрируйте, что
псевдообратная мат­ри­ца равна левообратной мат­ри­це высокой мат­ри­цы
с полным столбцовым рангом и что она равна правообратной мат­ри­це ши-
рокой мат­ри­цы с полным строчным рангом.

  Упражнение 8.6
  Правило LIVE EVIL применимо к обратной мат­ри­це умноженных мат­риц.
Проверьте эту применимость в исходном коде, создав две квадратные полно-
ранговые мат­ри­цы A и B, а затем примените евклидово расстояние, чтобы
сравнить
  1) (AB)–1,
  2) A–1B–1 и
  3) B–1A–1.
   Перед тем как приступать к программированию, сделайте предсказание
о том, какие результаты будут эквивалентными. Распечатайте свои резуль-
таты, используя форматирование, подобное приведенному ниже (я опустил
свои результаты, чтобы не влиять на ваше решение!):
Расстояние между (AB)^-1 и (A^-1)(B^-1) равно ___
Расстояние между (AB)^-1 и (B^-1)(A^-1) равно ___

  В качестве дополнительной задачи можно подтвердить, что правило LIVE
EVIL применимо к более длинной цепочке мат­риц, например к четырем
мат­ри­цам вместо двух.

  (TTT)–1(TTT) = I.
160  Обратные матрицы

  Упражнение 8.7
  Применимо ли правило LIVE EVIL к односторонней обратной мат­ри­це? То
есть будет ли верно, что (TTT)–1 = TTT–1? Как и в предыдущем упражнении,
сделайте предсказание, а затем проверьте его на Python.

  Упражнение 8.8
  Напишите исходный код, чтобы воспроизвести рис. 8.6. Начните с копи-
рования исходного кода из упражнения 7.3. После воспроизвения рисунка
сделайте трансформационную мат­ри­цу необратимой, задав нижний левый
элемент равным 1. Что еще нужно изменить в исходном коде, чтобы избежать
ошибок?

   Упражнение 8.9
   Это и следующее упражнения помогут обследовать обратную мат­ри­цу
и ее риск численной нестабильности, используя мат­ри­цу Гильберта. Начни-
те с создания мат­ри­цы Гильберта. Напишите функцию Python, которая на
входе принимает целое число и на выходе создает мат­ри­цу Гильберта, следуя
уравнению 8.1. Затем воспроизведите рис. 8.5.
   Я рекомендую писать вашу функцию Python, используя двойной цикл for
по строкам и столбцам (матричные индексы i и j), следуя математической
формуле. Убедившись в точности функции, можете бросить вызов самому
себе и переписать функцию без циклов for (подсказка: используйте внешнее
произведение). Точность своей функции можно подтвердить, сравнив ее
с функцией Гильберта, которая находится в библиотеке scipy.linalg.

   Упражнение 8.10
   Используя матричную функцию Гильберта, создайте мат­ри­цу Гильберта,
затем вычислите обратную ей мат­ри­цу, применив функцию np.linalg.inv,
и вычислите произведение двух мат­риц. Это произведение должно равнять-
ся единичной мат­ри­це, и, стало быть, евклидово расстояние между этим
произведением и истинной единичной матрицей, произведенной функцией
np.eye, должно быть равно 0 (в пределах ошибки вычислительного округле-
ния). Вычислите это евклидово расстояние.
   Поместите этот исходный код в цикл for по диапазону размеров мат­риц
от 3×3 до 12×12. По каждому размеру мат­ри­цы сохраняйте евклидово рас-
стояние и кондиционное число мат­ри­цы Гильберта. Как я уже писал ранее,
кондиционное число является мерой разброса числовых значений в мат­ри­це
и извлекается функцией np.linalg.cond.
   Затем повторите описанный выше исходный код, но теперь вместо мат­ри­
цы Гильберта используя мат­ри­цу гауссовых случайных чисел.
   Наконец, нанесите все результаты на график, как показано на рис. 8.7.
Я нанес расстояние и кондиционное число в логарифмической шкале, чтобы
облегчить визуальную интерпретацию.
                                                                            Упражнения по программированию  161

       Пожалуйста, прочувствуйте вдохновение и продолжайте обследовать ли-
     нейную алгебру, используя это упражнение! Попробуйте построить мат­ри­цу
     Гильберта, умноженную на обратную ей мат­ри­цу (подумайте о корректиров-
     ке цветовой шкалы), используя мат­ри­цы большего размера либо другие спе-
     циальные мат­ри­цы, извлекая другие свойства мат­ри­цы, такие как ранг или
     норма, и т. д. Вы отправились в путешествие по чудесной стране линейной
     алгебры, полное приключений, и Python – это ковер-самолет, который несет
     вас над ее чудесным ландшафтом.

                                Расстояние до единичной мат­ри­цы                         Кондиционное число мат­ри­цы
                                Гильберт                                                Гильберт
Логарифм евклидова расстояния




                                Случайные                                               Случайные




                                                                       Логарифм каппа




                                            Размер мат­ри­цы                                        Размер мат­ри­цы

                                                    Рис. 8.7  Результаты упражнения 8.10
Глава       9
                            Ортогональные
                                  матрицы
                          и QR-разложение

В этой книге вы познакомитесь с пятью главнейшими разложениями: орто-
гональным разложением векторов, QR-разложением, LU-разложением, соб-
ственным разложением и сингулярным разложением. Это не единственные
разложения в линейной алгебре, но они наиболее важны в науке о данных
и машинном обучении.
   В этой главе вы познакомитесь с QR. И попутно узнаете о новом спе-
циальном типе мат­р и­ц ы (ортогональной). QR-разложение – это рабочая
лошадка, которая приводит в движение приложения, включая получение
обратной мат­р и­ц ы, подгонку моделей методом наименьших квадратов
и получение собственного разложения. Таким образом, понимание и зна-
комство с QR-раз­ложением помогут вам улучшить свои линейно-алгебраи­
ческие навыки.



Ортогональные матрицы
Начну с того, что познакомлю вас с ортогональными мат­ри­ца­ми. Ортого-
нальная мат­ри­ца – это специальная мат­ри­ца, которая важна для нескольких
разложений, включая QR, собственное разложение и сингулярное разложе-
ние. Буква Q часто используется для обозначения ортогональных мат­риц.
Ортогональные мат­ри­цы обладают двумя свойствами:

Ортогональные столбцы
  Все столбцы попарно ортогональны.

Единично-нормные столбцы
  Норма (геометрическая длина) каждого столбца равна 1.
                                                      Ортогональные матрицы  163

  Эти два свойства транслируются в математическое выражение (вспомни-
те, что �a, b� – это альтернативное обозначение точечного произведения):




   Что оно означает? Оно означает, что точечное произведение столбца
с самим собой равно 1, а точечное произведение столбца с любым другим
столбцом равно 0. Мы имеем большое число точечных произведений только
с двумя возможными исходами. Мы можем организовать все точечные про-
изведения среди всех пар столбцов, предпозиционно умножив мат­ри­цу на
ее транспонированную версию. Вспомните, что умножение мат­риц опреде-
ляется как точечное произведение между всеми строками левой мат­ри­цы
со всеми столбцами правой мат­ри­цы; следовательно, строки QT являются
столбцами Q.
   Матричное уравнение, выражающее два ключевых свойства ортогональ-
ной мат­ри­цы, просто изумительно:

  QTQ = I.

  Выражение QTQ = I является реально удивительным и действительно очень
важным.
  Почему оно имеет такую большую важность? Потому что QT – это мат­ри­
ца, которая умножает мат­ри­цу Q, производя единичную мат­ри­цу. Это то же
самое определение, что и для обратной мат­ри­цы. Таким образом, обратной
матрицей ортогональной мат­ри­цы является ее транспонированная версия.
Это чертовски круто, потому что получать обратную мат­ри­цу утомительно
трудно и она подвержена численным неточностям, тогда как транспониро-
ванную мат­ри­цу можно получить быстро и точно.
  На самом ли деле такие мат­ри­цы существуют в дикой природе или это
всего лишь плод воображения исследователя данных? Да, они действительно
существуют. По сути дела, единичная мат­ри­ца является примером ортого-
нальной мат­ри­цы. Вот еще два примера:




  Пожалуйста, найдите время, чтобы убедиться, что каждый столбец имеет
единичную длину и ортогонален другим столбцам. Затем можно подтвердить
на Python:
Q1 = np.array([ [1,-1], [1,1] ]) / np.sqrt(2)
Q2 = np.array([ [1,2,2], [2,1,-2], [-2,2,-1] ]) / 3

print( Q1.T @ Q1 )
print( Q2.T @ Q2 )
164  Ортогональные матрицы и QR-разложение

   Оба результата являются единичной матрицей (в пределах ошибок округ­
ления порядка 10−15). Что произойдет, если вычислить QQT? Будет ли резуль-
тат все той же единичной матрицей? Попробуйте – и узнаете1!
   Еще одним примером ортогональной мат­ри­цы являются мат­ри­цы чистого
поворота, о которых вы узнали в главе 7. Можете вернуться к исходному коду
в той главе и убедиться, что трансформационная мат­ри­ца, умноженная на ее
транспонированную версию, является единичной матрицей, независимо от
угла поворота (при условии что во всех матричных элементах использует-
ся одинаковый угол поворота). Матрицы перестановок тоже ортогональны.
Матрицы перестановок используются для обмена строками мат­ри­цы; вы
узнаете о них в следующей главе при изложении LU-разложения.
   Как создавать такие величественные чудеса математики? Ортогональ-
ная мат­ри­ца вычисляется из неортогональной мат­ри­цы посредством QR-
разложения, которое, в сущности, представляет собой изощренную версию
процедуры Грама–Шмидта. А как работает процедура Грама–Шмидта? По
сути, это ортогональное разложение векторов, о котором вы узнали в главе 2.



Процедура Грама–Шмидта
Процедура Грама–Шмидта – это способ преобразования неортогональной
мат­р и­ц ы в ортогональную. Указанная процедура имеет большое образо-
вательное значение, но, к сожалению, очень мало прикладного значения.
Причина, как вы уже несколько раз читали, – в численных нестабильностях,
возникающих в результате многочисленных делений и умножений на кро-
шечные числа. К счастью, есть более изощренные и численно стабильные
методы QR-разложения, такие как отражения Хаусхолдера. Детали этого
алгоритма выходят за рамки данной книги, но они обрабатываются вы-
зываемыми из Python низкоуровневыми библиотеками численных вы-
числений.
  Тем не менее я опишу процедуру Грама–Шмидта (иногда сокращенно GS
или G-S), поскольку она показывает применение ортогонального разложе-
ния векторов, потому что вы собираетесь запрограммировать ее алгоритм
на Python, основываясь на следующих ниже математике и описании, а так-
же потому, что GS – это правильный способ концептуализировать принцип
и причину работы QR-разложения, даже если низкоуровневая реализация
немного отличается.
  Матрица V, содержащая столбцы с v1 по vn, преобразовывается в ортого-
нальную мат­ри­цу Q со столбцами qk согласно следующему ниже алгоритму.
  Для всех векторов-столбцов в V, начиная с первого (самого левого) и си-
стематически двигаясь к последнему (крайнему правому):
  1)	ортогонализовать vk ко всем предыдущим столбцам мат­ри­цы Q, ис-
       пользуя ортогональное векторное разложение. То есть вычислить ком-


1
    Это рассматривается подробнее в упражнении 9.1.
                                                               QR-разложение  165

      поненту vk, которая перпендикулярна qk–1, qk–2 и так далее вплоть до
      q1. Ортогонализованный вектор называется vk*1;
  2)	нормировать vk*, приведя его к единичной длине. Теперь это qk, k-й стол-
      бец мат­ри­цы Q.
  Звучит просто, не правда ли? Реализация этого алгоритма в исходном коде
может быть замысловатой из-за повторяющихся ортогонализаций. Но при
некоторой настойчивости можно разобраться (упражнение 9.2).



QR-разложение
Процедура GS преобразовывает мат­ри­цу в ортогональную мат­ри­цу Q. (Как
я уже написал в предыдущем разделе, мат­ри­ца Q на самом деле получается
посредством серии отражений от векторной плоскости, именуемой преоб-
разованием Хаусхолдера, но это связано с численными проблемами; GS –
отличный способ концептуализировать принцип формирования мат­риц Q.)

      Что в звуке твоем?
      «QR» в QR-разложении произносится как «кью ар»2. На мой взгляд, это настоящая
      упущенная возможность; линейную алгебру было бы интереснее изучать, если бы мы
      произносили ее как «QweRty-разложение». Но, к лучшему или к худшему, современ-
      ные условности формируются историческим прецедентом.

  Матрица Q очевидным образом отличается от изначальной мат­ри­цы (при
условии что изначальная мат­ри­ца не была ортогональной). И значит, мы по-
теряли информацию об этой мат­ри­це. К счастью, эту «потерянную» инфор-
мацию можно легко восстановить и сохранить в другой мат­ри­це R, которая
умножает Q3. Это приводит к вопросу о том, как создавать мат­ри­цу R. На
самом деле создание мат­ри­цы R является простым и вытекает из опреде-
ления QR:

      A = QR;
    QTQ = QTQR;
    QTA = R.

  Здесь вы видите всю красоту ортогональных мат­риц: мы можем решать
матричные уравнения, не беспокоясь о вычислении обратной мат­ри­цы.
  В следующем ниже исходном коде Python показано, как вычислять QR-раз­
ложение квадратной мат­ри­цы, а на рис. 9.1 показаны три мат­ри­цы:


1
    Первый вектор-столбец не ортогонализуется, поскольку предшествующих векто-
    ров нет; поэтому вы начинаете со следующего шага нормализации.
2
    Англ. queue are. – Прим. перев.
3
    Возможность восстановления R с помощью матричного умножения существует,
    поскольку GS – это серия линейных преобразований.
166  Ортогональные матрицы и QR-разложение

A = np.random.randn(6,6)
Q,R = np.linalg.qr(A)




                   Рис. 9.1  QR-разложение мат­ри­цы случайных чисел


  Несколько важных признаков QR-разложения видны на рис. 9.1, в том чис-
ле то, что A = QR (их разностью является мат­ри­ца нулей), и то, что мат­ри­ца
Q, умноженная на ее транспонированую версию, дает единичную мат­ри­цу.
  Посмотрите на мат­ри­цу R: это верхнетреугольная мат­ри­ца (все элементы
ниже диагонали равны нулю). Вряд ли это произошло случайно, учитывая,
что мы начали со случайной мат­ри­цы. На самом деле мат­ри­ца R всегда бу-
дет верхнетреугольной. Для того чтобы понять причину, нужно подумать об
алгоритме GS и организации точечных произведений в умножении мат­риц.
В следующем далее разделе я объясню причину, по которой мат­ри­ца R яв-
ляется верхнетреугольной; но до этого хотел бы, чтобы вы придумали ответ.


Размеры матриц Q и R
Размеры мат­риц Q и R зависят от размера подлежащей разложению мат­ри­
цы A и от того, является ли QR-разложение «экономным» (также именуемым
«сокращенным», или «компактным») либо «полным». На рис. 9.2 представлен
обзор всех возможных размеров.
   Экономное QR-разложение по сравнению с полным применимо только
к высоким мат­ри­цам. Вопрос в следующем: для высокой мат­ри­цы (M > N) мы
создаем мат­ри­цу Q с N столбцами или же с M столбцами? Первый вариант
                                                                   QR-разложение  167

называется экономным, или сокращенным1, и дает высокую мат­ри­цу Q, а по-
следний вариант называется полным2 и дает квадратную мат­ри­цу Q.




             Квадратная
          полноранговая


             Квадратная
            сингулярная


               Высокая
               «полная


               Высокая
           «экономная»



               Широкая


          Рис. 9.2  Размеры мат­риц Q и R в зависимости от размера мат­ри­цы A.
    Символ «?» указывает на то, что матричные элементы зависят от значений в мат­ри­це A,
                                т. е. это не единичная мат­ри­ца

  Возможно, покажется удивительным, что мат­ри­ца Q может быть квадрат-
ной, когда мат­ри­ца A – высокая (другими словами, Q может иметь больше
столбцов, чем A): откуда берутся дополнительные столбцы? На самом деле
ортогональные векторы можно создавать «из воздуха». Рассмотрим следую-
щий ниже пример на Python:
A = np.array([ [1,-1] ]).T
Q,R = np.linalg.qr(A,'complete')
Q*np.sqrt(2) # шкалируется с помощью sqrt(2),
             # чтобы получить целые числа

>> array([[-1., 1.],
          [ 1., 1.]])

  Обратите внимание на опциональный второй аргумент 'complete', ко-
торый производит полное QR-разложение. Задание этого аргумента рав-
ным 'reduced', который используется по умолчанию, дает экономное QR-
разложение, в котором мат­ри­ца Q имеет тот же размер, что и мат­ри­ца A.
  Поскольку из мат­ри­цы с N столбцами можно создать более M > N ортого-
нальных векторов, ранг мат­ри­цы Q всегда является максимально возможным


1
    Англ. economy, reduced. – Прим. перев.
2
    Англ. full, complete. – Прим. перев.
168  Ортогональные матрицы и QR-разложение

рангом, который равен M для всех квадратных мат­риц Q и N для экономной
мат­ри­цы Q. Ранг мат­ри­цы R совпадает с рангом мат­ри­цы A.
   Разница в ранге между мат­ри­ца­ми Q и A в результате ортогонализации
означает, что Q охватывает все ℝM, даже если столбцовое пространство мат­
ри­цы A является лишь низкоразмерным подпространством ℝM. Этот факт
является ключевым в том, почему сингулярное разложение выступает та-
ким удобным для выявления свойств мат­ри­цы, включая ее ранг и нуль-
пространство. Еще одна причина с нетерпением ждать информации об SVD
в главе 14!
   Примечание об уникальности: QR-разложение не является уникальным
для всех размеров и рангов мат­риц. Это означает, что можно получить A =
Q1R1 и A = Q2R2, гдеQ1 ¹ Q2. Однако все результаты QR-разложения облада-
ют одинаковыми свойствами, описанными в этом разделе. QR-разложение
можно сделать уникальным при наличии дополнительных ограничений
(например, положительных значений на диагоналях мат­р и­ц ы R), хотя
в большинстве случаев это необязательно и не реализовано ни в Python, ни
в MATLAB. Вы увидите эту неуникальность в упражнении 9.2 при сравнении
GS с QR.

Почему матрица R является верхнетреугольной
Надеюсь, вы серьезно задумались над данным вопросом. В QR-разложении
это сложный момент, поэтому если вы не смогли разобраться в нем са-
мостоятельно, то, пожалуйста, прочитайте следующие несколько абзацев,
а затем оторвите взгляд от книги/экрана и заново выведите аргумент ма-
тематически.
   Начну с того, что напомню три факта:
     мат­ри­ца R получается из формулы QTA = R;
     нижний треугольник мат­ри­цы произведения содержит точечные про-
       изведения между последующими строками левой мат­ри­цы и предшест­
       вующими столбцами правой мат­ри­цы;
     строки мат­ри­цы QT являются столбцами мат­ри­цы Q.
   Соединяя все вместе: поскольку ортогонализация проходит по столбцам
слева направо, последующие столбцы в мат­ри­це Q ортогонализируются
к предшествующим столбцам мат­ри­цы A. Следовательно, нижний треуголь-
ник мат­ри­цы R получается из пар ортогонализированных векторов. Напро-
тив, предшествующие столбцы в Q не ортогонализируются к последующим
столбцам мат­ри­цы A, поэтому их точечные произведения ожидаемо не будут
равны нулю.
   Заключительный комментарий: если бы столбцы i и j мат­ри­цы A уже были
ортогональны, то соответствующий (i, j)-й элемент в R был бы равен нулю.
На самом деле если вычислить QR-разложение ортогональной мат­ри­цы, то R
будет диагональной матрицей, в которой диагональные элементы являются
нормами каждого столбца в мат­ри­це A. Это означает, что если A = Q, то R = I,
что очевидно из уравнения, вычисленного для R. Вы обследуете этот вопрос
в упражнении 9.3.
                                                            Резюме  169


QR и обратные матрицы
QR-разложение обеспечивает численно более стабильный способ вычисле-
ния обратной мат­ри­цы.
  Давайте начнем с написания формулы QR-разложения и инвертирования
обеих частей уравнения (обратите внимание на применение правила LIVE
EVIL):

      A = QR;
      –1
  A        = (QR)–1;
  A–1 = R–1Q–1;
  A–1 = R–1QT.

  Таким образом, обратную мат­ри­цу мат­ри­цы A можно получить как ин-
версию мат­ри­цы R, умноженную на транспонированную версию мат­ри­цы Q.
Мат­рица Q численно устойчива из-за алгоритма отражения Хаусхолдера,
а мат­ри­ца R численно устойчива, потому что она является просто результа-
том матричного умножения.
  Теперь нам все еще нужно инвертировать R в явной форме, но инверти-
рование треугольных мат­риц численно является очень стабильным, если
выполняется с помощью процедуры, именуемой обратной подстановкой. Вы
узнаете о ней подробнее в следующей главе, но ключевой момент заключа-
ется в следующем: важным применением QR-разложения является предо-
ставление численно более стабильного способа инвертирования мат­риц по
сравнению с алгоритмом, который был представлен в предыдущей главе.
  С другой стороны, имейте в виду, что мат­ри­цы, которые теоретически
являются обратимыми, но близкими к сингулярным, по-прежнему очень
трудно инвертировать; QR-разложение может быть численно более стабиль-
ным, чем представленный в предыдущей главе традиционный алгоритм, но
это не гарантирует высококачественную обратную мат­ри­цу. Если окунуть
гнилое яблоко в мед, то оно по-прежнему останется гнилым.



Резюме
QR-разложение – великолепная вещь. Оно определенно входит в мой список
пяти лучших матричных разложений в линейной алгебре. Вот ключевые вы-
воды, которые следует вынести из этой главы.
    Ортогональная мат­ри­ца имеет столбцы, которые попарно ортогональ-
      ны и имеют норму, равную 1. Ортогональные мат­ри­цы являются клю-
      чом к нескольким матричным разложениям, включая QR-разложение,
      собственное разложение и сингулярное разложение. Ортогональные
      мат­ри­цы также важны в геометрии и компьютерной графике (напри-
      мер, мат­ри­цы чистого поворота).
170  Ортогональные матрицы и QR-разложение

      Неортогональная мат­ри­ца преобразовывается в ортогональную мат­ри­
        цу посредством процедуры Грама–Шмидта, которая предусматривает
        применение ортогонального разложения векторов, чтобы изолировать
        компоненту каждого столбца, который ортогонален всем предыдущим
        столбцам («предыдущий» означает слева направо).
      QR-разложение является результатом процедуры Грама–Шмидта (тех-
        нически оно реализовано более стабильным алгоритмом, но GS по-
        прежнему является правильным ключом к его пониманию).



Упражнения по программированию
    Упражнение 9.1
    Квадратная мат­ри­ца Q имеет следующие ниже тождества:

    QTQ = QQT = Q–1Q = QQ–1 = I.

  Продемонстрируйте это в исходном коде, вычислив Q из мат­ри­цы слу-
чайных чисел, потом вычислив QT и Q–1. Затем покажите, что все четыре
выражения дают единичную мат­ри­цу.

  Упражнение 9.2
  Реализуйте процедуру Грама–Шмидта, как описано ранее1. Используйте
мат­ри­цу случайных чисел 4×4. Сравните свой ответ с матрицей Q из функции
np.linalg.qr.
  Важно: в преобразованиях типа отражения Хаусхолдера присутствует
фундаментальная неопределенность знака. Это означает, что векторы могут
«переворачиваться» (умножаться на −1) в зависимости от незначительных
различий в алгоритме и реализации. Эта особенность существует во многих
матричных разложениях, включая собственные векторы. В главе 13 имеется
более глубокое и подробное изложение причины, по которой это происходит,
и что это означает. А пока результат таков: надо вычесть свою мат­ри­цу Q из
Python’овской мат­ри­цы Q и сложить свою мат­ри­цу Q и Python’овскую мат­
ри­цу Q. Ненулевые столбцы в одной будут нулями в другой.

  Упражнение 9.3
  В этом упражнении вы узнаете, что происходит при применении QR-
разложения к мат­ри­це, которая почти, но не совсем ортогональна. Во-первых,
создайте ортогональную мат­ри­цу под названием U из QR-разложения мат­
ри­цы случайных чисел 6×6. Вычислите QR-разложение мат­ри­цы U и под-
твердите, что R = I (и убедитесь, что вы понимаете причину!).
  Во-вторых, видоизмените нормы каждого столбца мат­ри­цы U. Установи-
те нормы столбцов 1–6 равными 10–15 (то есть первый столбец мат­ри­цы U
должен иметь норму 10, второй столбец должен иметь норму 11 и т. д.). Про-


1
    Не торопитесь с этим упражнением; оно довольно сложное.
                                                                                  Упражнения по программированию  171

гоните эту модулированную мат­ри­цу U через QR- разложение и подтвердите,
что ее R является диагональной матрицей с диагональными элементами,
равными 10–15. Какой будет QTQ для этой мат­ри­цы?
   В-третьих, нарушьте ортогональность мат­ри­цы U, установив элемент
u1.4 = 0. Что произойдет с R и почему?

  Упражнение 9.4
  Цель этого упражнения – сравнить численные ошибки, полученные при
использовании «традиционного» инверсного метода, с которым вы познако-
мились в предыдущей главе, с инверсным методом на основе QR. Мы будем
использовать мат­ри­цы случайных чисел, помня о том, что они, как правило,
численно стабильны и, следовательно, имеют точные обратные мат­ри­цы.
  Вот что нужно сделать: скопируйте исходный код из упражнения 8.2
в функцию Python, которая на входе принимает мат­ри­цу и на выходе пре-
доставляет обратную ей мат­р и­ц у. (Вы также можете включить проверку
того, что входная мат­ри­ца является квадратной и является полноранговой.)
Я назвал эту функцию oldSchoolInv. Затем создайте мат­ри­цу случайных чи-
сел 5×5. Вычислите обратную ей мат­ри­цу, используя традиционный метод
и представленный в этой главе метод на основе QR-разложения (для вычис-
ления R–1 можете использовать свой «традиционный метод»). Рассчитайте
ошибку вычисления обратной мат­ри­цы как евклидово расстояние от мат­ри­
цы, умноженной на обратную ей мат­ри­цу, до истинной единичной мат­ри­цы
из функции np.eye. Постройте гистограмму результатов, показав два метода
на оси x и ошибку (евклидово расстояние до I) на оси y, как представлено
на рис. 9.3.

                                                                Ошибка обратной мат­ри­цы (мат­ри­ца 5×5)
            Евклидово расстояние до единичной мат­ри­цы




                                                          Рис. 9.3  Результаты упражнения 9.4
       172  Ортогональные матрицы и QR-разложение

         Выполняйте исходный код несколько раз и инспектируйте гистограмму.
       Вы обнаружите, что иногда традиционный метод работает лучше, а иногда
       лучше работает метод на основе QR (чем меньше числа, тем лучше; теоре-
       тически столбцы должны иметь нулевую высоту). Попробуйте еще раз, ис-
       пользуя мат­ри­цу 30×30. Будут ли результаты более последовательными? На
       самом деле от прогона к прогону будет большой разброс. Это означает, что
       необходимо провести эксперимент, в котором сравнение повторяется много
       раз. Это следующее упражнение.

          Упражнение 9.5
          Поместите исходный код из предыдущего упражнения в цикл for с сотней
       итераций, в которых вы повторяете эксперимент, всякий раз используя дру-
       гую мат­ри­цу случайных чисел. Сохраняйте ошибку (евклидово расстояние)
       по каждой итерации и постройте график, подобный рис. 9.4, который пока-
       зывает среднее значение по всем прогонам эксперимента (серый столбик)
       и все отдельные ошибки (черные точки). Проведите эксперимент для мат­риц
       5×5 и 30×30.
          Вы также можете попробовать использовать функцию np.linalg.inv, что-
       бы вместо традиционного метода инвертировать R и увидеть, будет ли это
       иметь эффект.

                                              Ошибка обратной мат­ри­цы (мат­ри­ца 5×5)                                                 Ошибка обратной мат­ри­цы (мат­ри­ца 5×5)
Евклидово расстояние до единичной мат­ри­цы




                                                                                          Евклидово расстояние до единичной мат­ри­цы




                                                             Рис. 9.4  Результаты упражнения 9.5.
                                                       Обратите внимание на разницу в шкалировании оси y
                                                                 между левой и правой панелями

         Упражнение 9.6
         Интересное свойство квадратных ортогональных мат­риц состоит в том,
       что все их сингулярные числа (и их собственные числа) равны 1. Это означает,
       что они имеют индуцированную 2-норму, равную 1 (индуцированная норма
       является наибольшим сингулярным числом), и они имеют фробениусову
       норму M. Последний результат объясняется тем, что фробениусова норма
                                         Упражнения по программированию  173

равна квадратному корню из суммы квадратов сингулярных чисел. В этом
упражнении вы подтвердите эти свойства.
   Создайте ортогональную мат­ри­цу M×M как QR-разложение случайной
мат­р и­ц ы. Вычислите ее индуцированную 2-норму, используя функцию
np.linalg.norm, и вычислите ее фробениусову норму, используя уравнение,
которое вы узнали в главе 6, деленное на квадратный корень из M. Под-
твердите, что обе величины равны 1 (в разумных пределах допуска ошибки
округления). Проверьте, используя несколько разных значений M.
   Далее обследуйте значение индуцированной нормы, используя умноже-
ние мат­ри­цы на вектор. Создайте случайный -элементный вектор-столбец
v. Затем вычислите нормы векторов v и Qv. Эти нормы должны быть равны
друг другу (хотя не следует ожидать, что они будут равны 1).
   Наконец, возьмите лист бумаги и разработайте доказательство этой эмпи-
рической демонстрации. Указанное доказательство напечатано в следующем
абзаце, так что не смотрите вниз! Но вы можете обратиться к сноске, если
вам нужна подсказка1.
   Искренне надеюсь, что вы это читаете, чтобы проверить свои рассужде-
ния, а не потому, что жульничаете! Так или иначе, доказательство состоит
в том, что векторную норму ||v|| можно вычислить как vTv; поэтому векторная
норма ||Qv|| вычисляется как (Qv)TQv = vTQTQv. QTQ отменяется, давая еди-
ничную мат­ри­цу, при этом оставляя точечное произведение вектора с самим
собой. Вывод состоит в том, что ортогональные мат­ри­цы могут поворачивать
вектор, но не шкалировать его.

  Упражнение 9.7
  В этом упражнении будет освещена одна особенность мат­ри­цы R, которая
имеет отношение к пониманию того, как использовать QR для реализации
метода наименьших квадратов (глава 12): когда мат­ри­ца A является высо-
кой и имеет полный столбцовый ранг, первые N строк мат­ри­цы R являются
верхнетреугольными, тогда как строки N + 1 вплоть до M – нулевыми. Под-
твердите это на Python, используя случайную мат­ри­цу 10×4. Проследите,
чтобы использовалось полное QR-разложение, а не экономное (компактное)
разложение.
  Конечно же, мат­ри­ца R необратима, потому что она не квадратная. Но
  1)	подматрица, состоящая из первых N строк, является квадратной и пол-
      норанговой (когда мат­ри­ца A имеет полный столбцовый ранг), следо-
      вательно, имеет полную обратную мат­ри­цу, и
  2) высокая мат­ри­ца R имеет псевдообратную мат­ри­цу.
  Вычислите обе обратные мат­ри­цы и подтвердите, что полная обратная
мат­ри­ца первых N строк мат­ри­цы R равна первым N столбцам псевдообрат-
ной мат­ри­цы высокой мат­ри­цы R.




1
    Подсказка: распишите формулу точечного произведения для векторной нормы.
Глава         10
                          Приведение строк
                           и LU-разложение

Теперь переходим к LU-разложению. LU, как и QR, является одним из вы-
числительных стержней, лежащих в основе алгоритмов обработки данных,
включая подгонку модели методом наименьших квадратов и обратную мат­
ри­цу. Таким образом, эта глава имеет ключевое значение для вашего обра-
зования в области линейной алгебры.
  Особенность LU-разложения состоит в том, что его невозможно просто
выучить за один присест. Вместо этого сначала придется познакомиться с си-
стемами уравнений, приведением строк и гауссовым устранением. И в ходе
изучения этих тем вы также узнаете о ступенчатых матрицах и матрицах
перестановок. О да, дорогой читатель, это будет захватывающая и насыщен-
ная событиями глава.



Системы уравнений
В целях понимания LU-разложения и его применений нужно понять при-
ведение строк и устранение по Гауссу. А в целях понимания этих тем нужно
понять, как манипулировать уравнениями, преобразовывать их в матричное
уравнение и решать это матричное уравнение, используя приведение строк.
  Начнем с «системы» из одного уравнения:

  2x = 8.

   Уверен, что, научившись в школе, вы способны выполнять различные ма-
тематические манипуляции с уравнением – при условии что вы делаете одно
и то же с обеими частями уравнения. Это означает, что следующее уравнение
не совпадает с предыдущим, но они связаны между собой простыми ма-
нипуляциями. Что еще важнее, любое решение одного уравнения является
решением другого:

  5(2x – 3) = 5(8 – 3).
                                                 Системы уравнений  175

  Теперь перейдем к системе из двух уравнений:

  x = 4 – y;
  y = x/2 + 2.

  В этой системе уравнений найти уникальные значения x и y только из од-
ного из этих уравнений невозможно. Вместо этого, чтобы получить решение,
нужно рассматривать оба уравнения одновременно. Если вы попытаетесь
решить эту систему сейчас, то, вероятно, воспользуетесь стратегией под-
становки y в первом уравнении правой частью второго уравнения. После
отыскания решения для x в первом уравнении вы подставите это значение
во второе уравнение, чтобы найти y. Эта стратегия похожа (хотя и не так
эффективна) на обратную подстановку, определение которой я дам позже.
  Важной особенностью системы уравнений является то, что отдельные
уравнения можно складывать друг с другом либо вычитать. В следующих
ниже уравнениях я два раза добавил второе уравнение к первому и вычел
первое изначальное уравнение из второго (скобки добавлены для ясности):

  x + (2y) = 4 – y + (x + 4);
   y – (x) = x/2 + 2 – (4 – y).

  Даю вам возможность поработать над арифметикой, но итогом является
то, что x выпадает из первого уравнения, а y выпадает из второго урав-
нения. Это значительно упрощает вычисление решения (x = 4/3, y = 8/3).
И вот важный момент: умножение уравнения на скаляр и добавление их
к другим уравнениям облегчили отыскание решения системы. Опять же,
модулированная и изначальная системы – это не одни и те же уравнения,
но их решения одинаковы, поскольку две системы связаны серией линей-
ных операций.
  Это базовые знания, необходимые для того, чтобы научиться решать си-
стемы уравнений с помощью линейной алгебры. Но прежде чем научиться
этому подходу, вам нужно научиться представлять систему уравнений мат­
ри­ца­ми и векторами.


Конвертирование уравнений в матрицы
Конвертирование систем уравнений в матрично-векторное уравнение при-
меняется для решения систем уравнений и используется для настройки
формулы общей линейной модели в статистике. К счастью, переложение
уравнений в мат­ри­цы осуществляется концептуально просто и выполняется
в два шага.
   Во-первых, надо организовать уравнения так, чтобы константы находи-
лись в правой части уравнений. Константы – это числа, которые не при-
вязаны к переменным (иногда именуемые пересечениями, или сдвигами).
Переменные и умножающие их коэффициенты находятся в левой части урав-
нения в том же порядке (например, все уравнения должны иметь сначала
176  Приведение строк и LU-разложение

член x, затем член y и т. д.). Следующие ниже уравнения образуют систему
уравнений, с которой мы работали ранее, но в правильной организации:

  x + y = 4;
  –x/2 + y = 2.

  Во-вторых, надо выделить коэффициенты (числа, на которые умножаются
переменные; отсутствующие в уравнении переменные имеют коэффициент,
равный нулю) в мат­ри­цу с одной строкой в расчете на одно уравнение. Пере-
менные помещаются в вектор-столбец, который умножает мат­ри­цу коэффи-
циентов справа. А константы помещаются в вектор-столбец в правой части
уравнения. Наша примерная система имеет матричное уравнение, которое
выглядит следующим образом:




  И вуаля! Вы конвертировали систему уравнений в одно матричное уравне-
ние. К этому уравнению можно обращаться как к Ax = b, где A – это мат­ри­ца
коэффициентов, x – вектор неизвестных переменных, для которых нужно
найти решение (в данном случае x – это вектор, содержащий [x y]), а b – век-
тор констант.
  Пожалуйста, найдите минутку, чтобы убедиться, что вы понимаете, как
матричное уравнение отображается в систему уравнений. В частности, про-
работайте умножение мат­ри­цы на вектор, чтобы продемонстрировать, что
оно эквивалентно изначальной системе уравнений.


Работа с матричными уравнениями
Матричными уравнениями можно манипулировать так же, как и обычными
уравнениями, включая сложение, умножение, транспонирование и т. д., при
условии что манипуляции допустимы (например, в случае сложения размеры
мат­риц совпадают) и все манипуляции влияют на обе части уравнения. На-
пример, допустима следующая ниже прогрессия уравнений:

         Ax = b;
    v + Ax = v + b;
  (v + Ax)T = (v + b)T.

  Главное различие между работой с матричными уравнениями и скаляр-
ными уравнениями заключается в том, что, поскольку умножение мат­риц
зависит от стороны, мат­ри­цы необходимо умножать одинаково в обеих час­
тях уравнения.
  Например, допустима следующая ниже прогрессия уравнений:

   AX = B;
  CAX = CB.
                                                               Системы уравнений  177

  Обратите внимание, что C предпозиционно умножает обе части уравне-
ния. Напротив, следующая ниже прогрессия недопустима:

     AX = B;
    AXC = CB.

  Проблема здесь в том, что C постпозиционно умножает в левой части, но
предпозиционно умножает в правой части. Несомненно, есть несколько ис-
ключительных случаев, когда это уравнение будет допустимым (например,
если C является единичной матрицей или матрицей нулей), но в общем слу-
чае эта прогрессия недопустима.
  Давайте обратимся к примеру на Python. Мы найдем неизвестную мат­ри­цу
X в уравнении AX = B. Следующий ниже исходный код генерирует мат­ри­цы A
и B из случайных чисел. Вы уже знаете, что X можно найти, используя A–1.
Вопрос в том, имеет ли значение порядок умножения1.
A = np.random.randn(4,4)
B = np.random.randn(4,4)
# найти X
X1 = np.linalg.inv(A) @ B
X2 = B @ np.linalg.inv(A)
# остаток (должен быть матрицей нулей)
res1 = A@X1 - B
res2 = A@X2 - B

  Если бы умножение мат­риц было коммутативным (означая, что порядок
не имеет значения), то res1 и res2 должны были быть равны мат­ри­це нулей.
Давайте посмотрим:
res1:

[[-0.    0.    0.    0.]
 [-0.   -0.    0.    0.]
 [ 0.    0.    0.    0.]
 [ 0.    0.   -0.   -0.]]

res2:

[[-0.47851507        6.24882633    4.39977191    1.80312482]
 [ 2.47389146        2.56857366    1.58116135   -0.52646367]
 [-2.12244448       -0.20807188    0.2824044    -0.91822892]
 [-3.61085707       -3.80132548   -3.47900644   -2.372463 ]]

  Теперь вы знаете, как выражать систему уравнений одним матричным
уравнением. Я собираюсь вернуться к этому вопросу через несколько разде-
лов; сперва мне нужно научить вас приведению строк и ступенчатой форме
мат­ри­цы.

1
    Конечно же, вы знаете, что порядок имеет значение, но эмпирические демонстра-
    ции помогают развивать интуицию. И я хочу, чтобы вы привыкли использовать
    Python как инструмент для эмпирического подтверждения математических прин-
    ципов.
178  Приведение строк и LU-разложение


Приведение строк
В традиционной линейной алгебре теме приведения строк уделяется много
внимания, потому что это проверенный временем способ ручного решения
систем уравнений. Я серьезно сомневаюсь, что в своей карьере исследова-
теля данных вы будете решать какие-либо системы уравнений вручную. Но
о приведении строк полезно знать, и оно ведет напрямую к LU-разложению,
которое применяется на практике в прикладной линейной алгебре. Итак,
начнем.
   Приведение строк1 означает итеративное применение двух операций –
скалярного умножения и сложения – к строкам мат­ри­цы. Приведение строк
основано на том же принципе, что и добавление уравнений к другим урав-
нениям в системе.
   Запомните вот это утверждение: цель приведения строк – преобразовать
плотную мат­ри­цу в верхнетреугольную мат­ри­цу.
   Начнем с простого примера. В следующей ниже плотной мат­ри­це мы до-
бавляем первую строку ко второй строке, тем самым выбивая −2. И таким
образом мы преобразовали плотную мат­ри­цу в верхнетреугольную мат­ри­цу:




  Полученная в результате приведения строк верхнетреугольная мат­ри­ца
называется ступенчатой формой мат­ри­цы.
  Формально мат­ри­ца имеет ступенчатую форму, если
  1)	самое левое ненулевое число в строке (именуемое опорным элемен-
      том2) находится справа от опорного элемента вышележащих строк и
  2)	любые строки из одних нулей находятся ниже строк, содержащих не-
      нулевые числа.
  Подобно манипулированию уравнениями в системе, мат­ри­ца после при-
ведения строк отличается от мат­ри­цы до приведения строк. Но две мат­ри­цы
связаны линейным преобразованием. А поскольку линейные преобразова-
ния можно представлять мат­ри­ца­ми, то для выражения приведения строк
можно использовать матричное умножение:




  Я буду называть эту мат­ри­цу L–1 по причинам, которые станут ясны, когда
я введу понятие LU-разложения3. Таким образом, в выражении (L–1A = U)
мат­ри­ца L–1 – это линейное преобразование, отслеживающее манипуляции,

1
    Англ. Row Reduction; син. сокращение строк. – Прим. перев.
2
    Англ. Pivot. – Прим. перев.
3
    Внимание, спойлер: LU-разложение предусматривает представление мат­ри­цы как
    произведения нижнетреугольной и верхнетреугольной мат­риц.
                                                                 Приведение строк  179

которые мы реализовали посредством приведения строк. Пока что не нужно
сосредоточиваться на мат­ри­це L–1 – на самом деле при гауссовом отсеивании
ее часто игнорируют. Но ключевой момент (слегка расширенный по сравне-
нию с более ранним утверждением) заключается в следующем: приведение
строк предусматривает преобразовывание мат­ри­цы в верхнетреугольную
мат­ри­цу посредством манипуляций со строками, реализуемых как препозици-
онное умножение на трансформационную мат­ри­цу.
   Вот еще один пример мат­ри­цы 3×3. Для преобразования этой мат­ри­цы
в ступенчатую форму требуется два шага:




   Процедура приведения строк утомительна (см. врезку «Всегда ли процеду-
ра приведения строк так проста?»). Наверняка должна существовать функция
Python, которая делает это за нас! Все дело в том, что она есть и ее нет. Нет
функции Python, которая возвращает ступенчатую форму, подобную той, что
я создал в двух предыдущих примерах. Причина в том, что ступенчатая фор-
ма мат­ри­цы не уникальна. Например, в приведенной выше мат­ри­це 3×3 вы
могли умножить вторую строку на 2, чтобы получить вектор-строку [0 10 4].
Это создает совершенно допустимую, но другую ступенчатую форму той же
изначальной мат­ри­цы. И действительно, с этой матрицей связано бесконеч-
ное число ступенчатых мат­риц.
   С учетом сказанного, двухступенчатые формы мат­ри­цы предпочтитель-
нее бесконечно возможных ступенчатых форм. Эти две формы уникальны
с учетом некоторых ограничений и называются строчно приведенной сту-
пенчатой формой и матрицей U из LU-разложения. Я представлю обе позже;
сначала пора научиться использовать приведение строк для решения систем
уравнений.


                  Всегда ли процедура приведения строк так проста?
    Овладение навыком приведения строк мат­ри­цы к ее ступенчатой форме требует усерд-
    ной практики и усвоения нескольких приемов с крутыми названиями, такими как взаимо-
    обмен строк и взаимообмен строк и столбцов1. Приведение строк раскрывает несколько
    интересных свойств строчного и столбцового пространств мат­ри­цы. И хотя успешное при-
    ведение строк мат­ри­цы 5×6 с помощью ручного взаимообмена строк приносит чувство
    выполненного долга и удовлетворения, я полагаю, что ваше время лучше потратить на
    линейно-алгебраические концепции, которые имеют более прямое применение в науке
    о данных. Мы движемся к пониманию LU-разложения, и этого краткого введения в при-
    ведение строк будет для указанной цели вполне достаточно.




1
    Англ. partial pivoting; данный прием используется во избежание ошибок округле-
    ния, которые могут возникать при делении каждого значения строки на опорный
    элемент. – Прим. перев.
180  Приведение строк и LU-разложение


Метод устранения по Гауссу
К этому моменту книги вы знаете, как решать матричные уравнения, ис-
пользуя обратную мат­ри­цу. Что, если бы я вам сказал, что вы можете решить
матричное уравнение, не инвертируя никаких мат­риц1?
   Этот метод называется устранением по Гауссу, или гауссовым устранени-
ем2. Несмотря на свое название, этот алгоритм на самом деле был разработан
китайскими математиками почти за две тысячи лет до Гаусса, а затем открыт
заново Ньютоном за сотни лет до Гаусса. Но Гаусс внес в этот метод важный
вклад, в том числе технические приемы, реализованные в современных ком-
пьютерах.
   Метод гауссова устранения работает просто: надо расширить мат­ри­цу ко-
эффициентов вектором констант, привести строки к ступенчатой форме,
а затем применить обратную подстановку, чтобы найти решение для каждой
переменной по очереди.
   Начнем с системы из двух уравнений, которую мы решили ранее:

    x = 4 – y;
    y = x/2 + 2.

  Первым шагом является конвертирование этой системы уравнений в мат­
ричное уравнение. Данный шаг уже нами пройден; это уравнение напечатано
здесь как напоминание:




    Далее расширим мат­ри­цу коэффициентов вектором констант:




   Затем выполним приведение строк в этой расширенной мат­ри­це. Обрати-
те внимание, что при приведении строк вектор-столбец констант изменится:




  Получив мат­ри­цу в ступенчатой форме, переводим расширенную мат­ри­цу
обратно в систему уравнений. Она выглядит так:

    x + y = 4;
    3/2y = 4.

1
    Пожалуйста, вообразите в уме мем Матрицы, в которой Морфеус предлагает крас-
    ную и синюю таблетки, соответствующие новым знаниям в противоположность
    приверженности тому, что вы уже знаете.
2
    Англ. Gaussian Elimination; син. исключение по Гауссу. – Прим. перев.
                                                        Приведение строк  181

   Метод гауссова устранения посредством приведения строк удалил член
x во втором уравнении, а это означает, что отыскание y предусматривает
всего лишь несколько арифметических действий. Решив y = 8/3, подставьте
это значение в y в первом уравнении и найдите x. Эта процедура называется
обратной подстановкой.
   В предыдущем разделе я писал, что в Python нет функции для вычисления
ступенчатой формы мат­ри­цы, поскольку она не уникальна. А затем написал,
что существует одна уникальная ступенчатая мат­ри­ца, именуемая строч-
но приведенной ступенчатой формой (нередко сокращенно RREF1), которую
Python вычислит. Продолжайте читать, чтобы узнать подробности…


Метод устранения по Гауссу–Жордану
Давайте продолжим приведение строк нашей образцовой мат­р и­ц ы с це-
лью превратить все опорные элементы – крайние левые ненулевые числа
в каждой строке – в единицы. При наличии ступенчатой мат­р и­ц ы каждая
строка просто делится на ее опорный элемент. В этом примере первая
строка уже имеет 1 в крайнем левом положении, поэтому нам просто нужно
скорректировать вторую строку. В результате получаем следующую ниже
мат­р и­ц у:




  А теперь подходим к хитрости: мы продолжаем приведение строк вверх,
чтобы отсеить все элементы над каждым опорным элементом. Другими сло-
вами, нам нужна ступенчатая мат­ри­ца, в которой каждый опорный элемент
равен 1, и это единственное ненулевое число в ее столбце.




  Это и есть строчно приведенная ступенчатая форма (RREF) нашей изна-
чальной мат­ри­цы. Вы увидите единичную мат­ри­цу слева – RREF-форма всег-
да будет создавать единичную мат­ри­цу как подматрицу в левом верхнем углу
изначальной мат­ри­цы. Это результат установки всех опорных элементов
равными 1 и использования восходящего сведения строк, чтобы устранять
все элементы над каждым опорным элементом.
  Теперь мы продолжим гауссово устранение путем переложения мат­ри­цы
обратно в систему уравнений:

    x = 4/3;
    y = 8/3.


1
    От англ. Reduced Row Echelon Form. – Прим. перев.
182  Приведение строк и LU-разложение

  Нам больше не нужна ни обратная подстановка, ни даже базовая арифме-
тика: модифицированный метод гауссова устранения, который называется
методом устранения по Гауссу–Жордану, разъединил переплетенные пере-
менные в системе уравнений и обнажил решения по каждой переменной.
  Метод устранения по Гауссу–Жордану применялся людьми для решения
систем уравнений вручную более чем за столетие до того, как появились
компью­теры, чтобы помогать нам в числодробительной работе. На самом
деле в компьютерах реализован все тот же метод, лишь с малыми модифи-
кациями в целях обеспечения численной стабильности.
  RREF-форма уникальна, и, стало быть, мат­ри­ца имеет только одну ассоци-
ированную RREF-форму. В NumPy нет функции для вычисления RREF-формы
мат­ри­цы, но в библиотеке sympy она есть (sympy – это библиотека для сим-
вольных математических вычислений на Python и мощный двигатель для
математики «классной доски»):
import sympy as sym

# мат­ри­ца, конвертированная в sympy
M = np.array([ [1,1,4],[-1/2,1,2] ])
symMat = sym.Matrix(M)

# RREF-форма
symMat.rref()[0]

>>
[[1, 0, 1.33333333333333],
 [0, 1, 2.66666666666667]]



Обратная матрица посредством метода
устранения по Гауссу–Жордану
Ключевой момент метода устранения по Гауссу–Жордану заключается в том,
что приведение строк производит последовательность манипуляций со стро-
ками, которая решает систему уравнений. Эти манипуляции со строками
являются линейными преобразованиями.
  Любопытно, что описание метода устранения по Гауссу–Жордану согла-
суется с описанием обратной мат­ри­цы, то есть линейного преобразования,
которое решает систему уравнений. Но подождите, какую «систему уравне-
ний» решает обратная мат­ри­ца? Свежий взгляд на обратную мат­ри­цу даст
несколько новых идей. Рассмотрим вот эту систему уравнений:

  ax1 + by1 = 1;
  cx1 + dy1 = 0.

  Переведя в матричное уравнение, получим:
                                                         LU-разложение  183

  Посмотрите на вектор констант – это первый столбец единичной мат­ри­цы
2×2! Это означает, что применение RREF-формы к полноранговой квадрат-
ной мат­ри­це, расширенной первым столбцом единичной мат­ри­цы, покажет
линейное преобразование, которое переводит мат­ри­цу в первый столбец
единичной мат­ри­цы. А это, в свою очередь, означает, что вектор [x1 y1]T яв-
ляется первым столбцом обратной мат­ри­цы.
  Затем мы повторяем процедуру, но находя решение для второго столбца
обратной мат­ри­цы:

    ax2 + by2 = 0;
    cx2 + dy2 = 1.

  RREF-форма в этой системе дает вектор [x2 y2]T, который является вторым
столбцом обратной мат­ри­цы.
  Я отделил столбцы единичной мат­ри­цы, чтобы связать их с перспективой
решения систем уравнений. Но мы можем расширить всю единичную мат­
ри­цу целиком и найти обратную мат­ри­цу с помощью одной RREF-формы.
  Вот вид с высоты птичьего полета на получение обратной мат­ри­цы по-
средством метода устранения по Гауссу–Жордану (квадратные скобки обо-
значают расширенные мат­ри­цы, при этом вертикальная линия отделяет две
составляющие мат­ри­цы):

    rref([A | I]) ⇒ [I | A–1].

  Это интересно, поскольку обеспечивает механизм вычисления обратной
мат­ри­цы без вычисления определителей. С другой стороны, приведение
строк предусматривает большое число делений, увеличивая риск ошибок
численной прецизионности. Например, представьте, что даны два числа, ко-
торые по существу равны нулю плюс ошибка округления. Если мы в конечном
итоге придем к тому, что поделим эти числа во время получения RREF, то мы
получим дробь 10−15/10−16, которая на самом деле равна 0/0, но ответ будет 10.
  Вывод здесь аналогичен тому, который я изложил в предыдущей главе об
использовании QR-разложения для вычисления обратной мат­ри­цы: при-
менение метода устранения по Гауссу–Жордану для вычисления обратной
мат­ри­цы, вероятно, будет более численно стабильным, чем полный алго-
ритм вычисления обратной мат­ри­цы, но инвертировать мат­ри­цу, близкую
к сингулярной или имеющую высокое кондиционное число, будет трудно,
независимо от используемого алгоритма.



LU-разложение
Буквы «LU» в LU-разложении означают «нижний верхний»1, как в нижнем
треугольном, верхнем треугольнике. Идея состоит в том, чтобы разложить
мат­ри­цу на произведение двух треугольных мат­риц:

1
    Англ. lower upper. – Прим. перев.
184  Приведение строк и LU-разложение

  A = LU.

  Вот числовой пример:




  А вот соответствующий исходный код Python (обратите внимание, что
функция для LU-разложения находится в библиотеке SciPy):
import scipy.linalg # LU в библиотеке scipy
A = np.array([ [2,2,4], [1,0,3], [2,1,2] ])
_, L, U = scipy.linalg.lu(A)

# распечатать их
print('L: '), print(L)
print('U: '), print(U)

L:
[[1. 0. 0. ]
 [0.5 1. 0. ]
 [1. 1. 1. ]]

U:
[[ 2. 2. 4.]
 [ 0. -1. 1.]
 [ 0. 0. -3.]]

   Откуда взялись эти две мат­ри­цы? На самом деле вы уже знаете ответ: при-
ведение строк может быть выражено как (L–1A = U), где L–1 содержит набор
манипуляций со строками, который преобразовывает плотную мат­ри­цу A
в верхнетреугольную (ступенчатую) мат­ри­цу U.
   Поскольку ступенчатая форма не уникальна, LU-разложение не являет-
ся обязательно уникальным. То есть существует бесконечная пара нижне-
и верхнетреугольных мат­риц, которые можно перемножать, чтобы получать
мат­ри­цу A. Однако добавление ограничения, состоящего в том, что диагона-
ли мат­ри­цы L равны 1, обеспечивает, чтобы LU-разложение было уникаль-
ным для квадратной полноранговой мат­ри­цы A (это видно в предыдущем
примере). Уникальность LU-разложений рангово-пониженных мат­риц и не-
квадратных мат­риц потребует более продолжительного изложения, которое
я здесь не буду рассматривать; однако алгоритм LU-разложения библиотеки
SciPy является детерминированным, а это означает, что повторяющиеся LU-
разложения заданной мат­ри­цы будут идентичными.
                                                       LU-разложение  185


Взаимообмен строками посредством матриц
перестановок
Некоторые мат­ри­цы очень легко преобразовываются в верхнетреугольную
форму. Рассмотрим следующую ниже мат­ри­цу:




  Она не находится в ступенчатой форме, но была бы в ней, если бы мы по-
меняли местами вторую и третью строки. Взаимообмен строками – это один
из приемов приведения строк, который реализуется посредством мат­ри­цы
перестановок:




  Матрицы перестановок часто обозначаются как P. Таким образом, полное
LU-разложение на самом деле принимает следующий вид:

     PA = LU;
      A = PTLU.

  Примечательно, что мат­ри­цы перестановок ортогональны, поэтому P–1 =
 T
P . Вкратце: причина состоит в том, что все элементы мат­ри­цы перестановок
равны либо 0, либо 1, а поскольку строки обмениваются местами только один
раз, каждый столбец имеет ровно один ненулевой элемент (и действительно,
все мат­ри­цы перестановок являются единичными мат­ри­ца­ми со взаимооб-
меном строк). Следовательно, точечное произведение любых двух столбцов
равно 0, а точечное произведение столбца на самого себя равно 1, и, стало
быть, PTP = I.
  Важно: приведенные выше формулы дают математическое описание LU-
разложения. Библиотека Scipy на самом деле возвращает A = PLU, что также
можно было бы записать как PTA = LU. Упражнение 10.4 дает возможность
обследовать этот дезориентирующий момент.
  На рис. 10.1 показан пример LU-разложения, примененного к случайной
мат­ри­це.
186  Приведение строк и LU-разложение




                     Рис. 10.1  Визуализация LU-разложения

  LU-разложение используется в нескольких приложениях, включая вычис-
ление определителя и обратной мат­ри­цы. В следующей главе вы увидите, как
LU-разложение применяется в вычислении методом наименьших квадратов.



Резюме
Я открыл эту главу, пообещав увлекательное образовательное приключение.
Надеюсь, вы испытали несколько всплесков адреналина, изучая новые взгля-
ды на алгебраические уравнения, разложение мат­риц и обратную мат­ри­цу.
Вот ключевые выводы, которые следует вынести из этой главы.
    Системы уравнений можно преобразовывать в матричное уравнение.
      Помимо обеспечения компактного представления, это обеспечивает
      возможность находить самые изощренные линейно-алгебраические
      решения систем уравнений.
    При работе с матричными уравнениями следует помнить, что манипу-
      ляции должны применяться к обеим частям уравнения и что умноже-
      ние мат­риц некоммутативно.
    Приведение строк – это процедура, в которой строки мат­ри­цы A ска-
      лярно умножаются и складываются до тех пор, пока мат­ри­ца не будет
      линейно преобразована в верхнетреугольную мат­ри­цу U. Набор линей-
      ных преобразований можно хранить в еще одной мат­ри­це L–1, которая
      умножает мат­ри­цу A слева, чтобы произвести выражение L–1A = U.
    Приведение строк веками использовалось для ручного решения систем
      уравнений, включая обратную мат­ри­цу. Мы по-прежнему использу-
      ем процедуру приведения строк, хотя компьютеры берут на себя всю
      арифметику.
    Приведение строк также используется для реализации LU-разложения.
      LU-разложение является уникальным в условиях некоторых ограниче-
      ний, которые встроены в функцию lu() библиотеки SciPy.



Упражнения по программированию
  Упражнение 10.1
  LU-разложение бывает вычислительно емким, хотя оно и эффективнее
других разложений, таких как QR. Интересно, что LU-разложение часто ис-
                                           Упражнения по программированию  187

пользуется в качестве эталона для сравнения времени вычислений между
операционными системами, аппаратными процессорами, компьютерными
языками (например, C, Python и MATLAB) или реализуемыми алгоритмами.
Из любопытства я проверил время, которое потребовалось Python и MATLAB
на выполнение LU-разложения на тысяче мат­риц размера 100×100. На моем
ноутбуке MATLAB заняло около 300 мс, а Python – около 410 мс. Исходный
код Python в Google Colab занял около 1000 мс. Проверьте, сколько времени
он займет на вашем компьютере.

   Упражнение 10.2
   Примените метод матричного умножения, чтобы создать мат­ри­цу 6×8
ранга 3. Возьмите его LU-разложение и покажите три мат­ри­цы с их рангами
в заголовке, как на рис. 10.2. Обратите внимание на ранги трех мат­риц и на
все единицы на диагонали мат­ри­цы L. Можете свободно обследовать ранги
мат­риц с другими размерами и рангами.




                       Рис. 10.2  Результаты упражнения 10.2

   Упражнение 10.3
   Одним из примерений LU-разложения является вычисление определите-
ля1. Вот два свойства определителя: определитель треугольной мат­ри­цы ра-
вен произведению диагоналей, а определитель мат­ри­цы произведения равен
произведению определителей (то есть det(AB) = det(A)det(B)). Объединив эти
два факта вместе, вы сможете вычислить определитель мат­ри­цы как произ-
ведение диагоналей мат­ри­цы L, умноженное на произведение диагоналей
мат­ри­цы U. С другой стороны, поскольку все диагонали мат­ри­цы L равны 1
(при реализации на Python в целях обеспечения уникальности разложения),
определитель мат­ри­цы A – это просто произведение диагоналей мат­ри­цы U.
Попробуйте это на Python – и, перед тем как читать следующий далее абзац,
сравните с результатом вызова функции np.linalg.det(A) – несколько раз
с разными случайными мат­ри­ца­ми.

1
    Это один из бесчисленных аспектов линейной алгебры, которые вы могли бы
    изучить­ в традиционном учебнике по линейной алгебре; они интересны сами по
    себе, но имеют меньшее отношение к науке о данных.
188  Приведение строк и LU-разложение

  Получили ли вы тот же результат, что и Python? Допускаю, что вы обнару-
жили совпадение детерминантов по величине, но знаки, по-видимому, будут
различаться случайным образом. Почему это произошло? Это произошло
потому, что в инструкциях я опустил мат­ри­цу перестановок. Определитель
мат­ри­цы перестановок равен +1 при четном числе взаимообменов строк
и −1 при нечетном числе взаимообменов строк. Теперь вернитесь к своему
исходному коду и вставьте определитель P в свои вычисления.

  Упражнение 10.4
  Следуя формуле из раздела «LU-разложение» на стр. 183, обратная мат­ри­
ца выражается как

  A = PTLU;
  A–1 = (PTLU)–1;
  A–1 = U–1L–1P.

  Реализуйте третье уравнение напрямую, используя данные на выходе из
функции scipy.linalg.lu на мат­ри­це случайных чисел 4×4. Будет ли AA–1 еди-
ничной матрицей? Иногда да, а иногда нет, в зависимости от P. Это расхож-
дение возникает по причине описанных мной данных на выходе из функции
scipy.linalg.lu. Скорректируйте исходный код так, чтобы он соответствовал
соглашению, принятому в библиотеке SciPy, а не математическому согла-
шению.
  Вот вывод, который следует вынести из этого упражнения: отсутствие
сообщений об ошибках не обязательно означает, что ваш исходный код яв-
ляется правильным. Пожалуйста, проверяйте исправность своего математи-
ческого исходного кода как можно тщательнее.

   Упражнение 10.5
   Для мат­ри­цы A = PLU (с использованием упорядочения мат­ри­цы переста-
новок на Python) ATA может вычисляться как UTLTLU – без мат­риц переста-
новок. Почему можно отбросить мат­ри­цу перестановок? Ответьте на вопрос,
а затем, используя случайные мат­ри­цы, подтвердите на Python, что ATA =
UTLTLU, даже при P ¹ I.
Глава          11
     Общие линейные модели
      и наименьшие квадраты

Вселенная – действительно большое и реально замысловатое место. Все жи-
вотные на Земле обладают естественным любопытством, побуждающим их
обследовать и пытаться понять окружающую среду, но мы, люди, наделены
интеллектом, позволяющим разрабатывать научные и статистические ин-
струменты, чтобы выводить наше любопытство на новый уровень. Вот по-
чему у нас есть самолеты, аппараты МРТ, марсоходы, вакцины и, конечно же,
книги, подобные этой.
  Как мы познаем вселенную? Разрабатывая математически обоснованные
теории и собирая данные, чтобы тестировать и совершенствовать эти теории.
И это подводит нас к статистическим моделям. Статистическая модель – это
упрощенное математическое представление какого-то отдельного аспек-
та мира. Некоторые статистические модели характерны своей простотой
(например, предсказание роста фондового рынка в течение десятилетий);
другие гораздо более изощренны, например проект Blue Brain, в котором
симулируется деятельность мозга с такой точностью, что одна секунда симу-
лируемой активности требует 40 минут вычислительного времени.
  Ключевое отличие статистических моделей (в отличие от других мате-
матических моделей) заключается в том, что они содержат свободные па-
раметры, которые подгоняются к данным. Например, я знаю, что фондовый
рынок со временем вырастет, но не знаю, насколько. Поэтому я допускаю, что
изменение цены на фондовом рынке с течением времени (то есть наклон1)
является свободным параметром, числовое значение которого определяется
данными.
  Разработка статистической модели сопряжена с различными трудностями
и требует творческого подхода, опыта и знаний. Но отыскание свободных
параметров, основываясь на подгонке модели к данным, является элемен-
тарным вопросом линейной алгебры – на самом деле вы уже знаете всю
математику, необходимую для этой главы; это просто вопрос соединения
частей воедино и усвоения статистической терминологии.

1
    Син. угол наклона, угловой коэффициент. – Прим. перев.
190  Общие линейные модели и наименьшие квадраты


Общие линейные модели
Статистическая модель представляет собой систему уравнений, связыва-
ющих предсказатели (именуемые независимыми переменными) с наблюде-
ниями (именуемыми зависимой переменной). В модели фондового рынка
независимой переменной является время, а зависимой переменной – цена
на фондовом рынке (например, количественно определяемая как индекс S&P
500).
  В этой книге я сосредоточусь на общих линейных моделях, сокращенно
обозначаемых как GLM1. Например, регрессия является одним из типов об-
щей линейной модели.


Терминология
Статистики используют несколько иную терминологию, чем линейные алгеб­
раисты. В табл. 11.1 показаны ключевые буквы и описания векторов и мат­
риц, используемых в общей линейной модели.

Таблица 11.1. Таблица членов общей линейной модели
Линейная алгебра Статистика           Описание
Ax = b           Xβ = y               Общая линейная модель (GLM)
A                X                    Расчетная мат­ри­ца (столбцы = независимые
                                      переменные, предсказатели, регрессоры)
x                      β              Коэффициенты регрессии или бета-параметры
b                      y              Зависимая переменная, исход, мера результата,
                                      данные



Настройка общей линейной модели
Настройка общей линейной модели предусматривает:
  1)	определение уравнения, которое связывает предсказательные пере-
      менные с зависимой переменной;
  2) отображение наблюдаемых данных в уравнения;
  3) преобразование серии уравнений в матричное уравнение;
  4) решение этого уравнения.
  Я буду использовать простой пример, чтобы конкретизировать указан-
ную процедуру. У меня есть модель, которая предсказывает рост взрослого
человека на основе веса и роста родителей. Уравнение выглядит следующим
образом:

    y = β0 + β1w + β2h + ϵ,

1
    Англ. General Linear Model. – Прим. перев.
                                                    Общие линейные модели  191

где y – это рост человека, w – его вес, h – рост его родителей (средние пока-
затели матери и отца). ϵ – ошибка (также именуемая остатком), потому что
невозможно разумно ожидать, что вес и рост родителей полностью опреде-
ляют рост человека; наша модель не учитывает громадное число факторов,
и дисперсия, не связанная с весом и ростом родителей, будет поглощена
остатком.
  Моя гипотеза состоит в том, что вес и рост родителей имеют важность
для роста человека, но я не знаю степень важности каждой переменной.
Теперь введем члены β: это коэффициенты, или веса, которые говорят
мне о том, как комбинировать вес и рост родителей, чтобы предсказывать
рост человека. Другими словами, это линейно-взвешенная комбинация,
где β – это веса.
  Член β 0 называется пересечением1 (иногда его называют константой).
Член пересечения – это вектор, состоящий из одних единиц. Без указанного
члена линия наилучшей подгонки будет вынужденно проходить через на-
чало координат. Я объясню причину и покажу демонстрацию ближе к концу
главы.
  Теперь у нас есть уравнение, модель Вселенной (ну, хотя бы одна крошеч-
ная ее часть). Далее необходимо отобразить наблюдаемые данные в урав-
нения. Для простоты я выдумаю немного данных и сведу их в табл. 11.2
(как вы, наверное, догадались, y и h измеряются в сантиметрах, а w – в ки-
лограммах).

y       w      h           Таблица 11.2. Выдуманные данные
175     70     177         для статистической модели роста
181     86     190
159     63     180
165     62     172

  Отображение наблюдаемых данных в нашу статистическую модель
предусмат­ривает повторение уравнения четыре раза (что соответствует че-
тырем наблюдениям в наборе данных), всякий раз заменяя переменные y, w
и h измеренными данными:

    175 = β0 + 70β1 + 177β2 + ϵ;
    181 = β0 + 86β1 + 190β2 + ϵ;
    159 = β0 + 63β1 + 180β2 + ϵ;
    165 = β0 + 62β1 + 172β2 + ϵ.

  Пока что я опускаю член ϵ; об остатках я расскажу позже. Теперь нам нужно
переложить эту систему уравнений в матричное уравнение. Я знаю, что вы
знаете, как это делается, поэтому распечатаю здесь уравнение только для
того, чтобы вы могли подтвердить то, что уже знаете из главы 10:

1
    Син. точка пересечения, коэффициент сдвига. – Прим. перев.
192  Общие линейные модели и наименьшие квадраты




    И конечно же, это уравнение можно выразить кратко как Xβ = y.



Решение общих линейных моделей
Уверен, что вы уже знаете главную идею этого раздела: для того чтобы най-
ти вектор неизвестных коэффициентов β, надо просто умножить обе части
уравнения слева на левообратную мат­ри­цу X, то есть расчетную мат­ри­цу.
Решение выглядит так:

    Xβ = y;
    (XTX)–1XTXβ = (XTX)–1XTy;
    β = (XTX)–1XTy.

  Пожалуйста, всмотритесь в заключительное уравнение и удерживайте
свой взор на нем до тех пор, пока оно не отпечатается в вашем мозгу на-
всегда. Оно называется решением методом наименьших квадратов и явля-
ется одним из наиболее важных математических уравнений в прикладной
линейной алгебре. Вы будете встречать его в научных публикациях, учебни-
ках, блогах, лекциях, документационных литералах по функциям Python, на
рекламных щитах в Таджикистане1 и во многих других местах. При этом вы
будете видеть другие буквы или, возможно, некоторые дополнения, напри-
мер следующие ниже:

    b = (HTWH + λLTL)–1HTx.

  Смысл этого уравнения и интерпретация дополнительных мат­риц не важ-
ны (это различные способы регуляризации подгонки модели); важно то, что
вы способны увидеть формулу наименьших квадратов, встроенную в это за-
мысловато выглядящее уравнение (например, представьте, что W = I и λ = 0).
  Решение методом наименьших квадратов посредством левообратной мат­
ри­цы можно переложить напрямую в исходный код Python (переменная X –
это расчетная мат­ри­ца, а переменная y – вектор данных):
X_leftinv = np.linalg.inv(X.T@X) @ X.T

# найти коэффициенты
beta = X_leftinv @ y


1
    Признаться, никогда не видел это уравнение на таджикском рекламном щите, но
    суть в том, чтобы быть открытым ко всему новому.
                                            Решение общих линейных моделей  193

  Позже в этой главе я покажу результаты этих моделей – и то, как их ин-
терпретировать; сейчас же я бы хотел, чтобы вы сосредоточились на том, как
математические формулы переводятся в исходный код Python.

    Левообратная мат­ри­ца в спосоставлении с решателем методом наименьших
    квадратов NumPy
    Исходный код в этой главе является прямым переложением математики в исходный
    код Python. Вычисление левообратной мат­ри­цы в явной форме – не самый численно
    стабильный способ решения общей линейной модели (хотя и точен для простых за-
    дач этой главы), но я хочу, чтобы вы увидели, что кажущаяся абстрактной линейная
    алгебра работает реально. Существуют более численно стабильные способы решения
    общей линейной модели, включая QR-разложение (который вы увидите позже в этой
    главе) и более численно стабильные методы Python (с которыми вы познакомитесь
    в следующей главе).



Является ли решение точным?
Уравнение Xβ = y точно разрешимо, когда y находится в столбцовом про-
странстве расчетной мат­ри­цы X. Поэтому вопрос заключается в том, гаран-
тированно ли вектор данных находится в столбцовом пространстве статис­
тической модели. Ответ – нет, такой гарантии нет. На самом деле вектор
данных y почти никогда не находится в столбцовом пространстве мат­ри­цы X.
  В целях понимания причины, по которой такой гарантии нет, давайте пред-
ставим опрос студентов университета, в ходе которого исследователи пыта-
ются предсказать средний балл GPA (средний балл успеваемости) на основе
поведения, связанного с употреблением алкоголя. Опрос может содержать
данные, полученные от двух тысяч студентов, но имеет только три вопроса
(например, сколько алкоголя вы употребляете; как часто вы теряете созна-
ние; какой у вас средний балл). Данные содержатся в таблице 2000×3. Столб-
цовое пространство расчетной мат­ри­цы является 2D-подпространством
внутри этой объемлющей размерности 2000D, а вектор данных является
1D-подпространством внутри той же объемлющей размерности.
  Если данные находятся в столбцовом пространстве расчетной мат­ри­цы,
то это означает, что модель учитывает 100 % дисперсии данных. Но этого
почти никогда не происходит: реальные данные содержат шум и изменчи-
вость отбора экземпляров, а модели представляют собой упрощения, не учи-
тывающие всей изменчивости (например, средний балл GPA определяется
громадным числом факторов, которые наша модель игнорирует).
  Решение этой головоломки заключается в видоизменении уравнения об-
щей линейной модели так, чтобы учесть расхождение между модельно-пред-
сказанными и наблюдаемыми данными. Его можно выразить несколькими
эквивалентными способами (с точностью до знака):

     Xβ = y + ϵ;
  Xβ + ϵ = y;
       ϵ = Xβ – y.
194  Общие линейные модели и наименьшие квадраты

  Первое уравнение интерпретируется так: ϵ – это остаток, или член ошибки,
который добавляется в вектор данных, чтобы он помещался внутри столбцо-
вого пространства расчетной мат­ри­цы. Второе уравнение интерпретируется
следующим образом: остаточный член – это корректировка расчетной мат­
ри­цы таким образом, чтобы она идеально вписывалась в данные. Наконец,
интерпретация третьего уравнения такова: остаток определяется как раз-
ница между модельно-предсказанными и наблюдаемыми данными.
  Существует еще одна, очень проницательная интерпретация, которая под-
ходит к общим линейным моделям и методу наименьших квадратов с гео-
метрической точки зрения. Я вернусь к ней в следующем разделе.
  Суть же этого раздела в том, что наблюдаемые данные почти никогда не
находятся внутри подпространства, охватываемого регрессорами. По этой
причине также нередко можно увидеть, как общая линейная модель выра-
жается как X = βŷ, где ŷ = y + ϵ.
  Следовательно, цель общей линейной модели состоит в том, чтобы найти
линейную комбинацию регрессоров, максимально приближенную к наблю-
даемым данным. Подробнее об этом позже; теперь же я хочу познакомить
вас с геометрической перспективой наименьших квадратов.


Геометрическая перспектива наименьших
квадратов
До сих пор я представлял решение общей линейной модели с алгебраической
точки зрения решения матричного уравнения. Существует также геометри-
ческая перспектива общей линейной модели, которая обеспечивает альтер-
нативную перспективу и помогает раскрыть несколько важных особенностей
решения задачи о наименьших квадратах.
   Предположим, что столбцовое пространство расчетной мат­ри­цы C(X) яв-
ляется подпространством ℝM. В типичной ситуации это очень низкоразмер-
ное подпространство (то есть N << M), потому что статистические модели,
как правило, имеют гораздо больше наблюдений (строк), чем предсказателей
(столбцов). Зависимая переменная – это вектор y Î ℝM. Возникают вопросы:
принадлежит ли вектор y столбцовому пространству расчетной мат­ри­цы,
и если нет, то какая координата внутри столбцового пространства расчетной
мат­ри­цы максимально близка к вектору данных?
   Ответ на первый вопрос – нет, как я уже говорил в предыдущем разделе.
Второй вопрос имеет большую важность, потому что вы уже узнали ответ
в главе 2. Рассмотрите рис. 11.1, думая о решении.
   Итак, наша цель – найти набор коэффициентов β такой, что взвешенная
комбинация столбцов в X минимизирует расстояние до вектора данных y.
Мы можем назвать этот проекционный вектор ϵ. Как найти вектор ϵ и коэф-
фициенты β? Ортогональная проекция вектора используется точно так же,
как вы узнали в главе 2. Это означает, что можно применить тот же подход,
что и в главе 2, но с использованием мат­риц вместо векторов. Ключевым
                                          Решение общих линейных моделей  195

моментом является то, что кратчайшее расстояние между y и X определяется
проекционным вектором y – Xβ, который пересекает X под прямым углом:

          XTϵ = 0;
    T
   X (y – Xβ) = 0;
  XTy – XTXβ = 0;
        XTXβ = XTy;
            β = (XTX)–1XTy.




   Рис. 11.1  Абстрактное геометрическое представление общей линейной модели:
             найдите точку в столбцовом пространстве расчетной мат­ри­цы,
                    которая максимально близка к вектору данных

  Приведенная выше прогрессия уравнений просто замечательна: мы на-
чали с мысли об общей линейной модели как о геометрической проекции
вектора данных на столбцовое пространство расчетной мат­ри­цы, применили
принцип ортогональной проекции вектора, о котором вы узнали в начале
книги, и вуаля! Мы повторно получили то же самое левообратное решение,
что и при алгебраическом подходе.


В чем причина работы метода наименьших
квадратов?
Почему этот метод называется «наименьшими квадратами»? Что это за так
называемые квадраты и почему этот метод дает нам наименьший из них?
  «Квадраты» здесь относятся к квадратам ошибок между предсказанными
и наблюдаемыми данными. Для каждой -й предсказанной точки данных су-
ществует член ошибки, который определяется как ϵi = Xi β – yi. Обратите вни-
мание, что каждая точка данных предсказывается с использованием одного
и того же набора коэффициентов (то есть одинаковых весов, служащих для
комбинирования предсказателей в расчетной мат­ри­це). Все ошибки можно
улавливать в одном векторе:
196  Общие линейные модели и наименьшие квадраты

    ϵ = Xβ – y.

  Если модель вписывается в данные хорошо, то ошибки должны быть малы-
ми. Следовательно, можно сказать, что цель подгонки модели состоит в том,
чтобы выбирать элементы в β, которые минимизируют элементы в ϵ. Но
только одна минимизация ошибок приведет к тому, что модель будет предска-
зывать значения в сторону отрицательной бесконечности. И следовательно,
мы вместо этого минимизируем квадраты ошибок, которые соответствуют
их геометрическому квадрату расстояния до наблюдаемых данных y, не-
зависимо от того, является ли сама ошибка предсказания положительной
либо отрицательной1. Это то же самое, что минимизировать квадрат нормы
ошибок. Отсюда и название «наименьшие квадраты». Все это приводит к сле-
дующей ниже модификации:

    ||ϵ||2 = ||Xβ – y||2.

  Теперь на нее можно смотреть как на задачу оптимизации. В частности,
мы хотим найти набор коэффициентов, который минимизирует квадраты
ошибок. Эта минимизация выражается следующим образом:



  Решение этой оптимизации можно найти, установив производную целе-
вой функции равной нулю и применив чуть-чуть дифференциального ис-
числения2 и немного алгебры:




                       0 = XTXβ – XTy;
                  XTXβ = XTy;
                       β = (XTX)–1XTy.

  Удивительно, но мы начали с другой точки зрения – минимизировали
квадрат расстояния между модельно-предсказанными и наблюдаемыми зна-
чениями – и снова переоткрыли то же самое решение задачи о наименьших
квадратах, которое мы получили, просто используя линейно-алгебраическую
интуицию.
  На рис. 11.2 показано немного наблюдаемых данных (черными квадрата-
ми), их модельно-предсказанные значения (серыми точками) и расстояния
между ними (серыми пунктирными линиями). Все модельно-предсказанные

1
    Если вам интересно, то вместо квадратов расстояний также можно минимизиро-
    вать абсолютные расстояния. Эти две цели могут приводить к разным результатам;
    одним из преимуществ квадрата расстояния является удобная производная, кото-
    рая приводит к решению методом наименьших квадратов.
2
    Если вас не устраивает матричное исчисление, то не беспокойтесь об уравнениях;
    дело в том, что мы взяли производную по β, используя цепное правило.
                                    Общая линейная модель на простом примере  197

значения лежат на одной линии; цель метода наименьших квадратов состоит
в том, чтобы найти наклон и пересечение этой линии, которые минимизиру-
ют расстояния от предсказанных до наблюдаемых данных.


                         Наблюдаемые данные
                         Предсказанные данные




              Рис. 11.2  Визуальное понимание наименьших квадратов
                               на интуитивном уровне




                   Все дороги ведут к наименьшим квадратам
 Вы познакомились с тремя способами получения решения задачи о наименьших квад­
 ратах. Примечательно, что все подходы приводят к одному и тому же заключению:
 надо умножить обе части уравнения общей линейной модели слева на левообратную
 мат­ри­цу расчетной мат­ри­цы X. Разные подходы имеют уникальные теоретические пер-
 спективы, которые обеспечивают понимание природы и оптимальности метода наи-
 меньших квад­ратов. Но вся красота в том, что независимо от того, как начинать свое
 приключение в подгонке линейной модели, все в конечном итоге сходится к одному
 и тому же выводу.




Общая линейная модель на простом примере
В следующей главе вы увидите несколько примеров с реальными данными;
здесь же я хочу сосредоточиться на простом примере с поддельными данны-
ми. Поддельные данные получены в результате поддельного эксперимента,
в ходе которого я опросил случайную группу из 20 моих поддельных студен-
198  Общие линейные модели и наименьшие квадраты

тов и попросил их сообщить число пройденных ими моих онлайновых курсов
и их общую удовлетворенность жизнью1.
  В табл. 11.3 показаны первые 4 (из 20) строки мат­ри­цы данных.

Таблица 11.3. Таблица данных
Число курсов             Жизненное счастье
4                        25
12                       54
3                        21
14                       80

  Эти данные легче визуализировать в виде диаграммы рассеивания, кото-
рую вы видите на рис. 11.3.
                Общая удовлетворенность жизнью




                                                 Число пройденных курсов
                   Рис. 11.3  Поддельные данные из поддельного опроса

  Обратите внимание, что независимая переменная отложена по оси x, а за-
висимая переменная отложена по оси y. В статистике это общепринятое
правило.
  Нам нужно создать расчетную мат­ри­цу. Поскольку это простая модель
только с одним предсказателем, расчетная мат­ри­ца на самом деле пред-
ставляет собой только один вектор-столбец. Матричное уравнение Xβ = y
выглядит так (опять же, только первые четыре значения данных):

1
    В случае если это еще не совсем ясно: в данном примере используются полностью
    выдуманные данные; любое сходство с реальным миром имеет случайный ха­
    рактер.
                                                                  Общая линейная модель на простом примере  199




  Следующий ниже исходный код Python показывает решение. Переменные
numcourses и happy содержат данные; они обе являются списками и поэтому
должны быть конвертированы в многомерные массивы NumPy:
X = np.array(numcourses,ndmin=2).T

# вычислить левообратную мат­ри­цу
X_leftinv = np.linalg.inv(X.T@X) @ X.T

# найти коэффициенты
beta = X_leftinv @ happiness

  Формула наименьших квадратов говорит о том, что β = 5.95. Что означа-
ет это число? Это наклон в нашей формуле. Другими словами, по каждому
дополнительному курсу, который кто-либо проходит, его самооценка жиз-
ненного счастья увеличивается на 5.95 балла. Давайте посмотрим на то, как
этот результат выглядит на графике. На рис. 11.4 показаны данные (черными
квад­ратами), предсказанные значения счастья (соединенными линией серы-
ми точками) и остатки (пунктирной линией, соединяющей каждое наблюда-
емое значение с предсказанным значением).



                                                           Реальные данные
                                                           Предсказанные данные
                                                           Остаток
                Общая удовлетворенность жизнью




                                                                  Число пройденных курсов
                                                 Рис. 11.4  Наблюдаемые и предсказанные данные
                                                           (SSE = сумма квадратов ошибок)
200  Общие линейные модели и наименьшие квадраты

  Если, глядя на рис. 11.4, вы испытываете чувство беспокойства, то это
хорошо – значит, вы мыслите критически и заметили, что модель не очень
хорошо справляется с минимизацией ошибок. Можно легко представить, что
левая часть линии наилучшей подгонки должна быть сдвинута вверх, чтобы
получить более оптимальную подгонку. В чем тут проблема?
  Проблема в том, что расчетная мат­р и­ц а не содержит пересечения. Урав-
нение линии наилучшей подгонки таково: y = mx, означая, что y = 0 при
x = 0. В этой задаче указанное ограничение не имеет смысла – было бы
очень печально, если бы кто-то, кто не проходит мои курсы, был бы пол­
ностью лишен удовлетворенности жизнью. Вместо этого мы хотим, чтобы
наша линия имела форму y = mx + b, где b – это член пересечения, который
дает линии наилучшей подгонки возможность пересекать ось y при любом
значении. Статистически пересечение интерпретируется как ожидаемое
числовое значение наблюдений, когда предсказатели заданы равными
нулю.
  Добавление члена пересечения в расчетную мат­ри­цу дает следующие ниже
модифицированные уравнения (опять же, показаны только первые четыре
строки):




  Исходный код не меняется, за исключением создания расчетной мат­ри­цы:
X = np.hstack((np.ones((20,1)),
               np.array(numcourses,ndmin=2).T))

  Теперь мы обнаруживаем, что β – это двухэлементный вектор [22.9, 3.7].
Ожидаемый уровень счастья для того, кто прошел ноль курсов, составляет
22.9, а каждый дополнительный курс увеличивает его счастье на 3.7 балла.
Уверен, вы согласитесь, что рис. 11.5 выглядит намного лучше. И SSE со-
ставляет примерно половину того, что было, когда мы исключили пере-
сечение.
  Я позволю вам извлечь свои собственные выводы о поддельных резуль-
татах этого поддельного исследования, основанного на поддельных дан-
ных; вся суть была в том, чтобы увидеть численный пример того, как решать
систему уравнений путем формирования надлежащей расчетной мат­ри­цы
и отыскания решения для неизвестных регрессоров с помощью левообрат-
ной мат­ри­цы.
                                                       Наименьшие квадраты посредством QR-разложения  201



                                                       Реальные данные
                                                       Предсказанные данные
                                                       Остаток
            Общая удовлетворенность жизнью




                                                              Число пройденных курсов
                                             Рис. 11.5  Наблюдаемые и предсказанные данные,
                                                          теперь с пересечением




Наименьшие квадраты посредством
QR-разложения
Левообратный метод теоретически обоснован, но рискует численной не-
стабильностью. Отчасти это связано с тем, что для него требуется вычислять
обратную мат­ри­цу, которая, как вы уже знаете, может быть численно неста-
бильной. Но оказывается, что сама мат­ри­ца XTX может доставлять трудно-
сти. Умножение мат­ри­цы на ее транспонированную версию влияет на такие
свойства, как норма и кондиционное число. В главе 14 вы узнаете о кон-
диционном числе подробнее, но я уже упоминал, что мат­ри­цы с высоким
кондиционным числом бывают численно нестабильными и, следовательно,
расчетная мат­ри­ца с высоким кондиционным числом станет еще менее чис-
ленно стабильной при возведении в квадрат.
  QR-разложение обеспечивает более стабильный способ решения задачи
о наименьших квадратах. Проследите за следующей ниже последователь­
ностью уравнений:
202  Общие линейные модели и наименьшие квадраты

   Xβ = y;
  QXβ = y;
    Rβ = QTy;
     β = R–1QTy.

   Во-первых, эти уравнения немного упрощены по сравнению с реальными
низкоуровневыми численными реализациями. Например, мат­ри­ца R имеет
то же очертание, что и X, то есть является высокой (и, следовательно, необ-
ратимой), хотя только первые N строк отличны от нуля (как было обнаружено
в упражнении 9.7), а это означает, что строки с N + 1 по M не вносят вклад
в решение (при умножении мат­риц строки нулей дают нулевые результаты).
Эти строки можно удалить из R и из QTy. Во-вторых, взаимообмены строк
(реализованные посредством мат­риц перестановок) могут использоваться
для повышения численной стабильности.
   Но вот что самое приятное: отпадает всякая потребность в инвертиро-
вании мат­ри­цы R – она является верхнетреугольной, и поэтому решение
отыскивается с помощью обратной подстановки. Это то же самое, что решать
систему уравнений методом Гаусса–Жордана: расширить мат­ри­цу коэффи-
циентов константами, выполнить приведение строк, чтобы получить RREF-
форму, и извлечь решение из последнего столбца расширенной мат­ри­цы.
   Вывод здесь таков: QR-разложение решает задачу о наименьших квадра-
тах без возведения XTX в квадрат и без явного инвертирования мат­ри­цы.
Главный риск численной нестабильности связан с вычислением мат­ри­цы Q,
хотя при реализации посредством отражений Хаусхолдера она будет вполне
численно стабильной.
   Упражнение 11.3 проведет вас по этой реализации.



Резюме
Многие люди думают, что статистику трудно понять, потому что трудно по-
нять лежащую в ее основе математику. Безусловно, существуют продвинутые
статистические методы, предусматривающие продвинутую математику. Но
многие широко используемые статистические методы базируются на линей-
но-алгебраических принципах, которые вы теперь понимаете. Это означает,
что у вас больше нет оправданий, чтобы не освоить статистический анализ,
который применяется в науке о данных!
  Цель этой главы состояла в том, чтобы познакомить вас с терминологией
и математикой, лежащими в основе общей линейной модели, геометриче-
ской интерпретацией и последствиями математики для минимизации раз-
ниц между модельно-предсказанными и наблюдаемыми данными. Я также
показал применение регрессии на простом игрушечном примере. В следую­
щей главе вы увидите метод наименьших квадратов, реализованный на ре-
альных данных, и увидите расширения метода наименьших квадратов в ре-
грессии, такие как полиномиальная регрессия и регуляризация.
                                      Упражнения по программированию  203

  Вот ключевые выводы, которые следует вынести из этой главы.
    Общая линейная модель (GLM) – это статистический каркас для по-
      нимания нашей богатой и прекрасной Вселенной. Она работает путем
      формирования системы уравнений, точно такой же, как системы урав-
      нений, о которых вы узнали в предыдущей главе.
    Члены уравнений в линейной алгебре и статистике несколько отлича-
      ются; поняв терминологическую соотнесенность между ними, статис­
      тика станет проще, потому что вы уже знаете математику.
    Метод наименьших квадратов для решения уравнений посредством
      уравнения левообратной мат­ри­цы лежит в основе многих видов статис­
      тического анализа, и вы нередко будете видеть, как внутри кажущихся
      замысловатыми формул «спрятано» решение методом наименьших
      квадратов.
    Формула наименьших квадратов выводится посредством алгебры, гео-
      метрии или дифференциального исчисления. За счет этого обеспечи-
      вается несколько способов понимания и интерпретации метода наи-
      меньших квадратов.
    Умножение вектора наблюдаемых данных на левообратную мат­ри­цу
      в концептуальном плане является правильным образом мыслей о наи-
      меньших квадратах. На практике другие методы, такие как LU- и QR-
      разложение, являются более численно стабильными. К счастью, по
      этому поводу беспокоиться не приходится, потому что Python обраща-
      ется к низкоуровневым библиотекам (в основном библиотеке LAPACK),
      в которых реализованы наиболее численно стабильные алгоритмы.



Упражнения по программированию
  Упражнение 11.1
  Я объяснил, что остатки ортогональны предсказанным данным (другими
словами, ϵTŷ = 0). Проиллюстрируйте данное утверждение на игрушечном
наборе данных из этой главы. В частности, сделайте диаграмму рассеяния
предсказанных данных относительно ошибок (как на рис. 11.6). Затем вычис-
лите точечное произведение и коэффициент корреляции между остатками
и модельно-предсказанными данными. Теоретически оба результата должны
быть равны нулю, хотя будут некоторые ошибки округления. Какой из этих
двух видов анализа (точечное произведение либо корреляция) меньше и по-
чему?

  Упражнение 11.2
  Модельно-предсказанное счастье – это всего лишь один из способов ли-
нейного комбинирования столбцов расчетной мат­ри­цы. Но вектор остатков
не только ортогонален этой одной линейно-взвешенной комбинации; как раз
наоборот – вектор остатков ортогонален всему подпространству, охватывае­
мому расчетной матрицей. Продемонстрируйте это на Python (подсказка:
подумайте о левом нуль-пространстве и ранге).
204  Общие линейные модели и наименьшие квадраты




                Модельно-предсказанные данные




                                                             Остаточная ошибка
                                                Рис. 11.6  Решение упражнения 11.1



  Упражнение 11.3
  Теперь вы собираетесь вычислить метод наименьших квадратов посред-
ством QR-разложения, как я объяснял в разделе «Наименьшие квадраты по-
средством QR-разложения» на стр. 201. В частности, вычислите и сравните
следующие ниже методы решения:
  1) метод левообратной мат­ри­цы (XTX)–1XTy;
  2) метод QR-разложения с обратной матрицей R–1QTy;
  3)	метод устранения по Гауссу–Жордану на мат­ри­це R, расширенной век-
      тором QTy.
  Распечатайте бета-параметры из трех методов, как показано ниже. (Округ­
ление до трех цифр после запятой является дополнительным заданием по
программированию.)
Беты из левообратной мат­ри­цы:
[23.13 3.698]

Беты из QR с inv(R):
[23.13 3.698]

Беты из QR с обратной подстановкой:
[[23.13 3.698]]

  Наконец, распечатайте результирующие мат­ри­цы, полученные из QR-ме­
тода, как показано ниже:
                                                                                                       Упражнения по программированию  205

     Матрица R:
     [[ -4.472 -38.237]
      [ 0.      17.747]]

     Матрица R|Q'y:
     [[ -4.472 -38.237 -244.849]
      [ 0.        17.747 65.631]]

     Матрица RREF(R|Q'y):
     [[ 1.    0.    23.13 ]
      [ 0.    1.     3.698]]

        Упражнение 11.4
        Выбросы – это значения данных, которые являются необычными или не-
     репрезентативными. Выбросы могут вызвать серьезные проблемы в статис­
     тических моделях и, следовательно, могут вызывать серьезную «головную
     боль» у исследователей данных. В этом упражнении мы создадим выбросы
     в данных о счастье, чтобы понаблюдать за их влиянием на результирующее
     решение задачи о наименьших квадратах.
        В векторе данных измените первую наблюдаемую точку данных с 70 на
     170 (имитация опечатки при вводе данных). Затем пересчитайте подгонку
     методом наименьших квадратов и выведите данные на график. Повторите
     эту симуляцию выброса, но измените окончательную точку данных с 70 на
     170 (и верните первой точке данных ее исходное значение 70). Сравните с из-
     начальными данными, создав визуализацию, как на рис. 11.7.
Общая удовлетворенность жизнью




                                                           Общая удовлетворенность жизнью




                                                                                                                           Общая удовлетворенность жизнью




                                 Реальные данные
                                 Предсказ. данные
                                 Остаток




                                                                                                        Реальные данные                                                 Реальные данные
                                                                                                        Предсказ. данные                                                Предсказ. данные
                                                                                                        Остаток                                                         Остаток


                                 Число пройденных курсов                                    Число пройденных курсов                                         Число пройденных курсов

                                                       Рис. 11.7  Решение упражнения 11.4

       Интересно, что выброс был идентичен в переменной исхода (в обоих слу-
     чаях 70 превратились в 170), но влияние на подгонку модели к данным было
     совершенно разным из-за соответствующего значения по оси x. Это диф-
     ференцирующее влияние выбросов называется рычагом, и это то, о чем вы
     узнаете из более глубокого обсуждения статистики и подгонки моделей.

      Упражнение 11.5
      В этом упражнении вы вычислите обратную мат­ри­цу с помощью наимень-
     ших квадратов, следуя интерпретации, которую я представил в предыдущей
206  Общие линейные модели и наименьшие квадраты

главе. Мы рассмотрим уравнение XB = Y, где X – это полноранговая квадрат-
ная мат­ри­ца, которую необходимо инвертировать, B – мат­ри­ца неизвестных
коэффициентов (которая будет обратной матрицей), а Y – «наблюдаемые
данные» (единичная мат­ри­ца).
  Вы вычислите B тремя способами. Во-первых, примените левообратный
метод наименьших квадратов, чтобы вычислять мат­ри­цу по одному столбцу
за раз. Это делается путем вычисления наименьших квадратов между мат­
рицей X и каждым столбцом мат­ри­цы Y в цикле for. Во-вторых, примените
левообратный метод, чтобы вычислить всю мат­ри­цу B в одной строке ис-
ходного кода. И наконец, вычислите X–1 с помощью функции np.linalg.inv().
Умножьте каждую из этих мат­риц B на X и покажите на рисунке, подобном
рис. 11.8. Наконец, проверьте эквивалентность этих трех «разных» способов
вычисления обратной мат­ри­цы (так и должно быть, потому что обратная
мат­ри­ца является уникальной).
  Наблюдение: довольно странно (не говоря уже о том, чтобы зацикленно)
использовать обратную мат­ри­цу мат­ри­цы XTX для вычисления обратной
мат­ри­це X мат­ри­цы! Излишне говорить, это не вычислительный метод, ко-
торый можно было бы реализовать на практике. Однако это упражнение
усиливает интерпретацию обратной мат­ри­цы как преобразования, которое
проецирует мат­ри­цу на единичную мат­ри­цу, и понимание того, что эта про-
екционная мат­ри­ца может быть получена методом наименьших квадратов.
Сравнение решения задачи о наименьших квадратах с функцией np.linalg.
inv также иллюстрирует численные неточности, которые могут возникать
при вычислении левообратной мат­ри­цы.

    Посредством постолбцовых            Посредством построчных
      наименьших квадратов               наименьших квадратов        Посредством функции inv()




                               Рис. 11.8  Решение упражнения 11.5
Глава           12
               Применения метода
            наименьших квадратов

В этой главе вы увидите несколько применений подгонки на основе наи-
меньших квадратов к реальным данным. Попутно вы научитесь реализо-
вывать метод наименьших квадратов, используя несколько разных и более
численно стабильных функций Python, а также усвоите несколько новых
понятий из статистики и машинного обучения, таких как мультиколлине-
арность, полиномиальная регрессия и алгоритм поиска в параметрической
решетке как альтернатива методу наименьших квадратов.
  К концу этой главы у вас будет более глубокое понимание приемов при-
менения метода наименьших квадратов в приложениях, включая важность
численно стабильных алгоритмов для «трудных» ситуаций, связанных с ран-
гово-пониженными расчетными мат­ри­ца­ми. И вы увидите, что обеспечива-
емое методом наименьших квадратов аналитическое решение превосходит
эмпирический метод поиска значений параметров.



Предсказывание количеств велопрокатов
на основе погоды
Я – большой поклонник велосипедов и большой поклонник пибимпапа (ко-
рейского блюда, приготовляемого из риса и овощей или мяса). Поэтому я был
счастлив найти общедоступный набор данных о велопрокатах в Сеуле1. Этот
набор данных содержит почти девять тысяч наблюдений о количестве вело-


1
    Сатишкумар В. Э., Пак Чанву и Чо Йонгюн. Применение методов глубокой пере-
    работки данных для предсказания спроса на велопрокат в столичном городе
    (V. E. Sathishkumar, Jangwoo Park, and Yongyun Cho, Using Data Mining Techniques
    for Bike Sharing Demand Prediction in Metropolitan City, Computer Communications,
    153, (March 2020): 353–366); данные скачаны с https://archive.ics.uci.edu/ml/datasets/
    Seoul+Bike+Sharing+Demand.
208  Применения метода наименьших квадратов

сипедов, бравшихся напрокат внутри города, и переменных о погоде, вклю-
чая температуру, влажность, количество осадков, скорость ветра и т. д.
  Указанный набор данных предназначен для предсказания спроса на вело-
прокат в зависимости от погоды и времени года. Такая информация важна
потому, что это поможет компаниям-арендодателям велосипедов и местным
органам власти оптимизировать доступность более здоровых видов транс-
порта. Это отличный набор данных, с ним можно многое сделать, и я призы-
ваю вас потратить время на его обследование. В данной главе я сосредоточусь
на разработке относительно простых регрессионных моделей предсказания
количества велопрокатов на основе нескольких признаков.
  Хотя эта книга посвящена линейной алгебре, а не статистике, тем не ме-
нее, прежде чем применять и интерпретировать какой-либо статистический
анализ, важно тщательно инспектировать данные. Онлайновый исходный
код содержит более подробную информацию об импорте и инспектировании
данных с использованием библиотеки pandas. На рис. 12.1 показаны данные
о велопрокатах (зависимая переменная) и количестве осадков (одна из не-
зависимых переменных).



                                                                      Количество велопрокатов
        Количество велопрокатов




                                                           Дата



                                                                                    Осадки (мм)
                 Осадки (мм)




                                                           Дата
                                  Рис. 12.1  Диаграммы рассеяния некоторых данных
                        Предсказывание количеств велопрокатов на основе погоды  209

  Обратите внимание, что осадки являются разреженной переменной – в ос-
новном это нули с относительно малым числом ненулевых значений. Мы
вернемся к этой теме в упражнениях.
  На рис. 12.2 показана мат­ри­ца корреляций четырех отобранных пере-
менных. В целом перед началом статистического анализа всегда неплохо
обследовать мат­ри­цы корреляций, потому что они будут показывать кор-
релирующие переменные (если они есть) и могут выявлять ошибки в дан-
ных (например, если две предположительно разные переменные полностью
коррелированы). В этом случае мы видим, что количество велопрокатов по-
ложительно коррелирует со временем и температурой (люди берут больше
велосипедов в более позднее время дня и в более теплую погоду) и отрица-
тельно коррелирует с количеством осадков. (Обратите внимание, что я не по-
казываю здесь статистическую значимость, поэтому данные интерпретации
являются качественными.)



                Велосипеды




                       Час




                Температура




                     Дождь



                              Велосипеды   Час    Температура   Дождь
                              Рис. 12.2  Матрица корреляций
                              четырех отобранных переменных

  В первом анализе я хочу предсказать количество велопрокатов в зависи-
мости от количества осадков и времени года. Времена года (зима, весна, лето,
осень) – это текстовые метки в наборе данных, и нам нужно конвертировать
их в числа, чтобы провести анализ. Мы могли бы перевести четыре сезона
в числа от 1 до 4, но сезоны характеризуются цикличностью, а регрессии –
линейностью. С этим можно справиться несколькими способами, в том числе
применяя анализ ANOVA вместо регрессии, используя кодирование с одним
активным состоянием1 (которое применяется в моделях глубокого обучения)

1
    Англ. one-hot-encoding; данный термин происходит из терминологии цифровых
    интегральных микросхем, в которой он описывает конфигурацию микросхемы, до-
    пускающую, чтобы только один бит был положительным (активным). – Прим. перев.
210  Применения метода наименьших квадратов

или бинаризуя времена года. Я собираюсь использовать последний подход
и обозначить осень и зиму «0», а весну и лето – «1». Это интерпретируется
так: положительный бета-коэффициент указывает на большее количество
велопрокатов весной/летом по сравнению с осенью/зимой.
  (Примечание по касательной: с одной стороны, я мог бы все упростить,
выбрав только непрерывные переменные. Но я хочу подчеркнуть, что наука
о данных – это нечто большее, чем просто применение формулы к набору
данных; есть много нетривиальных решений, которые влияют на пригодные
для проведения виды анализа и, следовательно, виды результатов, которые
можно получать.)
  В левой части рис. 12.3 показана расчетная мат­ри­ца, визуализированная
в виде изображения. Это общепринятое представление расчетной мат­ри­
цы, поэтому убедитесь, что вам удобно ее интерпретировать. Столбцы – это
регрессоры, а строки – это наблюдения. Столбцы иногда нормализуются,
чтобы облегчить визуальную интерпретацию, в случае если регрессоры име-
ют очень разные числовые шкалы, хотя здесь я этого не делал. Как видно
по рисунку, количество осадков характеризуется разряженностью, и набор
данных охватывает два осенне-зимних периода (черные области в средней
колонке) и один весенне-летний период (белая область в середине). Пере-
сечение очевидным образом имеет полностью белый цвет, потому что оно
принимает одно и то же значение для каждого наблюдения.

                           Расчетная мат­ри­ца
                                                                                            Зима
                                                                                            Лето
                                                         Арендованные велосипеды
         Наблюдение




                      Осадки    Время года Пересечение
                                Регрессор                                          Осадки
                      Рис. 12.3  Расчетная мат­ри­ца и немного данных

   В правой части рис. 12.3 показаны данные, нанесенные на график в виде
количества осадков относительно велосипедов, бравшихся в прокат отдельно
за два сезона. Очевидно, что данные не лежат на прямой, потому что на обе-
их осях есть много значений, равных или близких к нулю. Другими словами,
визуальное инспектирование данных говорит о том, что взаимосвязи между
переменными нелинейны, а это означает, что линейный подход к моделиро-
                                                      Предсказывание количеств велопрокатов на основе погоды  211

ванию может быть субоптимальным. Опять же, это подчеркивает важность
визуального инспектирования данных и тщательного отбора подходящей
статистической модели.
  Тем не менее мы будем продвигаться вперед, используя линейную модель,
подгоняемую к данным с применением метода наименьших квадратов. Сле-
дующий ниже исходный код показывает, как я создал расчетную мат­ри­цу
(переменная data – это кадр данных библиотеки pandas):
# создать расчетную мат­ри­цу и добавить пересечение
desmat = data[['Осадки (mm)','Сезоны']].to_numpy()
desmat = np.append(desmat, np.ones((desmat.shape[0],1)), axis=1)

# извлечь зависимую переменную (DV)
y = data[['Велопрокаты']].to_numpy()

# выполнить подгонку модели к данным с
# использованием метода наименьших квадратов
beta = np.linalg.lstsq(desmat, y, rcond=None)

  Значения бета для осадков и сезона составляют соответственно −80 и 369.
Эти числа показывают, что во время дождя количество велопрокатов мень-
ше, а весной/летом велопрокат больше, чем осенью/зимой.
  На рис. 12.4 показаны предсказанные и наблюдаемые данные, отдельно
по двум сезонам. Если бы модель вписывалась в данные идеально, то точки
лежали бы на диагональной линии с наклоном, равным 1. Очевидно, это не
так, а значит модель была не очень хорошо подогнана к данным. И действи-
тельно, R2 составляет ничтожные 0.097 (другими словами, статистическая
модель объясняет около 1 % дисперсии данных). Кроме того, хорошо видно,
что модель предсказывает отрицательное количество велопрокатов, что не
поддается интерпретации – количество велопрокатов является строго неот-
рицательным.

                                                                    Подгонка модели (R2): 0.097
        Предсказанные количества велопрокатов




                                                                                                       Зима
                                                                                                       Лето

                                                                 Наблюдаемые количества велопрокатов
                                                Рис. 12.4  Диаграмма рассеяния предсказанных данных
                                                           относительно наблюдаемых данных
212  Применения метода наименьших квадратов

  До сих пор в исходном коде мы не получали ни предупреждений, ни оши-
бок; мы не сделали ничего плохого ни с точки зрения математики, ни с точки
зрения программирования. Однако примененная нами статистическая мо-
дель не является наиболее подходящей для этого исследовательского вопро-
са. В упражнениях 12.1 и 12.2 у вас будет возможность ее улучшить.


Регрессионная таблица с использованием
библиотеки statsmodels
Не слишком углубляясь в статистику, хочу вам показать, как создавать ре-
грессионную таблицу с помощью библиотеки statsmodels. Эта библиотека
работает с кадрами данных pandas вместо массивов NumPy. В следующем
ниже фрагменте исходного кода показано, как настраивать и вычислять ре-
грессионную модель (OLS1 означает обычный метод наименьших квадратов):
import statsmodels.api as sm

# извлечь данные (оставаясь с кадрами данных pandas)
desmat_df = data[['Осадки (mm)', 'Сезоны']]
obsdata_df = data['Велопрокаты']

# создать модель и выполнить ее подгонку
# (необходимо добавить пересечение в явной форме)
desmat_df = sm.add_constant(desmat_df)
model = sm.OLS(obsdata_df,desmat_df).fit()
print( model.summary() )

  Регрессионая таблица содержит много информации. Ничего страшного,
если вы не все понимаете; ключевыми искомыми элементами в ней являются
R2 и коэффициенты регрессии (coef) для регрессоров:
===========================================================================
Dep. Variable:            Велопрокаты R-squared:                    0.097
Model:                             OLS Adj. R-squared:                0.097
Method:                 Least Squares F-statistic:                    486.8
Date:                Wed, 26 Jan 2022 Prob (F-statistic):
Time:                        08:40:31 Log-Likelihood:               -68654.
No. Observations:                 8760 AIC:                       1.373e+05
Df Residuals:                     8757 BIC:                       1.373e+05
Df Model:                            2
Covariance Type:            nonrobust
===========================================================================
                   coef std err        t    P>|t|      [0.025        0.975]
===========================================================================
const          530.4946     9.313      56.963   0.000     512.239 548.750
Rainfall(mm) -80.5237       5.818 -13.841       0.000     -91.928 -69.120
Seasons        369.1267    13.127      28.121   0.000     343.395 394.858

1
    Англ. Ordinary Least Squares. – Прим. перев.
                         Предсказывание количеств велопрокатов на основе погоды  213

===========================================================================
Omnibus:                     1497.901 Durbin-Watson:                  0.240
Prob(Omnibus):                  0.000 Jarque-Bera (JB):            2435.082
Skew:                           1.168   Prob(JB):                      0.00
Kurtosis:                       4.104 Cond. No.                        2.80
===========================================================================



Мультиколлинеарность
Если вы посещали курсы статистики, то, возможно, слышали о термине муль-
тиколлинеарность. Его определение в Википедии гласит: «в модели множест­
венной регрессии одна предсказательная переменная может с существенной
степенью точности линейно предсказываться по другим переменным»1.
  Это означает, что в расчетной мат­ри­це существуют линейные зависимо-
сти. На языке линейной алгебры мультиколлинеарность – это просто причуд-
ливый термин, который обозначает линейную зависимость, что равносильно
утверждению, что расчетная мат­ри­ца имеет пониженный ранг либо что она
сингулярна.
  Рангово-пониженная расчетная мат­ри­ца не имеет левообратной мат­ри­
цы, а значит, задача о наименьших квадратах не решаема аналитически. Вы
увидите последствия мультиколлинеарности в упражнении 12.3.


             Решение общей линейной модели с мультиколлинеарностью
    На самом деле существует возможность аналитического вывода решения для общих ли-
    нейных моделей, которые имеют рангово-пониженную расчетную мат­ри­цу. Это делается
    с помощью модификации процедуры QR-разложения из предыдущей главы и с исполь-
    зованием псевдообратной мат­ри­цы Mура–Пенроуза. В случае рангово-пониженной рас-
    четной мат­ри­цы единственного решения нет, но можно выбрать решение с минималь-
    ной ошибкой. Это называется решением с минимальной нормой, или просто решением
    min-norm, и часто используется, например, в биомедицинской визуализации. Несмотря
    на специальные применения, расчетная мат­ри­ца с линейными зависимостями обычно
    указывает на проблему со статистической моделью и должна быть всячески обследована
    (распространенными источниками мультиколлинеарности являются ошибки ввода дан-
    ных и ошибки программирования).




Регуляризация
Регуляризация – это зонтичный термин, который относится к различным спо-
собам модификации статистической модели с целью улучшения численной
стабильности, преобразования сингулярных или плохо кондиционных мат­
риц в полноранговые (и, следовательно, обратимые) или улучшения обоб-
щаемости за счет уменьшения переподгонки. В зависимости от характера

1
    Википедия, поиск по «multicollinearity», https://en.wikipedia.org/wiki/Multicollinearity.
214  Применения метода наименьших квадратов

задачи и цели регуляризации применяется несколько форм регуляризации;
некоторые конкретные методы, о которых вы, возможно, слышали, включают
гребневую (она же L2), лассо (она же L1), тихоновскую и усадочную1.
   Разные методы регуляризации работают по-разному, но многие регуляри-
заторы «сдвигают» расчетную мат­ри­цу на некоторую величину. Из главы 5
вы помните, что сдвиг мат­ри­цы означает добавление некоторой константы
к диагонали в виде A + λI, а из главы 6 – что сдвиг мат­ри­цы может преобра-
зовывать рангово-пониженную мат­ри­цу в полноранговую.
   В этой главе мы выполним регуляризацию расчетной мат­ри­цы, сдвинув
ее в соответствии с некоторой долей ее фробениусовой нормы. За счет этого
видоизменится решение уравнения 12.1 наименьших квадратов.

    Уравнение 12.1. Регуляризация

    β = (XTX + γ||X||2F I)–1XTy.

   Ключевым параметром является γ (греческая буква гамма), который опре-
деляет степень регуляризации (обратите внимание, что γ = 0 соответствует
отсутствию регуляризации). Выбор подходящего параметра γ нетривиален
и нередко осуществляется с помощью статистических методов, таких как
перекрестная валидация.
   Наиболее очевидный эффект регуляризации заключается в том, что если
расчетная мат­ри­ца имеет пониженный ранг, то регуляризованная квадрат-
ная расчетная мат­ри­ца имеет полный ранг. Регуляризация также уменьшает
кондиционное число, которое измеряет «разброс» информации в мат­ри­це
(это отношение наибольшего сингулярного числа к наименьшему; вы узна-
ете о нем в главе 14). Благодаря ей увеличивается численная стабильность
мат­ри­цы. В статистическом плане последствием регуляризации является
«сглаживание» решения за счет снижения чувствительности модели к от-
дельным точкам данных, которые могут быть выбросами либо нерепрезен-
тативными, и, следовательно, вероятность наблюдать их в новых наборах
данных весьма мала.
   Почему я шкалирую на квадрат нормы Фробениуса? Учтите, что указанное
значение γ, скажем γ = 0.01, может иметь огромное или незначительное вли-
яние на расчетную мат­ри­цу в зависимости от диапазона числовых значений
в мат­ри­це. Поэтому мы шкалируем в числовой диапазон мат­ри­цы, то есть мы
интерпретируем параметр γ как долю регуляризации. Причина возведения
в квадрат нормы Фробениуса заключается в том, что ||X||2F = ||XTX||F . Другими
словами, квадрат нормы расчетной мат­ри­цы равен норме расчетной мат­ри­
цы, умноженной на ее транспонированную версию.
   На самом деле вместо нормы Фробениуса чаще используется среднее зна-
чение собственных чисел расчетной мат­ри­цы. Познакомившись с собствен-
ными числами в главе 13, вы сможете сравнить два метода регуляризации.
   Реализации регуляризации в исходном коде посвящено упражнение 12.4.

1
    Англ. shrinkage. – Прим. перев.
                                                      Полиномиальная регрессия  215


Полиномиальная регрессия
Полиномиальная регрессия похожа на обычную регрессию, но независимы-
ми переменными являются значения оси x, возводимые в более высокие
степени. То есть каждый столбец i расчетной мат­ри­цы определяется как xi,
где x – это обычно время или пространство, но может быть и другими пере-
менными, такими как дозировка лекарства или численность населения. Ма-
тематическая модель выглядит так:

  y = β0 x 0 + β 1x 1 + … + β n x n.

  Обратите внимание на x0 = 1, которое дает пересечение модели. В против-
ном случае это по-прежнему будет обычной регрессией – цель состоит в том,
чтобы найти значения β, которые минимизируют квадраты разниц между
предсказанными и наблюдаемыми данными.
  Степень многочлена равна наибольшей степени i. Например, полиноми-
альная регрессия четвертой степени имеет члены до x4 (если нет члена x3, то
это по-прежнему модель четвертой степени с β3 = 0).
  На рис. 12.5 показан пример отдельных регрессоров и расчетные мат­ри­
цы многочлена третьей степени (имейте в виду, что многочлен n-й степени
имеет n + 1 регрессоров, включая пересечение). Полиномиальные функции
являются базисными векторами, служащими для моделирования наблюда-
емых данных.

                         Отдельные регрессоры                       Расчетная мат­ри­ца




                 Рис. 12.5  Расчетная мат­ри­ца полиномиальной регрессии
216  Применения метода наименьших квадратов

   За исключением специальной расчетной мат­ри­цы, полиномиальная ре-
грессия выполняется точно так же, как и любая другая регрессия: применить
левообратную мат­ри­цу (либо более вычислительно стабильные альтернати-
вы), чтобы получить набор коэффициентов, такой что взвешенная комбина-
ция регрессоров (то есть предсказанные данные) совпадает с наблюдаемыми
данными наилучшим образом.
   Полиномиальные регрессии используются для подгонки кривых и ап-
проксимации нелинейных функций. Приложения включают моделирование
временных рядов, динамику населения, функции доза–реакция в медицин-
ских исследованиях и физические нагрузки на опорные балки. Полиномы
также могут выражаться в 2D, которые используются для моделирования
пространственной структуры, такой как распространение землетрясений
и активность мозга.
   Но довольно фоновой информации. Давайте поработаем на примере.
Я выбрал набор данных, который основан на модели удвоения населения.
Вопрос заключается в следующем: «Сколько времени потребуется для того,
чтобы население человечества удвоилось (например, с пятисот миллионов
до одного миллиарда)?» Если скорость прироста населения сама по себе уве-
личивается (поскольку у большего числа людей рождается больше детей,
а у тех, кто вырастает, рождается еще больше детей), то время удвоения будет
уменьшаться с каждым удвоением. С другой стороны, если рост населения
замедляется (люди рожают меньше детей), то время удвоения будет увели-
чиваться по сравнению с последующими удвоениями.
   Я нашел подходящий набор данных в интернете1. Это небольшой набор
данных, поэтому все цифры доступны в онлайновом исходном коде и пока-
заны на рис. 12.6. Указанный набор данных включает в себя как фактические
измеренные данные, так и проекции до 2100 года. Эти проекции в будущее
основаны на ряде допущений, и никто толком не знает, как сложится будущее
(поэтому вам следует находить баланс между подготовкой к будущему и на-
слаждением моментом). Тем не менее данные на сей момент показывают, что
за последние пятьсот лет (по меньшей мере) население человечества удва-
ивалось с возрастающей частотой, и авторы набора данных предсказывают,
что скорость удвоения немного увеличится в следующем столетии.
   Для подгонки к данным я выбрал многочлен третьей степени, а затем соз-
дал модель и выполнил ее подгонку, используя следующий ниже исходный
код (переменная year содержит координаты по оси x, а переменная doubleTime
содержит зависимую переменную):
# расчетная мат­ри­ца
X = np.zeros((N, 4))
for i in range(4):
    X[:, i] = np.array(year)**i

# выполнить подгонку модели и
# вычислить предсказанные данные


1
    Розер Макс, Ричи Ханна и Ортис-Оспина Эстебан. Прирост населения мира, Our-
    WorldInData.org, 2013 г., https://ourworldindata.org/world-population-growth.
                                                          Полиномиальная регрессия  217

beta = np.linalg.lstsq(X, doubleTime, rcond=None)
yHat = X@beta[0]



             Время удвоения (годы)




                                                   Год
                                     Рис. 12.6  График данных

  На рис. 12.7 показаны предсказанные данные с использованием полино-
миальной регрессии, созданной этим исходным кодом.
             Время удвоения (годы)




                                                   Год
                                     Рис. 12.7  График данных

  Модель улавливает как нисходящий тренд, так и спроецированный ви-
ток вверх в данных. Без дальнейшего статистического анализа невозможно
сказать, что это наилучшая модель или что указанная модель статистически
значимо хорошо вписывается в данные. Но ясно одно, что полиномиальные
218  Применения метода наименьших квадратов

регрессии хорошо подходят для подгонки кривых. В упражнении 12.5 вы
продолжите обследовать эту модель и данные, но рекомендую вам поэкс­
периментировать с исходным кодом, создающим рис. 12.7, пробуя разные
параметры степени.
  Полиномиальные регрессии находят широкое применение, и в библио-
теке NumPy имеются специальные функции для создания и подгонки таких
моделей:
beta = np.polyfit(year, doubleTime, 3) # 3-я степень
yHat = np.polyval(beta, year)




Поиск в параметрической решетке
для отыскания модельных параметров
Метод наименьших квадратов посредством левообратной мат­ри­цы – от-
личный способ подгонки моделей к данным. Метод наименьших квадратов
является точным, быстрым и детерминированным (это означает, что при
каждом повторном прогоне исходного кода вы будете получать один и тот
же результат). Но он работает только для подгонки линейных моделей, и не
все модели можно подгонять с помощью линейных методов.
   В этом разделе я познакомлю вас с еще одним методом оптимизации,
используемым для выявления модельных параметров, именуемым поиском
в параметрической решетке1. Поиск в параметрической решетке работает
путем отбора значений параметра в параметрическом пространстве, вычис-
ления подгонки модели к данным с каждым значением параметра, а затем
отбора значения, дающего наилучшую подгонку модели.
   В качестве простого примера возьмем функцию y = x2. Нам нужно найти
минимум этой функции. Разумеется, мы уже знаем, что минимум находится
в x = 0; это поможет понять и оценить результаты поиска в решетке пара-
метров.
   При выполнении поиска в параметрической решетке мы начинаем
с предопре­деленного набора тестируемых значений x. Давайте воспользуем-
ся набором (−2, −1, 0, 1, 2). Это наша «решетка». Затем по каждому значению
в решетке мы вычисляем функцию и получаем y = (4, 1, 0, 1, 4). И находим, что
минимум y происходит при x = 0. В этом случае решение на основе решетки
совпадает с истинным решением.
   Но поиск в параметрической решетке не гарантирует оптимального реше-
ния. Например, представьте, что наша решетка была бы (−2, −0.5, 1, 2.5); зна-
чения функции были бы y = (4, 0.25, 1, 6.25), и мы бы заключили, что x = –0.5
является значением параметра, которое минимизирует функцию y = x2. Этот


1
    Англ. Grid Search; идея поиска в параметрической решетке состоит в создании
    «решетки» параметров и просто опробывании всех возможных их комбинаций. –
    Прим. перев.
       Поиск в параметрической решетке для отыскания модельных параметров  219

вывод будет «отчасти правильным», потому что в указанной решетке это луч-
шее решение. Ошибки при поиске в параметрической решетке также могут
возникать из-за неправильно выбранного диапазона значений. Представьте,
например, что наша решетка была бы (−1000, −990, −980, −970); мы пришли
бы к выводу, что y = x2 минимизируется при x = –970.
   Дело в том, что и диапазон, и разрешающая способность (расстояние меж-
ду точками решетки) имеют большую важность, потому что они определяют
получаемое вами решение, которое может быть наилучшим, довольно хоро-
шим либо ужасным. В приведенном выше игрушечном примере подходящий
диапазон и разрешающая способность определяются легко. В многосложных,
многопеременных, нелинейных моделях для поиска в параметрической ре-
шетке могут потребоваться дополнительная работа и обследование подхо-
дящих параметров.
   Я выполнил поиск в параметрической решетке на данных о «счастливых
студентах» из предыдущей главы (напомню: это были поддельные данные
из поддельного опроса, показывающего, что люди, записавшиеся на большее
количество моих курсов, были более удовлетворены жизнью). Модель этих
данных имеет два параметра – пересечение и наклон, поэтому мы вычисляем
эту функцию в каждой точке двумерной решетки возможных пар парамет­
ров. Результаты показаны на рис. 12.8.
   Что означает этот график и как его интерпретировать? Две оси соответ-
ствуют значениям параметров, поэтому каждая координата на этом графике
создает модель с соответствующими значениями параметров. Затем вычис-
ляется подгонка каждой из этих моделей к данным, сохраняется и визуали-
зируется в виде изображения.
   Координата с наилучшей полгонкой к данным (наименьшая сумма квадра-
тов ошибок) является оптимальным набором параметров. На рис. 12.8 также
показано аналитическое решение с использованием метода наименьших
квадратов. Они близки, но не накладываются друг на друга точно. В упраж-
нении 12.6 у вас будет возможность реализовать этот поиск по параметри-
ческой решетке, а также обследовать вопрос важности разрешающей способ-
ности решетки для точности результатов.
   Зачем вообще кому-то использовать поиск в параметрической решетке,
когда метод наименьших квадратов работает лучше и быстрее? Все верно,
не следует использовать метод поиска в параметрической решетке, когда
жизнеспособным решением является метод наименьших квадратов. Вместе
с тем поиск в параметрической решетке является полезным методом отыска-
ния параметров в нелинейных моделях и нередко используется, например,
для определения гиперпараметров в моделях глубокого обучения (гиперпара-
метры – это конструктивные особенности модельной архитектуры, которые
отбираются исследователем, а не извлекаются из данных). В случае больших
моделей поиск в параметрической решетке бывает времязатратным, но его
параллелизация может делать его выполнение более приемлемым.
   Вывод состоит в том, что поиск в параметрической решетке является не-
линейным методом подгонки моделей к данным в ситуациях, когда линей-
ные методы применить невозможно. По ходу своего путешествия по науке
о данных и машинному обучению вы также узнаете о дополнительных не-
220  Применения метода наименьших квадратов

линейных методах, включая симплекс и знаменитый алгоритм градиентного
спуска, который приводит в действие глубокое обучение.

                                 SSE (подгонка модели к данным)
                                   Минимум по параметрической решетке
                                   Аналитическое решение
                   Пересечение




                                             Наклон
             Рис. 12.8  Результаты поиска в параметрической решетке
                       на наборе данных «Счастливый студент».
                 Интенсивность показывает сумму квадратов ошибок,
                               подогнанных к данным




Резюме
Надеюсь, вам понравилось читать о применении метода наименьших квад­
ратов и сравнении с другими подходами к подгонке моделей. Следующие
ниже упражнения являются наиболее важной частью этой главы, поэтому
я не хочу отнимать у вас время подведением итогов. Вот важные моменты
главы.
    Визуальное обследование данных имеет большую важность для отбора
      правильных статистических моделей и правильной интерпретации
      статистических результатов.
    Линейная алгебра используется для количественного оценивания на-
      боров данных, включая мат­ри­цы корреляций.
    Методы визуализации мат­риц, о которых вы узнали в главе 5, полезны
      для инспектирования расчетных мат­риц.
    В разных областях математические понятия иногда получают разные
      названия. В этой главе понятие мультиколлинеарности является одним
      из таких примеров; оно означает линейные зависимости в расчетной
      мат­ри­це.
                                      Упражнения по программированию  221

    Регуляризация предусматривает «сдвиг» расчетной мат­ри­цы на неко-
      торую малую величину, в целях повышения численной стабильности
      и вероятности обобщения на новых данных.
    Глубокое понимание линейной алгебры поможет вам отбирать наи-
      более подходящий вид статистического анализа, интерпретировать
      результаты и предвидеть возможные проблемы.
    Полиномиальная регрессия аналогична «обычной» регрессии, но
      столбцы в расчетной мат­ри­це определяются как значения по оси x,
      возводимые в возрастающую степень. Полиномиальные регрессии ис-
      пользуются для подгонки кривых.
    Поиск в параметрической решетке – это нелинейный метод подгонки
      моделей. Линейный метод наименьших квадратов является оптималь-
      ным подходом, когда модель является линейной.



Упражнения по программированию
Упражнения по аренде велосипедов
  Упражнение 12.1
  Возможно, часть проблемы с отрицательными велопрокатами на рис. 12.4
можно решить, исключив дни без дождя. Повторите анализ и график этого
анализа, но выберите только те строки данных, в которых имеется нулевое
количество осадков. Улучшатся ли результаты с точки зрения более высокого
R2 и положительного предсказанного количества велопрокатов?

   Упражнение 12.2
   Поскольку времена года – это категориальная переменная, дисперси-
онный анализ действительно был бы более подходящей статистической
мо­д елью, чем регрессия. Возможно, бинаризованным временам года не
хватает чувствительности для предсказания количеств велопрокатов (на-
пример, осенью могут быть теплые солнечные дни, а весной – холодные
дождливые дни), и поэтому температура может быть самым лучшим пред-
сказателем.
   Замените времена года в расчетной мат­ри­це температурой и выполни-
те повторный прогон регрессии (можно использовать все дни, а не только
дни без осадков из предыдущего упражнения) и воспроизведите рис. 12.9.
Остается проблема предсказания отрицательных велопрокатов (это связано
с линейностью модели), но R2 получается выше, и предсказание выглядит
качественно лучше.
222  Применения метода наименьших квадратов

                                                           Подгонка модели (R2): 0.313
      Предсказанные количества велопрокатов




                                                        Наблюдаемые количества велопрокатов
                                              Рис. 12.9  Результаты упражнения 12.2



Упражнения по мультиколлинеарности
  Упражнение 12.3
  Это упражнение продолжает работу с моделью из упражнения 12.21. Ука-
занная модель содержит три регрессора, включая пересечение. Создайте но-
вую расчетную мат­ри­цу, содержащую четвертый регрессор, определенный
как некоторая линейно-взвешенная комбинация температуры и количества
осадков. Дайте этой расчетной мат­ри­це другое имя переменной, потому что
она понадобится вам в следующем упражнении. Подтвердите, что расчетная
мат­ри­ца имеет четыре столбца, но имеет ранг 3, и вычислите мат­ри­цу кор-
реляций расчетной мат­ри­цы.
  Обратите внимание, что в зависимости от перевесовки двух переменных
корреляция ожидаемо не будет равна 1, даже при линейных зависимостях;
здесь вы также не ожидаете воспроизвести точные корреляции:
Размер расчетной мат­ри­цы: (8760, 4)
Ранг расчетной мат­ри­цы: 3

Матрица корреляций расчетной мат­ри­цы:
[[1.      0.05028     nan 0.7057 ]
 [0.05028 1.          nan 0.74309]
 [    nan     nan     nan     nan]
 [0.7057 0.74309      nan 1.      ]]

1
    Если вы столкнулись с ошибками Python, то попробуйте выполнить повторный
    прогон предыдущего исходного кода, а затем повторно создать переменные рас-
    четной мат­ри­цы.
                                              Упражнения по программированию  223

  Выполните подгонку модели, используя три разных программных подхода:
  1)	прямая реализация с левообратной матрицей, как вы узнали из пре-
      дыдущей главы;
  2) с использованием функции NumPy lstsqr;
  3) с использованием статистических моделей.
  Для всех трех методов вычислите R2 и коэффициенты регрессии. Распеча-
тайте результаты, как показано ниже. Отчетливо видна численная нестабиль-
ность функции np.linalg.inv на рангово-пониженной расчетной мат­ри­це.
ПОДГОНКА МОДЕЛИ К ДАННЫМ:
Левообратная мат­ри­ца: 0.0615
np lstsqr             : 0.3126
statsmodels           : 0.3126

БЕТА-КОЭФФИЦИЕНТЫ:
Левообратная мат­ри­ца: [[-1528.071 11.277 337.483 5.537 ]]
np lstsqr             : [[ -87.632 7.506 337.483 5.234 ]]
statsmodels           : [ -87.632 7.506 337.483 5.234 ]

   Попутное замечание: никаких сообщений об ошибках, ни предупрежде-
ний не поступало; Python просто выдал результаты, хотя с расчетной матри-
цей явно что-то не так. Можно обсуждать достоинства такого поведения, но
этот пример еще раз подчеркивает важность понимания линейной алгебры
в науке о данных и что правильная наука о данных – это больше, чем просто
знание математики.


Упражнения по регуляризации
  Упражнение 12.4
  Здесь вы займетесь обследованием эффектов регуляризации на рангово-
пониженную расчетную мат­ри­цу, которую вы создали в предыдущем упраж-
нении. Начните с реализации (XTX + γ||X||2F I)–1, используя γ = 0 и γ = 0.01.
Распечатайте размер и ранг двух мат­риц. Вот мои результаты (интересно
отметить, что расчетная мат­ри­ца ранга 3 является настолько численно не-
стабильной, что ее «обратная мат­ри­ца» фактически имеет ранг 2):
размер inv(X'X + 0.0*I): (4, 4)
ранг inv(X'X + 0.0*I) : 2

размер inv(X'X + 0.01*I): (4, 4)
ранг inv(X'X + 0.01*I) : 4

  Теперь об эксперименте. Здесь цель состоит в том, чтобы обследовать эф-
фекты регуляризации на подгонку модели к данным. Напишите исходный
код, который будет вычислять подгонку к данным как R2, используя метод
наименьших квадратов с регуляризацией на расчетных матрицах с мульти-
коллинеарностью и без нее. Поместите этот исходный код в цикл for, реали-
224  Применения метода наименьших квадратов

зующий диапазон значений γ от 0 до 0.2. Затем покажите результаты в виде
рисунка, подобного рис. 12.10.
   Между прочим, тривиальным для полноранговых расчетных мат­риц слу-
чаем является уменьшение подгонки модели с увеличением регуляриза-
ции – и действительно, регуляризация предназначена для того, чтобы де-
лать модель менее чувствительной к данным. Важный вопрос заключается
в том, улучшает ли регуляризация подгонку к тестовому набору данных или
валидационному блоку, который был исключен при подгонке модели. Если
регуляризация выгодна, то можно ожидать увеличение обобщаемости ре-
гуляризованной модели до некоторого γ, а затем снова ее уменьшение. Это
тот уровень детализации, о котором вы узнаете из специальной книги по
статистике или машинному обучению, хотя в главе 15 вы все же доберетесь
до перекрестной валидации исходного кода.


                                                                   Изначальные
                                                                   Мультиколлинеарные
     R 2 подгонки к данным




                                              Регуляризация λ
                             Рис. 12.10  Результаты упражнения 12.4



Упражнение по полиномиальной регрессии
  Упражнение 12.5
  Цель этого упражнения – выполнить подгонку полиномиальной регрессии,
используя диапазон степеней от нуля до девяти. В цикле for перевычисляйте
регрессию и предсказанные значения данных. Покажите результаты, как на
рис. 12.11.
  В этом упражнении высвечиваются проблемы недоподгонки и перепод-
гонки. Модель со слишком малым числом параметров плохо справляется
с предсказыванием данных. С другой стороны, модель с большим числом
параметров слишком хорошо прилегает к данным и рискует оказаться чрез-
мерно чувствительной к шуму и быть не способной обобщать на новых дан-
ных. Стратегии отыскания баланса между недоподгонкой и переподгонкой
                                               Упражнения по программированию  225

включают перекрестную валидацию и байесов информационный критерий;
с этими темами вы познакомитесь в книге по машинному обучению или
статистике.

    Степень = 0        Степень = 1     Степень = 2       Степень = 3    Степень = 4




    Степень = 5        Степень = 6     Степень = 7       Степень = 8    Степень = 9




                        Рис. 12.11  Результаты упражнения 12.5



Упражнения по поиску в параметрической
решетке
  Упражнение 12.6
  Здесь ваша цель будет простой: воспроизвести рис. 12.8, следуя инструкци-
ям, представленным в тексте вокруг этого рисунка. Распечатайте коэффици-
енты регрессии для сравнения. Например, следующие ниже результаты были
получены с использованием параметра разрешающей способности парамет­
рической решетки, заданной равной 50:
Аналитический результат:
Пересечение: 23.13, наклон: 3.70

Эмпирический результат:
Пересечение: 22.86, наклон: 3.67

   Когда у вас будет рабочий исходный код, попробуйте несколько разных
параметров разрешающей способности. Я сделал рис. 12.8, используя разре-
шающую способность 100; вам также следует попробовать другие значения,
например 20 или 500. Еще обратите внимание на время вычисления при
более высоких значениях разрешающей способности – и это только двухпа-
раметрическая модель! Исчерпывающий поиск в параметрической решетке
с высокой разрешающей способностью для 10-параметрической модели по-
требует чрезвычайно больших вычислительных ресурсов.

  Упражнение 12.7
  Вы видели два разных метода оценивания подгонки модели к данным:
сумма квадратов ошибок и R2. В предыдущем упражнении для оценива-
ния подгонки модели к данным вы использовали сумму квадратов ошибок;
226  Применения метода наименьших квадратов

в этом упражнении вы определите, является ли вариант с R2 столь же жиз-
неспособным.
   Часть исходного кода в этом упражнении будет простой: надо видоизме-
нить исходный код из предыдущего упражнения, вычислив R2 вместо SSE
(проследите, чтобы видоизменения коснулись копии исходного кода, без
перезаписи предыдущего упражнения).
   Теперь самое сложное: вы обнаружите, что R2 выглядит ужасно! Этот по-
казатель дает совершенно неправильный ответ. Ваша задача – выяснить при-
чину, по которой это так (онлайновое решение содержит изложение данного
вопроса). Совет: сохраняйте предсказанные данные по каждой паре парамет­
ров, чтобы иметь возможность инспектировать предсказанные значения,
а затем сравнивать их с наблюдаемыми данными.
Глава          13
                                             Собственное
                                              разложение

Собственное разложение1 – жемчужина линейной алгебры. Что такое жемчу-
жина? Позвольте мне процитировать прямо из книги «20 000 лье под водой»:

    Для поэта жемчужина – слеза моря, для восточных народов – окаменевшая капля
    росы; для женщин – драгоценный овальной формы камень с перламутровым
    блеском, который они носят, как украшение, на руках, на шее, в ушах; для хими-
    ка – соединение фосфорнокислых солей с углекислым кальцием; и, наконец, для
    натуралиста – просто болезненный нарост, представляющий собою шаровидные
    наплывы перламутра внутри мягкой ткани мантии у некоторых представителей
    двустворчатых моллюсков.
                                                                      – Жюль Верн

  Дело в том, что один и тот же объект видится по-разному в зависимости от
его использования. То же самое и с собственным разложением: собственное
разложение имеет геометрическую интерпретацию (оси вращательной инва-
риантности), статистическую интерпретацию (направления максимальной
ковариации), системно-динамическую интерпретацию (стабильные состоя­
ния системы), теоретико-графовую интерпретацию (влияние узла на его
сеть), финансово-рыночную интерпретацию (выявление ковариирующих
акций) и многие другие.
  Собственное разложение (и SVD, которое, как вы узнаете в следующей гла-
ве, тесно связано с собственным разложением) – один из самых важных вкла-
дов линейной алгебры в науку о данных. Цель этой главы – дать вам интуи-
тивное понимание собственных чисел и собственных векторов – результатов
собственного разложения мат­ри­цы. Попутно вы узнаете о диагонализации
и других специальных свойствах симметричных мат­риц. В главе 14 – после
того, как собственное разложение будет расширено на SVD, в главе 15 вы
увидите несколько его приложений.

1
    Англ. Eigendecomposition; син. спектральное разложение. – Прим. перев.
228  Собственное разложение


                                  Только для квадратов
    Собственное разложение определено только для квадратных мат­риц. Матрицу размера
    M×N невозможно разложить, если только M не равно N. Неквадратные мат­ри­цы можно
    раскладывать с помощью SVD. Каждая квадратная мат­ри­ца размера M×M имеет M соб-
    ственных чисел (скаляров) и M соответствующих собственных векторов. Собственное раз-
    ложение предназначено для выявления этих M векторно-скалярных пар.




Интерпретации собственных чисел
и собственных векторов
В следующих разделах я опишу несколько способов интерпретации собствен-
ных чисел/векторов. Конечно же, математика в любом случае остается оди-
наковой, но наличие нескольких перспектив может облегчить интуитивное
понимание, которое, в свою очередь, поможет разобраться в том, как и по-
чему собственное разложение имеет такую важность в науке о данных.


Геометрия
На самом деле в главе 5 я уже познакомил вас с геометрической концепцией
собственных векторов. На рис. 5.5 мы обнаружили, что существует особая
комбинация мат­ри­цы и вектора, при которой мат­ри­ца растягивает – но не
поворачивает – этот вектор. Этот вектор является собственным вектором
мат­ри­цы, а величина растяжения – собственным числом.
  На рис. 13.1 показаны векторы до и после умножения на мат­ри­цу 2×2.
Два вектора на левом графике (v1 и v2) являются собственными векторами,
а два вектора на правом графике – нет. Собственные векторы указывают
в одном направлении до и после постпозиционного умножения мат­р и­
цы. В собственных числах кодируется степень растяжения; попытайтесь
угадать собственные числа на основе визуального осмотра графика. От-
веты – в сноске1.
  Вот геометрическая картина: собственный вектор означает, что умноже-
ние мат­ри­цы на вектор действует как умножение вектора на скаляр. Давайте
посмотрим, сможем ли мы записать это в виде уравнения (мы можем, и это
напечатано в уравнении 13.1).

    Уравнение 13.1. Уравнение собственного числа

    Av = λv.


1
    Приблизительно 0.6 и 1.6.
                     Интерпретации собственных чисел и собственных векторов  229

  Будьте осторожны с интерпретацией этого уравнения: оно не говорит, что
мат­ри­ца равна скаляру; в нем говорится, что влияние мат­ри­цы на вектор
такое же, как влияние скаляра на тот же вектор.




                    Рис. 13.1  Геометрия собственных векторов

  Эта формула называется уравнением собственного числа, и это еще одна
ключевая формула линейной алгебры, которую стоит запомнить. Вы будете
сталкиваться с ней на протяжении всей этой главы, вы увидите ее неболь-
шую вариацию в следующей главе, и вы будете сталкиваться с ней много раз,
изучая многопеременную статистику, обработку сигналов, оптимизацию,
теорию графов и бесчисленное число других приложений, в которых среди
нескольких одновременно регистрируемых признаков выявляются законо-
мерности.


Статистика (анализ главных компонент)
Одной из причин, по которой люди применяют статистику, является вы-
явление и количественное оценивание взаимосвязей между переменными.
Например, повышение глобальной температуры коррелирует с сокращением
числа пиратов1, но насколько сильна эта взаимосвязь? Разумеется, когда
у вас есть только две переменные, будет достаточно простой корреляции
(подобной той, о которой вы узнали в главе 4). Но в многопеременном на-
боре данных, включающем десятки или сотни переменных, двухпеременные
корреляции не способны выявлять глобальные закономерности.
  Давайте конкретизируем это на примере. Криптовалюты – это цифровые
хранилища стоимости, закодированные в блочной цепи, так называемом
блокчейне, который представляет собой систему отслеживания транзакций.
Вы, наверное, слышали о биткойне и эфириуме; существуют десятки тысяч
других криптомонет, которые имеют различное предназначение. Можно за-

1
    «Открытое письмо школьному совету Канзаса», Церковь летающего макаронного
    монстра, spaghettimonster.org/about/open-letter.
230  Собственное разложение

дасться вопросом, работает ли все криптопространство как единая система
(имеется в виду, что стоимость всех монет движется вверх и вниз вместе)
или же в этом пространстве есть независимые подкатегории (имеется в виду,
что стоимость некоторых монет или групп монет меняется независимо от
стоимости других монет).
   Эту гипотезу можно проверить, выполнив анализ главных компонент
на наборе данных, содержащем цены различных криптовалют с течением
времени. Если бы весь крипторынок работал как единое целое, то график
крутого склона1 (график собственных чисел мат­ри­цы ковариаций в наборе
данных) показал бы, что одна компонента отвечает за бóльшую часть дис-
персии системы, а все остальные компоненты – за очень малую дисперсию
(график А на рис. 13.2). Напротив, если бы рынок криптовалют имел, скажем,
три главенствующие подкатегории с независимыми движениями цен, то мы
бы ожидали увидеть три больших собственных числа (график B на рис. 13.2).
              Собственное число




                                                    Собственное число




                                  Число компонент                       Число компонент
                     Рис. 13.2  Симулированные графики крутого склона
                на многопеременных наборах данных (данные просимулированы,
                       чтобы проиллюстрировать возможные результаты)



Подавление шума
Большинство наборов данных содержат шум. Шум относится к вариациям
в наборе данных, которые либо необъяснимы (например, случайные вариа-
ции), либо нежелательны (например, артефакты шума электрических линий
в радиосигналах). Существует целый ряд способов ослабления или устране-
ния шума, и оптимальная стратегия подавления шума зависит от характера
и происхождения шума, а также от характеристик сигнала.
   Одним из методов подавления случайного шума является идентификация
собственных чисел и собственных векторов системы, а также «редукция» на-
правлений в пространстве данных, связанных с малыми собственными чис-
лами. При этом принято исходить из допущения, что случайный шум вносит
относительно малый вклад в общую дисперсию. «Редуцирование» размер-

1
    Англ. scree plot. – Прим. перев.
                                          Отыскание собственных чисел  231

ности данных означает реконструкцию набора данных после приравнивания
к нулю некоторых собственных чисел, которые ниже некоторого порога.
   В главе 15 вы увидите пример использования собственного разложения
для подавления шума.


Уменьшение размерности (сжатие данных)
Информационные коммуникационные технологии, такие как телефоны, ин-
тернет и телевидение, создают и передают огромный объем данных, таких
как изображения и видео. Передача данных может занимать много времени
и средств, и выгодно сжимать данные перед их передачей. Сжатие означает
уменьшение размера данных (в байтах) при минимальном влиянии на ка-
чество данных. Например, файл изображения в формате TIFF может иметь
размер 10 Мб, тогда как преобразованная в JPG версия может иметь размер
0.1 Мб, сохраняя при этом достаточно хорошее качество.
   Один из способов размерно уменьшить набор данных состоит в том, чтобы
взять его собственное разложение, отбросить собственные числа и собствен-
ные векторы, связанные с малыми направлениями в пространстве данных,
а затем передать только относительно большие пары собственный вектор/
число. На самом деле для сжатия данных чаще используется SVD (в главе 15
вы увидите пример), хотя принцип остается тем же.
   Современные алгоритмы сжатия данных на самом деле работают быстрее
и эффективнее, чем ранее описанный метод, но идея остается той же: раз-
ложить набор данных на набор базисных векторов, которые охватывают наи-
более важные признаки данных, а затем восстановить высококачественную
версию изначальных данных.



Отыскание собственных чисел
Для того чтобы выполнить собственное разложение квадратной мат­ри­цы,
сначала нужно найти собственные числа, а затем использовать каждое соб-
ственное число для отыскания соответствующего собственного вектора. Соб-
ственные числа подобны ключам, которые вставляются в мат­ри­цу, чтобы
получить доступ к мистическому собственному вектору.
  Отыскание собственных чисел мат­ри­цы на Python выполняется очень лег-
ко:
matrix = np.array([
    [1,2],
    [3,4]
    ])

# получить собственные числа
evals = np.linalg.eig(matrix)[0]
232  Собственное разложение

  Два собственных числа (округленные до сотых) равны −0.37 и 5.37.
  Однако важный вопрос не в том, какая функция возвращает собственные
числа, а в том, как собственные числа мат­ри­цы идентифицируются.
  Для того чтобы найти собственные числа мат­ри­цы, мы начинаем с урав-
нения собственного числа, показанного в уравнении 13.1, и выполняем не-
сколько простых арифметических действий, как показано в уравнении 13.2.

    Уравнение 13.2. Реорганизованное уравнение собственного числа

    Av = λv;
    Av – λv = 0;
    (A – λI)v = 0.

   Первое уравнение является точным повторением уравнения собственного
числа. Во втором уравнении мы просто вычли правую часть, чтобы прирав-
нять уравнение к вектору нулей.
   Переход от второго к третьему уравнению требует некоторого пояснения.
В левой части второго уравнения есть два векторных члена, оба из которых
содержат v. И поэтому мы выносим этот вектор за скобки. Но в результате
мы остаемся с операцией вычитания мат­ри­цы и скаляра (A – λ), которая
в линейной алгебре не определена1. Вместо этого мы сдвигаем мат­ри­цу на λ.
Это подводит нас к третьему уравнению. (Попутное замечание: выражение
λI иногда называется скалярной матрицей.)
   Что означает это третье уравнение? Оно означает, что собственный вектор
находится в нуль-пространстве мат­ри­цы, сдвинутой на ее собственное число.
   Если это поможет вам понять концепцию собственного вектора как нуль-
пространственного вектора сдвинутой мат­ри­цы, то можете подумать о до-
бавлении двух дополнительных уравнений:

    Ã = A – λI;
    Ãv = 0.

  Почему это утверждение столь проницательно? Вспомните, что в линей-
ной алгебре тривиальные решения игнорируются, поэтому мы не считаем
v = 0 собственным вектором. А это означает, что сдвинутая на собственное
число мат­ри­ца является сингулярной, потому что только сингулярные мат­
ри­цы имеют нетривиальное нуль-пространство.
  А что еще мы знаем о сингулярных матрицах? Мы знаем, что их опреде-
литель равен нулю. Следовательно:

    |A – λI| = 0.

  Хотите верьте, хотите нет, но это ключ к отысканию собственных чисел:
надо сдвинуть мат­ри­цу на неизвестное собственное число λ, приравнять его


1
    Как я писал в главе 5, Python вернет результат, но это будет транслированное вы-
    читание скаляра, которое не является линейно-алгебраической операцией.
                                                 Отыскание собственных чисел  233

определитель к нулю и найти λ. Давайте посмотрим, как это выглядит для
мат­ри­цы 2×2:




        (a – λ)(d – λ) – bc = 0;
  λ2 – (a + d)λ + (ad – bc) = 0.

   Для того чтобы найти решение для двух значений λ, можно применить
квадратичную формулу. Но сам ответ не важен; важно увидеть логическую
последовательность математических понятий, сформулированных ранее
в этой книге:
     умножение мат­ри­цы на вектор действует как умножение вектора на
       скаляр (уравнение собственного числа);
     мы приравниваем уравнение собственного числа к вектору нулей и вы-
       носим за скобки общие члены;
     этим показывается, что собственный вектор находится в нуль-
       пространстве мат­ри­цы, сдвинутой на собственное число. Мы не счита-
       ем вектор нулей собственным вектором, а значит, сдвинутая мат­ри­ца
       является сингулярной;
     поэтому мы приравниваем определитель сдвинутой мат­ри­цы к нулю
       и находим неизвестное собственное число.
   Приравненный к нулю определитель сдвинутой собственным числом мат­
ри­цы называется характеристическим многочленом мат­ри­цы.
   Обратите внимание, что в предыдущем примере мы начали с мат­ри­цы
2×2 и получили член λ2, и, значит, это полиномиальное уравнение второй
степени. Возможно, из школьного курса алгебры вы помните, что многочлен
n-й степени имеет n решений, некоторые из которых могут быть комплекс­
нозначными (это называется фундаментальной теоремой алгебры). И по-
этому будет два удовлетворяющих уравнению числа λ.
   Совпадающие двойки не случайны: характеристический многочлен мат­
ри­цы M×M будет иметь член λM. По этой причине мат­ри­ца M×M будет иметь
M собственных чисел.

    Утомительные практические задачи
    На этом этапе в традиционном учебнике по линейной алгебре вам поручили бы най-
    ти вручную собственные числа десятков мат­риц 2×2 и 3×3. У меня по поводу такого
    рода упражнений смешанные чувства: с одной стороны, решение задач вручную дей-
    ствительно помогает усваивать механизм отыскания собственных чисел; но, с другой
    стороны, в этой книге я хочу сосредоточить внимание на концепциях, исходном коде
    и приложениях, не увязая в утомительной арифметике. Если вы чувствуете вдохнове-
    ние решать задачи на собственные числа вручную, тогда дерзайте! Вы можете найти
    массу таких задач в традиционных учебниках или в интернете. Но я принял смелое
234  Собственное разложение

     (и, возможно, спорное) решение избегать в этой книге задач, решаемых вручную,
     и вместо этого использовать упражнения, посвященные программированию и глубо-
     кому пониманию.




Отыскание собственных векторов
Как и в случае с собственными числами, отыскание собственных векторов на
Python выполняется очень легко:
evals,evecs = np.linalg.eig(matrix)
print(evals), print(evecs)

[-0.37228132 5.37228132]

[[-0.82456484 -0.41597356]
 [ 0.56576746 -0.90937671]]

  Собственные векторы находятся в столбцах мат­ри­цы evecs и в том же
порядке, что и собственные числа (то есть собственный вектор в первом
столбце мат­ри­цы evecs парно связан с первым собственным числом в век-
торе evals). Мне нравится использовать имена переменных evals и evecs,
потому что они короткие и осмысленные. Вы также, возможно, заметите,
что нередко используются имена переменных L и V или D и V. L – это Λ (за-
главная буква λ), а V – это V, мат­ри­ца, в которой каждый столбец i является
собственным вектором vi. D означает диагональ, потому что собственные
числа часто хранятся в диагональной мат­ри­це по причинам, которые я объ-
ясню позже в этой главе.

     Собственные векторы в столбцах, а не в строках!
     Самое важное, что нужно помнить о собственных векторах при программировании, –
     это то, что они хранятся в столбцах мат­ри­цы, а не в строках! Такие размерно-индек-
     сационные ошибки легко возникают при работе с квадратными мат­ри­ца­ми (потому
     что Python не будет реагировать сообщением об ошибке), но случайное использова-
     ние строк вместо столбцов мат­ри­цы собственных векторов может иметь катастрофи-
     ческие последствия в приложениях. Если вы сомневаетесь, то вспомните изложение
     в главе 2 о том, что в линейной алгебре общепринято исходить из допущения, что
     векторы имеют ориентацию вдоль столбца.

  Хорошо, но опять же, приведенный выше исходный код показывает, как
получать собственные векторы мат­ри­цы из функции NumPy. Это можно уз-
нать из документационного литерала docstring по функции np.linalg.eig.
Важный вопрос: а откуда берутся собственные векторы и как их отыскивать?
  На самом деле я уже писал, как отыскивать собственные векторы: надо
найти вектор v, который находится в нуль-пространстве мат­ри­цы, сдвинутой
на λ. Другими словами:

  vi Î N(A – λiI).
                                         Отыскание собственных векторов  235

  Давайте посмотрим на числовой пример. Ниже приведены мат­ри­ца и ее
собственные числа:




  Сосредоточимся на первом собственном числе. Для того чтобы выявить
его собственный вектор, мы сдвигаем мат­ри­цу на 3 и находим вектор в его
нуль-пространстве:




  Это означает, что [1 1] является собственным вектором мат­ри­цы, ассоци-
ированным с собственным числом 3.
  Я нашел этот нуль-пространственный вектор, просто взглянув на мат­ри­цу.
Как выявляют нуль-пространственные векторы (то есть собственные векто-
ры мат­ри­цы) на практике?
  Нуль-пространственные векторы можно найти методом Гаусса–Джорда-
на, чтобы решить систему уравнений, где мат­ри­ца коэффициентов – это
λ-сдвинутая мат­ри­ца, а вектор констант – вектор нулей. Это хороший спо-
соб концептуализировать решение. В реализации применяются более ста-
бильные численные методы отыскания собственных чисел и собственных
векторов, в том числе QR-разложение и процедура, именуемая степенным
методом.


Неопределенность собственных векторов
по знаку и шкале
Давайте я вернусь к числовому примеру из предыдущего раздела. Я написал,
что [1 1] – это собственный вектор мат­ри­цы, потому что этот вектор является
базисом нуль-пространства мат­ри­цы, сдвинутой на ее собственное число 3.
   Посмотрите на сдвинутую мат­ри­цу и спросите себя, является ли [1 1] един-
ственным возможным базисным вектором нуль-пространства? Даже близ-
ко ничего похожего! Можно также использовать [4 4] или [−5.4 −5.4], или…
Я думаю, вы понимаете, к чему это идет: любая шкалированная версия век-
тора [1 1] является базисом этого нуль-пространства. Другими словами, если
v – это собственный вектор мат­ри­цы, то таким же является и αv для любого
действительно-значного α, кроме нуля.
   В этой связи следует отметить, что важность собственных векторов обу­
словливается их направлением, а не их модулем.
   Бесконечность возможных нуль-пространственных базисных векторов
приводит к двум вопросам:
     существует ли один «самый лучший» базисный вектор? «Самого луч-
       шего» базисного вектора как такового не существует, но удобно иметь
236  Собственное разложение

        единично-нормализованные собственные векторы (евклидова нор-
        ма 1). Это особенно полезно для симметричных мат­риц по причинам,
        которые будут объяснены позже в этой главе1;
      каков «правильный» знак собственного вектора? Никакой. На самом
        деле при использовании разных версий NumPy, а также разных про-
        грамм, таких как MATLAB, Julia или Mathematica, вы можете получать
        разные знаки собственных векторов из одной и той же мат­ри­цы. Не-
        определенность знака собственного вектора – это просто особенность
        жизни в нашей Вселенной. В таких применениях, как PCA, существуют
        принципиальные способы придания им знака, но это просто общепри-
        нятое соглашение, облегчающее интерпретацию.



Диагонализация квадратной матрицы
Уравнение собственного числа, с которым вы теперь знакомы, содержит одно
собственное число и один собственный вектор. Это означает, что мат­ри­ца
M×M имеет M уравнений собственных чисел:

    Av1 = λ1v1;
      
    AvM = λMvM.

  В этой серии уравнений нет ничего неправильного, но она довольно урод-
лива, а уродство нарушает один из принципов линейной алгебры: делать
уравнения компактными и элегантными. Поэтому мы преобразовываем эту
серию уравнений в одно матричное уравнение.
  Ключевым моментом при написании уравнения матричного собственно-
го числа является то, что каждый столбец мат­ри­цы собственных векторов
шкалируется ровно одним собственным числом. Это реализуется с помощью
постпозиционного умножения на диагональную мат­ри­цу (как вы узнали из
главы 6).
  Таким образом, вместо хранения собственных чисел в векторе мы храним
собственные числа в диагонали мат­ри­цы. Следующее ниже уравнение пока-
зывает форму диагонализации для мат­ри­цы 3×3 (с использованием @ вместо
числовых значений мат­ри­цы). В мат­ри­це собственных векторов первое под-
строчное число соответствует собственному вектору, а второе подстрочное
число – элементу собственного вектора. Например, v12 – это второй элемент
первого собственного вектора:

1
    Для того чтобы снять неопределенность, следует отметить, что за счет этого мат­
    ри­ца собственных векторов делается ортогональной матрицей.
                                      Диагонализация квадратной матрицы  237




                                =


  Пожалуйста, найдите время, чтобы убедиться, что каждое собственное
число шкалирует все элементы соответствующего собственного вектора, а не
какие-либо другие собственные векторы.
  В более общем случае матричное уравнение собственных чисел – также
именуемое диагонализацией квадратной мат­ри­цы – таково:

  AV = VΛ.

  Функция eig библиотеки NumPy возвращает собственные векторы в мат­
ри­це и собственные числа в векторе. Это означает, что для диагонализации
мат­ри­цы в NumPy требуется немного дополнительного исходного кода:
evals,evecs = np.linalg.eig(matrix)
D = np.diag(evals)

  Между прочим, в математике часто интересно и познавательно перестав-
лять уравнения, находя решения для разных переменных. Рассмотрим сле-
дующий ниже список эквивалентных объявлений:

  AV = VΛ;
  A = VΛV–1;
  Λ = V–1AV.

   Второе уравнение показывает, что мат­ри­ца A становится диагональной
внутри пространства V (то есть V перемещает нас в «диагональное простран-
ство», а затем V–1 возвращает нас назад из диагонального пространства). Это
можно интерпретировать в контексте базисных векторов: мат­ри­ца A явля-
ется плотной в стандартном базисе, но затем мы применяем набор преоб-
разований (V), чтобы повернуть мат­ри­цу в новый набор базисных векторов
(собственных векторов), в которых информация разрежена и представлена
диагональной матрицей. (В конце уравнения нам нужно вернуться назад
в стандартное базисное пространство, отсюда и V–1.)
238  Собственное разложение


Особая удивительность симметричных матриц
Из предыдущих глав вы уже знаете, что симметричные мат­ри­цы обладают
особыми свойствами, благодаря которым с ними очень удобно работать.
Теперь вы готовы познакомиться с еще двумя специальными свойствами,
относящимися к собственному разложению.



Ортогональные собственные векторы
Симметричные мат­ри­цы имеют ортогональные собственные векторы. Это
означает, что все собственные векторы симметричной мат­ри­цы попарно
ортогональны. Давайте я начну с примера, затем разберу последствия ор-
тогональности собственных векторов и, наконец, покажу доказательство:
# всего лишь случайная симметричная мат­ри­ца
A = np.random.randint(-3, 4, (3,3))
A = A.T@A

# ее собственное разложение
L,V = np.linalg.eig(A)

# все попарные точечные произведения
print( np.dot(V[:,0], V[:,1]) )
print( np.dot(V[:,0], V[:,2]) )
print( np.dot(V[:,1], V[:,2]) )

  Все три точечных произведения равны нулю (в пределах вычислительных
ошибок округления порядка 10−16. (Обратите внимание, что я создал симмет­
ричные мат­ри­цы как случайную мат­ри­цу, умноженную на ее транспониро-
ванную версию.)
  Свойство ортогональности собственного вектора означает, что точечное
произведение между любой парой собственных векторов равно нулю, тогда
как точечное произведение собственного вектора с самим собой не равно
нулю (поскольку мы не считаем вектор нулей собственным вектором). Это
можно записать как VTV = D, где D – это диагональная мат­ри­ца с диагоналя-
ми, содержащими нормы собственных векторов.
  Но можно сделать еще лучше, чем просто диагональную мат­ри­цу: вспом-
ните, что важность собственных векторов обусловлена их направлением, а не
их модулем. И поэтому собственный вектор может иметь любой желаемый
модуль (кроме, очевидно, нулевого модуля).
  Давайте прошкалируем все собственные векторы так, чтобы они имели
единичную длину. Вопрос к вам: если все собственные векторы ортогональ-
ны и имеют единичную длину, что произойдет, если умножить мат­ри­цу соб-
ственных векторов на ее транспонированную версию?
  Вы, конечно же, знаете ответ:

  VTV = I.
                                            Ортогональные собственные векторы  239

   Другими словами, мат­ри­ца собственных векторов симметричной мат­ри­
цы является ортогональной матрицей! Это имеет целый ряд последствий
для науки о данных, в том числе то, что собственные векторы очень легко
инвертировать (потому что их надо просто транспонировать). Существуют
и другие последствия использования ортогональных собственных векторов
для таких применений, как анализ главных компонент, о которых я расскажу
в главе 15.
   В главе 1 я писал, что в этой книге сравнительно мало доказательств. Но
ортогональные собственные векторы симметричных мат­риц – это настоль-
ко важная концепция, что вам действительно нужно убедиться, что данное
утверждение доказано.
   Цель этого доказательства – показать, что точечное произведение любой
пары собственных векторов равно нулю. Мы исходим из двух допущений:
   1) мат­ри­ца A – это симметричная мат­ри­ца и
   2)	λ1 и λ2 – это отдельные собственные числа мат­ри­цы A (отдельные
       в смысле, что они не могут быть равны друг другу) с v1 и v2 в качестве
       соответствующих им собственных векторов.
   Попробуйте проследить за каждым шагом равенства слева направо в урав-
нении 13.3.

  Уравнение 13.3. Доказательство ортогональности собственных векторов
  для симметричных мат­риц

  λ1v1Tv2 = (Av1)Tv2 = v1TATv2 = v1Tλ2v2 = λ2v1Tv2.

  Члены в середине – это просто преобразования; обратите внимание на
первый и последний члены. Они переписываются в уравнении 13.4, а затем
вычитаются для обнуления.

  Уравнение 13.4. Продолжение доказательства ортогональности
  собственных векторов…

  λ1v1Tv2 = λ2v1Tv2;
  λ1v1Tv2 = λ2v1Tv2 = 0.

   Оба члена содержат точечное произведение v1Tv2, которое можно исклю-
чить. Это подводит нас к заключительной части доказательства, показанной
в уравнении 13.5.

  Уравнение 13.5. Доказательство ортогональности собственных векторов,
  часть 3

  (λ1 – λ2)v1Tv2 = 0.

  Приведенное выше последнее уравнение говорит о том, что два числа
умножаются, чтобы получить 0, а это означает, что одно или оба этих числа
должны быть равны нулю. Разность λ1 – λ2 не может быть равна нулю, по-
скольку мы исходили из допущения, что они различаются. Следовательно,
240  Собственное разложение

v1Tv2 должно быть равно нулю, а это означает, что два собственных вектора
ортогональны. Вернитесь к уравнениям, чтобы убедиться, что это доказа-
тельство не работает для несимметричных мат­риц, когда AT ¹ A. Таким
образом, собственные векторы несимметричной мат­ри­цы не ограничены
быть ортогональными (они будут линейно независимыми для всех различа-
ющихся собственных чисел, но я опущу это обсуждение и доказательство).


Действительно-значные собственные числа
Второе особое свойство симметричных мат­риц состоит в том, что они имеют
действительно-значные собственные числа (и, следовательно, действитель-
но-значные собственные векторы).
  Давайте я начну с демонстрации того, что мат­ри­цы – даже со всеми дей-
ствительно-значными элементами – могут иметь комплексно-значные соб-
ственные числа:
A = np.array([[-3, -3, 0],
              [ 3, -2, 3],
              [ 0, 1, 2]])

# ее собственное разложение
L,V = np.linalg.eig(A)
L.reshape(-1,1) # напечатать в виде вектора-столбца

>> array([[-2.744739 +2.85172624j],
          [-2.744739 -2.85172624j],
          [ 2.489478 +0.j        ]])

(Будьте осторожны с интерпретацией этого массива NumPy; это не мат­ри­
ца 3×2; это вектор-столбец 3×1, содержащий комплексные числа. Обратите
внимание на j и отсутствие запятой между числами.)
  3×3-мат­ри­ца A имеет два комплексно-значных собственных числа и одно
действительно-значное собственное число. Сопряженные с комплексно-
значными собственными числами собственные векторы сами будут комп­
лексно-­значными. В этой конкретной мат­ри­це нет ничего особенного; я бук-
вально сгенерировал ее из случайных целых чисел от −3 до +3. Интересно,
что комплексно-значные решения выводятся сопряженными парами. Это
означает, что если существует λi = a + ib, то существует и λk = a – ib. Соот-
ветствующие им собственные векторы также являются комплексными со-
пряженными парами.
  Не буду вдаваться в подробности комплексно-значных решений, кроме
как покажу, что комплексные решения задачи собственного разложения эле-
ментарны1.
  Симметричные мат­ри­цы гарантированно имеют действительно-значные
собственные числа, следовательно, также действительно-значные собствен-

1
    Под «элементарными» я подразумеваю математически ожидаемые; интерпрета-
    ция сложных решений в собственном разложении далеко не элементарна.
                                      Собственное разложение сингулярных матриц  241

ные векторы. Давайте я начну с того, что видоизменю приведенный выше
пример, чтобы сделать мат­ри­цу симметричной:
A = np.array([[-3, -3, 0],
              [-3, -2, 1],
              [ 0, 1, 2]])

# ее собственное разложение
L,V = np.linalg.eig(A)
L.reshape(-1,1) # напечатать в виде вектора-столбца

>> array([[-5.59707146],
          [ 0.22606174],
          [ 2.37100972]])

  Это только один конкретный пример; может, нам тут повезло? Рекомен-
дую сделать паузу и обследовать этот вопрос самостоятельно в онлайновом
исходном коде; вы можете создать случайную симметричную мат­ри­цу (пу-
тем создания случайной мат­ри­цы и собственного разложения ATA) любого
размера, чтобы подтвердить, что собственные числа являются действитель-
но-значными.
  Получение гарантированных действительно-значных собственных чисел
из симметричных мат­риц – это настоящая удача, потому что с комплексны-
ми числами зачастую сложно работать. Многие используемые в науке о дан-
ных мат­ри­цы являются симметричными, поэтому если в своих приложениях
в этой области вы видите комплексные собственные числа, то, возможно,
проблема связана с исходным кодом или данными.

      Эффективное применение симметрии
      Если вы знаете, что работаете с симметричной матрицей, то вместо функции np.linalg.
      eig можете использовать функцию np.linalg.eigh (или вместо функции SciPy eig –
      функцию eigh). Символ h означает «эрмитову мат­ри­цу»1, то есть комплексную версию
      симметричной мат­ри­цы. Функция eigh в целом работает быстрее и является более чис-
      ленно стабильной, чем функция eig, но оперирует только симметричными мат­ри­ца­ми.




Собственное разложение сингулярных матриц
Я вставил этот раздел сюда, потому что обнаружил, что у студентов часто
возникает идея, что сингулярные мат­ри­цы невозможно разложить проце-
дурой собственного разложения или что собственные векторы сингулярной
мат­ри­цы должны быть какими-то необычными.
  Эта идея совершенно неверна. Применять собственное разложение к син-
гулярным мат­ри­цам – это совершенно нормально. Вот краткий пример:
# сингулярная мат­ри­ца
A = np.array([[1,4,7],

1
    Англ. Hermitian. – Прим. перев.
242  Собственное разложение

              [2,5,8],
              [3,6,9]])

# ее собственное разложение
L,V = np.linalg.eig(A)

  Распечатка ранга, собственных чисел и собственных векторов этой мат­
ри­цы приведена ниже:
print( f'Ранг = {np.linalg.matrix_rank(A)}\n' )
print('Собственные числа: '), print(L.round(2)), print(' ')
print('Собственные векторы:'), print(V.round(2))

>> Ранг = 2

Собственные числа:
[16.12 -1.12 -0. ]

Собственные векторы:
[[-0.46 -0.88 0.41]
 [-0.57 -0.24 -0.82]
 [-0.68 0.4 0.41]]

   Эта мат­ри­ца ранга 2 имеет одно нуль-значное собственное число с ненуле-
вым собственным вектором. Вы можете использовать онлайновый исходный
код, чтобы обследовать собственное разложение других рангово-понижен-
ных случайных мат­риц.
   Существует одно особое свойство собственного разложения сингулярных
мат­риц, при котором по крайней мере одно собственное число гарантиро-
ванно равно нулю. Это не означает, что количество ненулевых собственных
чисел равно рангу мат­ри­цы – это верно для сингулярных чисел (скаляров из
SVD), но не для собственных чисел. Но если мат­ри­ца – сингулярная, то хотя
бы одно собственное число равно нулю.
   Верно и обратное: каждая полноранговая мат­ри­ц а имеет ноль нуль-
значных собственных чисел.
   Одно из объяснений причины, по которой это происходит, заключается
в том, что сингулярная мат­ри­ца уже имеет нетривиальное нуль-пространство,
а это означает, что λ = 0 обеспечивает нетривиальное решение уравнения
(A – λI)v = 0. Это видно в приведенном выше примере мат­ри­цы: ассоцииро-
ванный с λ = 0 собственный вектор – это нормализованный вектор [1 −2 1],
то есть линейно-взвешенная комбинация столбцов (или строк), которая дает
вектор нулей.
   Основные выводы, которые следует вынести из этого раздела, таковы:
   1)	собственное разложение допустимо для рангово-пониженных мат­
       риц и
   2)	наличие по меньшей мере одного нуль-значного собственного числа
       указывает на рангово-пониженную мат­ри­цу.
                   Квадратичная форма, определенность и собственные числа  243


Квадратичная форма, определенность
и собственные числа
Следует признать, что термины квадратичная форма и определенность вы-
глядят пугающе. Но не беспокойтесь – оба этих термина просты и открыва-
ют доступ к продвинутой линейной алгебре и таким ее применениям, как
анализ главных компонент и симуляции методом Монте-Карло. И что еще
лучше: интеграция исходного кода Python в ваше обучение даст вам огром-
ное преимущество в изучении этих понятий по сравнению с традиционными
учебниками по линейной алгебре.


Квадратичная форма матрицы
Рассмотрим следующее ниже выражение:

  vTAw = α.

  Другими словами, мы пред- и постпозиционно умножаем квадратную мат­
ри­цу на один и тот же вектор w и получаем скаляр. (Обратите внимание, что
это умножение допустимо только для квадратных мат­риц.)
  Это называется квадратичной формой мат­ри­цы A.
  Какую мат­ри­цу и какой вектор мы используем? Идея квадратичной фор-
мы заключается в использовании одной конкретной мат­ри­цы и набора всех
возможных векторов (соответствующего размера). Важный вопрос касается
знаков α всех возможных векторов. Давайте посмотрим на пример:




   Для этой конкретной мат­ри­цы не существует возможной комбинации x
и y, которая может давать отрицательный ответ, потому что квадраты членов
(2x2 и 3y2) всегда будут перевешивать перекрестный член (4xy), даже если x
или y – отрицательные. Кроме того, α может быть неположительным только
при x = y = 0.
   Это нетривиальный результат квадратичной формы. Например, следую-
щая ниже мат­ри­ца может иметь положительное или отрицательное α в за-
висимости от значений x и y:




  Вы можете подтвердить, что задание [x y] равным [−1 1] дает отрицатель-
ный результат квадратичной формы, а [−1 −1] дает положительный результат.
244  Собственное разложение

   Как вообще можно узнать, какой скаляр (положительный, отрицательный
либо нулевой) будет произведен квадратичной формой для всех возможных
векторов? Ключ происходит из учитывания того, что полноранговая мат­ри­ца
собственных векторов охватывает все ℝM, и, следовательно, каждый вектор
из ℝM может быть выражен как некоторая взвешенная линейная комбинация
собственных векторов1. Затем мы начинаем с уравнения собственного числа
и умножаем его слева на собственный вектор, чтобы вернуться к квадратич-
ной форме:

    Av = λv;
    vTAv = λvTv;
    vTAv = λ||v||2.

  Последнее уравнение является ключевым. Обратите внимание, что ||vTv||2 –
строго положительный (модули векторов не могут быть отрицательными,
и мы игнорируем вектор нулей), а это означает, что знак правой части урав-
нения полностью определяется собственным числом λ.
  В этом уравнении используется только одно собственное число и его соб-
ственный вектор, но нам нужно знать о любом возможном векторе. Ключе-
вой для понимания момент состоит в том, чтобы учесть, что если уравнение
допустимо для каждой пары «собственный вектор – собственное число», то
оно допустимо для любой комбинации пар «собственный вектор – собствен-
ное число». Например:

                   v1TAv1 = λ1||v1||2;
                   v2TAv2 = λ2||v2||2;
    (v1 + v2)TA(v1 + v2) = (λ1 + λ2)||v1 + v2||2;
                      uTAu = ζ||u||2.

  Другими словами, мы можем задать любой вектор u как некоторую ли-
нейную комбинацию собственных векторов и некоторый скаляр ζ как ту же
самую линейную комбинацию собственных чисел. Во всяком случае, это не
меняет того принципа, что знак правой части – а значит, и знак квадратич-
ной формы – определяется знаком собственных чисел.
  Теперь давайте подумаем об этих уравнениях в условиях разных допуще-
ний о знаках собственных чисел λ.

Все собственные числа положительны
  Правая часть уравнения – всегда положительная, а это означает, что vTAv
  всегда положителен для любого вектора v.

Собственные числа положительны либо равны нулю
  vTAv неотрицателен и будет равен нулю при λ = 0 (что происходит, когда
  мат­ри­ца является сингулярной).

1
    Ради краткости я здесь опускаю некоторые тонкости относительно редких случаев,
    когда мат­ри­ца собственных векторов не охватывает всемерное подпространство.
                    Квадратичная форма, определенность и собственные числа  245

Собственные числа отрицательны либо равны нулю
  Результат квадратичной формы будет нулевым или отрицательным.

Собственные числа – отрицательные
  Результат квадратичной формы будет отрицательным для всех векторов.


Определенность
Определенность – это характеристика квадратной мат­ри­цы и определяется
знаками собственных чисел мат­ри­цы, которые равносильны знакам резуль-
татов квадратичной формы. Определенность влияет на обратимость мат­ри­
цы, а также на продвинутые методы анализа данных, такие как обобщенное
собственное разложение (используемое в многопеременных линейных клас-
сификаторах и обработке сигналов).
  Как показано в табл. 13.1, существует пять категорий определенности;
знаки + и − указывают знаки собственных чисел.

Таблица 13.1. Категории определенности
Категория                         Квадратичная       Собственные    Обратимая
                                  форма              числа
Положительно определенная         Положительная      +              Да
Положительно полуопределенная     Неотрицательная    +и0            Нет
Неопределенная                    Положительная      +и−            Зависит
                                  и отрицательная
Отрицательно полуопределенная     Неположительная    −и0            Нет
Отрицательно определенная         Отрицательная      −              Да

  «Зависит» в таблице означает, что мат­ри­ца может быть обратимой или
сингулярной в зависимости от чисел мат­ри­цы, а не от категории определен-
ности.


ATA является положительной (полу)определенной
Любая мат­ри­ца, которая может быть выражена как произведение мат­ри­цы
и ее транспонированной версии (то есть S = ATA), гарантированно будет
положительно определенной либо положительно полуопределенной. Ком-
бинация этих двух категорий часто записывается как «положительно (полу)
определенная».
  Все мат­ри­цы ковариаций данных являются положительно (полу)опреде-
ленными, поскольку они определяются как произведение мат­ри­цы данных
на ее транспонированую версию. Это означает, что все мат­ри­цы ковариаций
имеют неотрицательные собственные числа. Все собственные числа будут
положительными, когда мат­ри­ца данных является полноранговой (имеет
полный столбцовый ранг, если данные хранятся в виде наблюдений по при-
246  Собственное разложение

знакам), и будет иметься по меньшей мере одно нуль-значное собственное
число, если мат­ри­ца данных является рангово-пониженной.
  Доказательство того, что S положительно (полу)определена, основано на
выписывании его квадратичной формы и применении нескольких алгебраи­
ческих манипуляций. (Обратите внимание, что для перехода от первого урав-
нения ко второму требуется просто переставить скобки; в линейной алгебре
такое «доказательство с помощью скобок» является общепринятым.)

    wTSw = wT(ATA)w
           = (wTAT)Aw
           = (Aw)T(Aw)
           = ||Aw||2.

  Дело в том, что квадратичная форма ATA равна квадрату модуля мат­ри­
цы, умноженной на вектор. Модули не могут быть отрицательными и могут
быть равны нулю только тогда, когда вектор равен нулю. А если Aw = 0 для
нетривиального w, то A является сингулярной.
  Следует иметь в виду, что хотя все мат­ри­цы ATA симметричны, не все
симметричные мат­ри­цы могут быть выражены как ATA. Другими словами,
матричная симметрия сама по себе не гарантирует положительной (полу)
определенности, потому что не все симметричные мат­ри­цы могут быть вы-
ражены как произведение мат­ри­цы и ее транспонированной версии.


            Кого вообще волнуют квадратичная форма и определенность?
    Положительная определенность востребована в науке о данных, потому что некоторые
    линейно-алгебраические операции применимы только к таким «хорошо обеспечен-
    ным» мат­ри­цам, включая разложение Холецкого, используемое для создания коррели-
    рованных наборов данных в симуляциях Монте-Карло. Положительно определенные
    мат­ри­цы также важны для задач оптимизации (например, градиентного спуска), пото-
    му что можно гарантированно находить минимум. В своем бесконечном стремлении
    улучшить свое мастерство в науке о данных вы можете столкнуться с техническими
    документами, в которых используется аббревиатура SPD1: симметричная положительно
    определенная.




Обобщенное собственное разложение
Обратите внимание, что следующее ниже уравнение совпадает с фундамен-
тальным уравнением собственного числа:

    Av = λIv.


1
    Англ. Symmetric Positive Definite. – Прим. перев.
                                             Обобщенное собственное разложение  247

  Это очевидно, поскольку Iv = v. Обобщенное собственное разложение
предусматривает замену единичной мат­ри­цы еще одной матрицей (не еди-
ничной либо матрицей нулей):

    Av = λBv.

  Обобщенное собственное разложение также называется одновременной
диагонализацией двух мат­риц. Результирующая пара (λ, v) не является соб-
ственным числом/вектором только мат­ри­цы A или только мат­ри­цы B. Вместо
этого две мат­ри­цы имеют общие пары «собственное число/вектор».
  В концептуальном плане обобщенное собственное разложение может трак-
товаться как «обычное» собственное разложение мат­ри­цы произведения:

    C = AB–1;
    Cv = λv.

  Это только концептуально; на практике же обобщенное собственное раз-
ложение не требует от мат­ри­цы B быть обратимой.
  Здесь не тот случай, когда любые две мат­ри­цы могут быть диагонализиро-
ваны одновременно. Но эта диагонализация возможна, если B положительно
(полу)определена.
  В библиотеке NumPy обобщенное собственное разложение не вычисляет-
ся, но в SciPy такая возможность есть. Если вы знаете, что две мат­ри­цы сим-
метричны, то можете применить функцию eigh, которая является численно
более стабильной:
#   создать коррелированные мат­ри­цы
A   = np.random.randn(4,4)
A   = A@A.T
B   = np.random.randn(4,4)
B   = B@B.T + A/10

# обобщенное собственное разложение
from scipy.linalg import eigh
evals,evecs = eigh(A,B)

  Помните о порядке входимых аргументов: второй аргумент является кон-
цептуально инвертированным.
  В науке о данных обобщенное собственное разложение используется
в анализе классификации. В частности, линейный дискриминантный ана-
лиз Фишера основан на обобщенном собственном разложении двух мат­риц
ковариации данных. В главе 15 вы увидите соответствующий пример.


                  Несметное число тонкостей собственного разложения
    О свойствах собственного разложения можно было бы сказать гораздо больше. Вот не-
    сколько примеров: сумма собственных чисел равна следу мат­ри­цы, а произведение соб-
    ственных чисел равно определителю; не все квадратные мат­ри­цы можно диагонализо-
    вать; некоторые мат­ри­цы имеют повторяющиеся собственные числа, что влияет на их
248  Собственное разложение

 собственные векторы; комплексные собственные числа действительно-значных мат­риц
 находятся внутри круга на комплексной плоскости. Математические познания в области
 собственных чисел имеют глубокие корни, но эта глава дает все базовые знания, необхо-
 димые для работы с собственным разложением в приложениях.




Резюме
Вот это была глава так глава! Напомню ее ключевые моменты.
    Собственное разложение идентифицирует M пар скаляр/вектор мат­ри­
      цы M×M. Эти пары «собственное число / собственный вектор» отражают
      особые направления в мат­ри­це и имеют громадное число применений
      в науке о данных (обычно это анализ главных компонент), а также
      в геометрии, физике, вычислительной биологии и массе других тех-
      нических дисциплин.
    Собственные числа отыскиваются путем принятия допущения о том,
      что мат­ри­ца, сдвинутая на неизвестный скаляр λ, является сингуляр-
      ной, посредством приравнивания ее определителя к нулю (именуемого
      характеристическим многочленом) и вычисления чисел λ.
    Собственные векторы отыскиваются путем отыскания базисного век-
      тора для нуль-пространства λ-сдвинутой мат­ри­цы.
    Диагонализация мат­ри­цы означает представление мат­ри­цы в виде
      VΛV –1, где V – это мат­ри­ца с собственными векторами в столбцах,
      а Λ – диагональная мат­ри­ца с собственными числами в диагональных
      элементах.
    Симметричные мат­ри­цы обладают рядом особых свойств в собствен-
      ном разложении; наиболее важным для науки о данных является то,
      что все собственные векторы попарно ортогональны. Это означает,
      что мат­ри­ца собственных векторов является ортогональной матрицей
      (когда собственные векторы единично-нормализованы), что, в свою
      очередь, означает, что обратная мат­ри­ца мат­ри­цы собственных век-
      торов является ее транспонированной версией.
    Определенность мат­ри­цы относится к знакам ее собственных чисел.
      В науке о данных наиболее релевантными категориями являются по-
      ложительно (полу)определенные, что означает, что все собственные
      числа либо неотрицательны, либо положительны.
    Матрица, умноженная на ее транспонированную версию, всегда поло-
      жительно (полу)определена, что означает, что все мат­ри­цы ковариа­
      ций имеют неотрицательные собственные числа.
    Учение о собственном разложении носит насыщенный и подробный
      характер, и в этой области исследований было открыто много захва-
      тывающих тонкостей, частных случаев и применений. Надеюсь, что
      обзор этой темы в данной главе обеспечит прочную основу для ваших
      потребностей как исследователя данных и, возможно, вдохновит вас
      узнать­еще больше о фантастической красоте собственного разложения.
                                           Упражнения по программированию  249


Упражнения по программированию
  Упражнение 13.1
  Интересно, что собственные векторы A–1 сопадают с собственными век-
торами A, а собственные числа равны λ–1. Докажите, что это так, выписав
собственное разложение A и A–1. Затем проиллюстрируйте это, используя
случайную полноранговую симметричную мат­ри­цу 5×5.

  Упражнение 13.2
  Воссоздайте левую панель рис. 13.1, но вместо столбцов используйте стро-
ки мат­ри­цы V. Конечно же, вы знаете, что это ошибка программирования,
но результаты будут поучительными: тест на геометрию не будет пройден,
поскольку мат­ри­ца, умноженная на ее собственный вектор, только растяги-
вается.

  Упражнение 13.3
  Цель этого упражнения – продемонстрировать, что собственные числа не-
разрывно сопряжены со своими собственными векторами. Диагонализируй-
те симметричную мат­ри­цу случайных целых чисел1, созданную с помощью
аддитивного метода (см. упражнение 5.9), но переупорядочите собственные
числа в случайном порядке (назовем эту мат­ри­цу Λ
                                                 ) без переупорядочивания
собственных векторов.
  Сначала продемонстрируйте, что вы можете реконструировать изначаль-
ную мат­ри­цу как VΛV–1. Точность реконструкции можно вычислить как рас-
стояние Фробениуса между изначальной и реконструированной мат­ри­ца­
ми. Далее, попытайтесь реконструировать мат­ри­цу, используя Λ. Насколько
близка реконструированная мат­ри­ца к изначальной? Что произойдет, если
вместо случайного переупорядочивания взаимно поменять местами только
два самых больших собственных числа? Как насчет двух наименьших соб-
ственных чисел?
  Наконец, создайте гистограмму, показывающую расстояния Фробениуса
до изначальной мат­ри­цы для разных вариантов взаимообмена (рис. 13.3).
(Разумеется, из-за случайных мат­риц – и, следовательно, случайных соб-
ственных чисел – ваш график не будет выглядеть точно так же, как мой.)

  Упражнение 13.4
  Одно интересное свойство случайных мат­риц состоит в том, что их ком-
плексно-значные собственные числа распределены по кругу с радиусом, про-
порциональным размеру мат­ри­цы. В целях демонстрации этого свойства
вычислите 123 случайные мат­ри­цы 42×42, извлеките их собственные числа,
разделите на квадратный корень из размера мат­ри­цы (42) и нанесите соб-
ственные числа на комплексную плоскость, как показано на рис. 13.4.


1
    В упражнениях я очень часто использую симметричные мат­ри­цы; это связано
    с тем, что они имеют действительно-значные собственные числа, но это не меняет
    ни принципа, ни математики, а просто облегчает визуальный осмотр решений.
250  Собственное разложение

                                                                            Точность реконструкции



           Расстояние Фробениуса до изначальной мат­ри­цы




                                                            Ни одного         Все           Два самых        Два самых
                                                                                             больших           малых
                                                                        Тип взаимообмена собственных чисел
                                                            Рис. 13.3  Результаты упражнения 13.3
           Мнимая часть




                                                                              Действительная часть
                                                            Рис. 13.4  Результаты упражнения 13.4

  Упражнение 13.5
  Это упражнение поможет вам получше разобраться в том, что собственный
вектор является базисом нуль-пространства сдвинутой собственным числом
мат­ри­цы, а также выявит риски числовых ошибок прецизионности. Приме-
ните процедуру собственного разложения к случайной симметричной мат­
                                       Упражнения по программированию  251

ри­це 3×3. Затем по каждому собственному числу примените функцию scipy.
linalg.null_space(), чтобы найти базисный вектор нуль-пространства каждой
сдвинутой мат­ри­цы. Совпадают ли эти векторы с собственными векторами?
Обратите внимание, что вам, возможно, понадобится принять во внимание
нормы и знаковые неопределенности собственных векторов.
  При выполнении исходного кода несколько раз на разных случайных мат­
рицах вы, скорее всего, будете получать ошибки Python. Ошибка возникает
из-за пустого нуль-пространства λ-сдвинутой мат­ри­цы. Указанная ошибка
при обследовании возникает из-за того, что сдвинутая мат­ри­ца имеет пол-
ный ранг. (Не верьте мне на слово; подтвердите это сами!) Этого не должно
происходить, что еще раз подчеркивает, что
  1)	конечно-прецизионная математика на компьютерах не всегда соот-
      ветствует математике на «классной доске» и
  2)	вам следует использовать конкретно ориентированные и более числен-
      но стабильные функции, вместо того чтобы пытаться делать прямое
      переложение формул в исходный код.

  Упражнение 13.6
  В данном упражнении я собираюсь научить вас третьему методу создания
случайных симметричных мат­риц1. Начните с создания диагональной мат­
ри­цы 4×4, по диагонали которой расположены положительные числа (напри-
мер, это могут быть числа 1, 2, 3, 4). Затем создайте 4×4-мат­ри­цу Q из QR-
разложения мат­ри­цы случайных чисел. Используйте эти мат­ри­цы в качестве
собственных чисел и собственных векторов и умножьте их соответствующим
образом, чтобы собрать мат­ри­цу. Убедитесь, что собранная мат­ри­ца сим-
метрична и что ее собственные числа равны собственным числам, которые
указали вы.

  Упражнение 13.7
  Вернемся к упражнению 12.4. Повторите это упражнение, но вместо квад­
рата фробениусовой нормы расчетной мат­ри­цы используйте среднее зна-
чение собственных чисел (это называется усадочной регуляризацией). Как
полученное число соотносится с числом из главы 12?

   Упражнение 13.8
   Это и следующее упражнение тесно связаны между собой. Мы создадим
суррогатные данные с заданной матрицей корреляций (в этом упражнении),
а затем удалим корреляцию (в следующем упражнении). Формула для созда-
ния данных с заданной структурой корреляции такова:

    Y = VΛ1/2X,

где V и Λ – это собственные векторы и собственные числа мат­ри­цы корре-
ляций, а X – N×T-мат­ри­ца некоррелированных случайных чисел (N каналов
и T временных точек).

1
    Первые два были мультипликативным и аддитивным методами.
252  Собственное разложение

  Примените эту формулу, чтобы создать 3×10 000-мат­ри­цу Y данных со
следующей ниже структурой корреляции:




  Затем вычислите эмпирическую корреляционную мат­ри­цу мат­ри­цы X.
Она не будет точно равна R, потому что мы отбираем образцы в случайном
порядке из конечного набора данных. Но она должна быть достаточно близ-
кой (например, в пределах 0.01).

  Упражнение 13.9
  Теперь давайте удалим эти навязанные корреляции с помощью отбелива-
ния. Термин отбеливание используется в обработке сигналов и изображений
и обозначает устранение корреляций. Многопеременный временной ряд
можно отбелить, реализовав следующую ниже формулу:

     = YTVΛ–1/2.
    Y

  Примените эту формулу к мат­ри­це данных из предыдущего упражнения
и убедитесь, что мат­ри­ца корреляций является единичной матрицей (опять
же, с некоторым допуском на случайный отбор данных).

  Упражнение 13.10
  В обобщенном собственном разложении собственные векторы не ортого-
нальны, даже когда обе мат­ри­цы симметричны. Подтвердите на Python, что
V–1 ¹ VT. Это происходит потому, что хотя и A, и B симметричны, C = AB не
симметрична1.
  Однако собственные векторы ортогональны относительно B, и, стало быть,
VTBV = I. Подтвердите эти свойства, выполнив обобщенное собственное раз-
ложение двух симметричных мат­риц и получив рис. 13.5.

  Упражнение 13.11
  Давайте обследуем шкалирование собственных векторов. Начните с соз-
дания 4×4-мат­ри­цы случайных целых чисел, извлеченных в интервале меж-
ду −14 и +14. Диагонализируйте мат­ри­цу и эмпирически подтвердите, что
A = VΛV–1. Подтвердите, что евклидова норма каждого собственного вектора
равна 1. Обратите внимание, что квадрат комплексного числа вычисляется
как это число, умноженное на его комплексно-сопряженное число (подсказ-
ка: используйте функцию np.conj()).
  Затем умножьте мат­ри­цу собственных векторов на любой ненулевой ска-
ляр. Я использовал π без особой веской причины, кроме того что его было
интересно набирать на клавиатуре. Влияет ли этот скаляр на точность ре-

1
    Причина, по которой произведение двух симметричных мат­риц не является сим-
    метричным, совпадает с причиной, по которой мат­ри­ца R из QR-разложения имеет
    нули на нижней диагонали.
                                        Упражнения по программированию  253

конструированной мат­ри­цы и/или норм собственных векторов? Почему да
или почему нет?
  Наконец, повторите эту процедуру, но используя симметричную мат­ри­цу,
и замените V–1на VT. Изменят ли эти изменения умозаключение?




                   Рис. 13.5  Результаты упражнения 13.10
Глава           14
                                          Сингулярное
                                           разложение

Предыдущая глава была по-настоящему насыщенной! Я изо всех сил старался
сделать ее понятной и строгой, не слишком увязая в деталях, которые имеют
меньшую релевантность для науки о данных.
  К счастью, бóльшая часть того, что вы узнали о собственном разложении,
применима и к сингулярному разложению (SVD)1. Это означает, что данная
глава будет проще и короче.
  Сингулярное разложение предназначено для разложения мат­ри­цы на про-
изведение трех мат­риц, именуемых левыми сингулярными векторами (U),
сингулярными числами (Σ) и правыми сингулярными векторами (V):

    A = UΣVT.

  Это разложение, должно быть, выглядит похоже на собственное разложе-
ние. На самом деле сингулярное разложение можно трактовать как обобще-
ние собственного разложения на неквадратные мат­ри­цы – либо трактовать
собственное разложение как частный случай собственного разложения для
квадратных мат­риц2.
  Сингулярные числа сравнимы с собственными числами, а мат­ри­цы сингу-
лярных векторов сравнимы с собственными векторами (эти два набора вели-
чин совпадают при некоторых обстоятельствах, которые я объясню позже).



Общая картина сингулярного разложения
Сначала я познакомлю вас с идеей и интерпретацией мат­риц, а позже в этой
главе объясню, как вычислять сингулярное разложение.


1
    Англ. Singular Value Decomposition. – Прим. перев.
2
    Сингулярное разложение – это не то же самое, что собственное разложение для
    всех квадратных мат­риц; подробнее об этом позже.
                                   Общая картина сингулярного разложения  255

  На рис. 14.1 показан общий вид сингулярного разложения.




               Рис. 14.1  Общая картина сингулярного разложения

  На этой диаграмме видны многие важные признаки сингулярного разло-
жения; эти признаки будут рассмотрены подробнее в данной главе, но если
их сгруппировать, то получится вот такой список:
    и U, и V – это квадратные мат­ри­цы, даже если A не является квад­
      ратной;
    мат­ри­цы сингулярных векторов U и V ортогональны, а значит, UTU = I
      и VTV = I. Напомним, что это означает, что каждый столбец ортогона-
      лен другому столбцу и любое подмножество столбцов ортогонально
      любому другому (непересекающемуся) подмножеству столбцов;
    первые r столбцов мат­ри­цы U обеспечивают ортогональные базисные
      векторы для столбцового пространства мат­ри­цы A, тогда как осталь-
      ные столбцы предоставляют ортогональные базисные векторы для
      левого нуль-пространства (если только r не равно M, в коем случае
      мат­ри­ца имеет полный столбцовый ранг, а левое нуль-пространство
      является пустым);
    первые r строк мат­ри­цы VT (которые являются столбцами мат­ри­цы V)
      предоставляют ортогональные базисные векторы для строчного про-
      странства, тогда как остальные строки предоставляют ортогональные
      базисные векторы для нуль-пространства;
    мат­ри­ца сингулярных чисел – это диагональная мат­ри­ца того же раз-
      мера, что и A. Сингулярные числа всегда сортируются от наибольшего
      (вверху слева) к наименьшему (внизу справа);
    все сингулярные числа неотрицательны и действительно-значные.
      Они не могут быть ни комплексными, ни отрицательными, даже если
      мат­ри­ца содержит комплексно-значные числа;
    количество ненулевых сингулярных чисел равно рангу мат­ри­цы.
  Возможно, самое удивительное в сингулярном разложении то, что оно
показывает все четыре подпространства мат­р и­ц ы: столбцовое простран-
ство и левое пустое пространство охватываются первыми r и последними
от M – r до M столбцами мат­ри­цы U, тогда как строчное пространство и пус­
тое пространство охватываются первыми r и последними N – r до N стро-
ками мат­р и­ц ы V T. В случае прямоугольной мат­р и­ц ы если r = M, то левое
256  Сингулярное разложение

нуль-пространство является пустым, а если r = N, то нуль-пространство
является пустым.


Сингулярные числа и ранг матрицы
Ранг мат­ри­цы определяется как количество ненулевых сингулярных чисел.
Причина пристекает из предыдущего изложения, что столцовое простран-
ство и строчное пространство мат­ри­цы определяются как левый и правый
сингулярные векторы, которые шкалируются соответствующими им сингу-
лярными числами, чтобы иметь некий «объем» в матричном пространстве,
тогда как левое и правое нуль-пространства определяются как левый и пра-
вый сингулярные векторы, шкалированные в нули. Таким образом, размер-
ность столбцового и строчного пространств определяется количеством не-
нулевых сингулярных чисел.
  Собственно говоря, можно заглянуть в функцию NumPy np.linalg.matrix_
rank, чтобы увидеть, как Python вычисляет ранг мат­ри­цы (я немного отре-
дактировал исходный код, чтобы сосредоточиться на ключевых понятиях):
S = svd(M, compute_uv=False) # вернуть только сингулярные числа
tol = S.max() * max(M.shape[-2:]) * finfo(S.dtype).eps
return count_nonzero(S > tol)

  Возвращаемое значение – это количество сингулярных чисел, которые
превышают пороговое значение tol. Что такое tol? Это уровень допуска, учи-
тывающий возможные ошибки округления. Он определяется как машинная
прецизионность для этого типа данных (eps), шкалированная по наибольше-
му сингулярному числу и размеру мат­ри­цы.
  Таким образом, мы еще раз видим разницу между «математикой на класс-
ной доске» и прецизионной математикой, реализованной на компьютерах:
ранг мат­ри­цы на самом деле вычисляется не как количество ненулевых син-
гулярных чисел, а как количество сингулярных чисел, которые больше не-
которого малого числа. Существует риск того, что малые, но по-настоящему
ненулевые сингулярные числа будут проигнорированы, но это перевеши-
вает риск неправильного завышения ранга мат­ри­цы, когда по-настоящему
нуль-значные сингулярные числа оказываются ненулевыми из-за ошибок
прецизионности.



Сингулярное разложение на Python
На языке Python сингулярное разложение вычисляется довольно просто:
U, s, Vt = np.linalg.svd(A)

  Следует помнить о двух особенностях функции svd в библиотеке NumPy.
Во-первых, сингулярные числа возвращаются в виде вектора, а не в виде мат­
                        Сингулярное разложение и одноранговые «слои» матрицы  257

ри­цы того же размера, что и A. Это означает, что вам нужен дополнительный
исходный код получения мат­ри­цы Σ:
S = np.zeros(np.shape(A))
np.fill_diagonal(S, s)

   Сначала вы, возможно, подумаете об использовании np.diag(s), но она
создает правильную мат­ри­цу сингулярных чисел только для квадратной мат­
ри­цы A. Поэтому я сначала создаю мат­ри­цу нулей правильного размера,
а затем заполняю диагональ сингулярными числами.
   Вторая особенность заключается в том, что NumPy возвращает мат­ри­
цу VT, а не V. Возможно, это будет дезориентировать читателей, знакомых
с MATLAB, потому что функция MATLAB svd возвращает мат­ри­цу V. Подсказ-
ка находится в документационном литерале, описывающем мат­ри­цу vh, где
буква h обозначает эрмитов, то есть имя симметричной комплексно-значной
мат­ри­цы.
   На рис. 14.2 показаны результаты на выходе из функции svd (в которых
сингулярные числа конвертированы в мат­ри­цу).

                                     U                                          V
                            (левые сингулярные                         (правые сингулярные
           A                     векторы)                                    векторы)
        Матрица                                  (сингулярные числа)




        Рис. 14.2  Общая картина сингулярного разложения образцовой мат­ри­цы




Сингулярное разложение и одноранговые
«слои» матрицы
Первое уравнение, которое я показал в предыдущей главе, было векторно-
скалярной версией уравнения собственного числа (Av = λv). Я открыл эту
главу с матричного уравнения сингулярного разложения (A = UΣVT); как
это уравнение выглядит для одного вектора? Его можно написать двумя
разными способами, которые подчеркивают разные признаки сингулярного
разложения:

  Av = uσ;
  uTA = σvT.
258  Сингулярное разложение

   Эти уравнения чем-то похожи на уравнение собственного числа, за исклю-
чением того, что вместо одного вектора используются два. И следовательно,
интерпретации немного более тонкие: в общем случае эти уравнения говорят
о том, что влияние мат­ри­цы на один вектор такое же, как влияние скаляра
на другой вектор.
   Обратите внимание, что первое уравнение означает, что u находится
в столбцовом пространстве мат­ри­цы A, а v обеспечивает веса для комби-
нирования столбцов. То же самое относится и ко второму уравнению, но v
находится в строчном пространстве мат­ри­цы A, а u обеспечивает веса.
   Но это не то, на чем я хотел бы сосредоточиться в данном разделе; я хочу
рассмотреть, что происходит при умножении одного левого сингулярного
вектора на один правый сингулярный вектор. Поскольку сингулярные век-
торы парно соединены с одним и тем же сингулярным числом, нам нужно
умножить i-й левый сингулярный вектор на i-е сингулярное число на i-й пра-
вый сингулярный вектор.
   Обратите внимание на ориентацию в этом умножении вектора на вектор:
столбец слева, строка справа (рис. 14.3). Это означает, что результатом будет
внешнее произведение того же размера, что и изначальная мат­ри­ца. Кроме
того, это внешнее произведение представляет собой одноранговую мат­ри­цу,
норма которой определяется сингулярным числом (поскольку сингулярные
векторы имеют единичную длину):

    u1σ1v1T = A1.




                Рис. 14.3  Внешнее произведение сингулярных векторов
                               создает матричный «слой»

  Подстрочная 1 в уравнении указывает на использование первых сингуляр-
ных векторов и первого (наибольшего) сингулярного числа. Я обозначил ре-
зультат как A1, потому что это не изначальная мат­ри­ца A, а одноранговая мат­
ри­ца того же размера, что и A. И не просто любая одноранговая мат­ри­ца – это
самый важный «слой» мат­ри­цы. Он является самым важным, потому что имеет
наибольшее сингулярное число (подробнее об этом – в следующем разделе).
  С учетом этого изначальная мат­ри­ца реконструируется путем суммирова-
ния всех «слоев» сингулярного разложения, ассоциированных с σ > 01:




1
    Суммировать нуль-значные сингулярные числа смысла нет, потому что это просто
    сложение мат­риц нулей.
                        Сингулярное разложение из собственного разложения  259

   Суть этого суммирования в том, что не обязательно использовать все r
слоев; вместо этого можно реконструировать какую-то другую мат­ри­цу, на-
зовем ее Ã, содержащую первые k < r слоев. Это называется низкоранговой
аппроксимацией мат­ри­цы A – в данном случае -ранговой аппроксимацией.
   Низкоранговая аппроксимация применяется, например, в очистке данных.
Идея состоит в том, что информация, ассоциированная с малыми сингуляр-
ными числами, вносит малый вклад в суммарную дисперсию набора данных
и, следовательно, может отражать шум, который можно удалить. Подробнее
об этом – в следующей главе.



Сингулярное разложение из собственного
разложения
Итак, к этому моменту главы вы уже знакомы с основами понимания и ин-
терпретации мат­риц сингулярного разложения. Уверен, вам интересно, что
это за волшебная формула, которая производит сингулярное разложение.
Может быть, она настолько невероятно многосложна, что его мог понять
только Гаусс? А может, ее придется так долго объяснять, что в одну главу не
влезет?
  Отнюдь нет!
  На самом деле сингулярное разложение – по-настоящему элементарное
(концептуально; другое дело – расчитывать сингулярное разложение вруч-
ную). Оно вычисляется из мат­ри­цы собственного разложения, умноженной
всего лишь на ее транспонированную версию. Следующие ниже уравнения
показывают, как получать сингулярные числа и левые сингулярные векторы:

  AAT = (UΣVT)(UΣVT)T
      = UΣVTVΣTVT
      = UΣ2UT.

  Другими словами, собственные векторы мат­ри­цы AAT – это левосингуляр-
ные векторы мат­ри­цы A, а квадраты собственных чисел мат­ри­цы AAT – это
сингулярные числа мат­ри­цы A.
  Этот ключевой для понимания момент раскрывает три признака сингу-
лярного разложения:
  1)	сингулярные числа неотрицательны, потому что числа в квадрате не
      могут быть отрицательными;
  2)	сингулярные числа – действительно-значные, потому что симметрич-
      ные мат­ри­цы имеют действительно-значные собственные числа;
  3) сингулярные векторы ортогональны, потому что собственные векторы
      симметричной мат­ри­цы ортогональны.
  Правые сингулярные числа получаются в результате предпозиционного
умножения транспонированной мат­ри­цы:
260  Сингулярное разложение

    ATA = (UΣVT)T(UΣVT)
        = VΣTUTUΣVT
        = VΣ2VT.

  На самом деле уравнение сингулярного разложения можно реорганизо-
вать под решение правосингулярных векторов, не вычисляя собственного
разложения ATA:

    VT = Σ–1UTA.

  Разумеется, есть комплементарное уравнение получения U, если уже из-
вестна V.


Сингулярное разложение матрицы АТА
Вкратце: если мат­ри­ца может быть выражена как S = ATA, то ее левые и пра-
вые сингулярные векторы равны. Другими словами:

    S = UΣVT = VΣUT = UΣUT = VΣVT.

  Доказательство этого утверждения происходит из расписывания сингу-
лярного разложения S и ST, а затем рассмотрения последствия S = ST. Я остав-
ляю его вам на самостоятельное обследование! И также призываю вас под-
твердить его на Python, используя случайные симметричные мат­ри­цы.
  По сути, для симметричной мат­ри­цы сингулярное разложение – это то
же самое, что собственное разложение. Этот факт имеет последствия для
анализа главных компонент (PCA), поскольку PCA может выполняться с ис-
пользованием собственного разложения мат­ри­цы ковариаций в данных, син-
гулярного разложения мат­ри­цы ковариаций либо сингулярного разложения
мат­ри­цы данных.


Конвертация сингулярных чисел в дисперсию:
объяснение
Сумма сингулярных чисел представляет собой общую величину «дисперсии»
в мат­ри­це. Что это значит? Если представить, что информация в мат­ри­це
содержится в пузыре, то сумма сингулярных чисел подобна объему этого
пузыря.
   Причина, по которой вся дисперсия содержится в сингулярных числах,
заключается в том, что сингулярные векторы нормированы в единичный
модуль, а это означает, что они не предоставляют информации о модуле
(то есть ||Uw|| = ||w||)1. Другими словами, сингулярные векторы указывают,
а сингулярные числа говорят, насколько далеко.

1
    Доказательство этого утверждения находится в упражнении 14.3.
                       Сингулярное разложение из собственного разложения  261

  «Сырые» сингулярные числа находятся в числовой шкале мат­ри­цы. Это
означает, что если умножить данные на скаляр, то сингулярные числа уве-
личатся. А это, в свою очередь, означает, что сингулярные числа трудно ин-
терпретировать и практически невозможно сравнивать между разными на-
борами данных.
  По этой причине нередко удобно конвертировать сингулярные числа
в процент общей объясненной дисперсии. Формула проста; каждое сингу-
лярное число i нормализуется следующим образом:




  Эта нормализация распространена в анализе главных компонент, напри-
мер чтобы определять число компонент, на которые приходится 99 % дис-
персии. Данное число можно интерпретировать как показатель сложности
системы.
  Важно отметить, что указанная нормализация не влияет на относитель-
ные расстояния между сингулярными числами; она просто меняет числовую
шкалу на более удобочитаемую.


Кондиционное число
В этой книге я несколько раз намекал, что кондиционное число мат­ри­цы ис-
пользуется для обозначения численной стабильности мат­ри­цы. Теперь, когда
вы знаете о сингулярных числах, вы способны лучше понять, как вычислять
и интерпретировать кондиционное число.
   Кондиционное число мат­ри­цы определяется как отношение наибольшего
сингулярного числа к наименьшему. Оно часто обозначается буквой κ (гре-
ческой буквой каппа):




  Кондиционное число очень часто используется в статистике и машинном
обучении с целью оценивания стабильности мат­ри­цы при вычислении ее
обратной мат­ри­цы и при использовании для решения систем уравнений (на-
пример, наименьших квадратов). Разумеется, необратимая мат­ри­ца имеет
кондиционное число NaN, потому что σ/0 = ?! .
  Но полноранговая мат­ри­ца с большим кондиционным числом все же мо-
жет быть численно нестабильной. Обратная мат­ри­ца, хотя и являясь тео­
ретически обратимой, на практике бывает ненадежной. Такие мат­ри­цы
называются плохо обусловленными. Вы, возможно, встречали этот термин
в предупреждающих сообщениях Python, иногда сопровождаемый такими
фразами, как «точность результата не гарантируется».
  В чем проблема с плохо обусловленной матрицей? По мере увеличения
кондиционного числа мат­ри­ца тяготеет к сингулярности. Следовательно,
плохо обусловленная мат­ри­ца является «почти сингулярной», а обратная
262  Сингулярное разложение

ей мат­ри­ца становится недостоверной из-за повышенного риска числовых
ошибок.
  Влияние плохо обусловленной мат­ри­цы можно трактовать несколькими
способами. Одним из них является снижение прецизионности решения из-за
ошибок округления. Например, кондиционное число порядка 105 означает,
что решение (например, обратная мат­ри­ца или задача наименьших квад­
ратов) теряет пять значащих цифр (это означало бы, например, переход от
прецизионности 10−16 к 10−11).
  Вторая связанная с предыдущей интерпретация – это коэффициент уси-
ления шума. Если у вас есть мат­ри­ца с кондиционным числом порядка 104,
то шум может повлиять на решение задачи наименьших квадратов на 104.
Это может показаться много, но оно будет незначительным усилением, если
ваши данные имеют прецизионность 10−16.
  В-третьих, кондиционное число указывает на чувствительность решения
к возмущениям в мат­ри­це данных. Хорошо обусловленная мат­ри­ца может
пертурбироваться (добавлением большего шума) минимальным изменени-
ем решения. Напротив, добавление малого шума к плохо обусловленной
мат­ри­це может приводить к совершенно другим решениям.
  Каков порог плохой обусловленности мат­ри­цы? Никакого. Не существует
магического числа, которое отличает хорошо обусловленную мат­ри­цу от пло-
хо обусловленной. Различные алгоритмы будут применять разные пороги,
которые зависят от числовых значений в мат­ри­це.
  Ясно одно: следует серьезно относиться к предупреждающим сообщениям
о плохо обусловленных матрицах. Обычно они указывают на то, что что-то
не так, и результатам не следует доверять.


                Что делать с плохо обусловленными мат­ри­ца­ми?
 К сожалению, это не тот вопрос, на который я могу дать конкретный ответ. Правильный
 поступок при наличии плохо обусловленной мат­ри­цы во многом зависит от мат­ри­цы
 и от задачи, которую вы пытаетесь решить. Для ясности: плохо обусловленные мат­ри­цы
 по своей сути не являются плохими, и вы никогда не должны легкомысленно отбрасы-
 вать мат­ри­цу только из-за ее кондиционного числа. Плохо обусловленная мат­ри­ца несет
 в себе потенциальные проблемы только для определенных операций и, следовательно,
 актуальна только для определенных мат­риц, таких как мат­ри­цы статистических моделей
 или мат­ри­цы ковариаций.
 Варианты исправления плохо обусловленной мат­ри­цы включают регуляризацию, умень-
 шение размерности и улучшение качества данных или выделение признаков.




Сингулярное разложение и псевдообратная
матрица Mура–Пенроуза
Сингулярное разложение обратной мат­ри­цы выглядит довольно элегантно.
Исходя из допущения, что мат­ри­ца – квадратная и обратимая, мы получаем:
                                                            Резюме  263

  A–1 = (UΣVT)–1
     = VΣ–1U–1
     = VΣ–1UT.

  Другими словами, нам нужно только инвертировать Σ, потому что U–1 = UT.
Кроме того, поскольку Σ является диагональной матрицей, обратная ей мат­
ри­ца получается путем простого инвертирования каждого диагонального
элемента. С другой стороны, этот метод по-прежнему подвержен численной
нестабильности, потому что крошечные сингулярные числа, которые могут
отражать ошибки прецизионности (например, 10−15), становятся галактиче-
ски большими при инвертировании.
  Теперь что касается алгоритма вычисления псевдообратной мат­ри­цы
Mура–Пенроуза. Вы ждали этого во многих главах, и я ценю ваше терпение.
  Псевдообратная мат­ри­ца Mура–Пенроуза вычисляется почти точно так
же, как полная обратная мат­ри­ца, показанная в предыдущем примере; един-
ственное видоизменение состоит в инвертировании ненулевых диагональных
элементов в Σ вместо попыток инвертировать все диагональные элементы.
(На практике «ненулевое значение» реализуется как превышение порога,
чтобы учитывать ошибки прецизионности.)
  Вот и все! Вот так вычисляется псевдообратная мат­ри­ца. Вы видите, что
это очень просто и интуитивно понятно, но для понимания потребовался
значительный объем базовых знаний о линейной алгебре.
  А еще лучше, поскольку сингулярное разложение работает с мат­ри­ца­ми
любого размера, псевдообратная мат­ри­ца Mура–Пенроуза может приме-
няться к неквадратным мат­ри­цам. На самом деле псевдообратная мат­ри­ца
Mура–Пенроуза высокой мат­ри­цы равна ее левообратной мат­ри­це, а псев-
дообратная мат­ри­ца широкой мат­ри­цы равна ее правообратной. (Напомню,
что псевдообратная мат­ри­ца обозначается как A†, A+ или A*.)
  Вы получите больше опыта работы с псевдообратной матрицей, реализуя
ее самостоятельно в упражнениях.



Резюме
Надеюсь, вы согласитесь, что после того, как вы приложили усилия, что-
бы узнать о собственном разложении, теперь потребовалось лишь немно-
го дополнительных усилий, чтобы разобраться в сингулярном разложении.
Сингулярное разложение, пожалуй, является самым важным разложением
в линейной алгебре, потому что оно раскрывает насыщенную и подробную
информацию о мат­ри­це. Вот ключевые моменты, которые следует вынеси
из этой главы.
     Сингулярное разложение разлагает мат­ри­цу (любого размера и ранга)
       в произведение трех мат­риц, именуемых левыми сингулярными век-
       торами U, сингулярными числами Σ и правыми сингулярными векто-
       рами VT.
264  Сингулярное разложение

    Первые r (где r – это ранг мат­ри­цы) левых сингулярных векторов обес­
      печивают ортонормальный базис столбцового пространства мат­ри­цы,
      тогда как последующие сингулярные векторы обеспечивают ортонор-
      мальный базис левого нуль-пространства.
    Аналогичная история с правыми сингулярными векторами: первые
      r векторов обеспечивают ортонормальный базис строчного простран-
      ства, а последующие векторы обеспечивают ортонормальный базис
      нуль-пространства. Следует учесть, что правильные сингулярные век-
      торы на самом деле являются строками мат­ри­цы V, то есть столбцами
      мат­ри­цы VT.
    Количество ненулевых сингулярных чисел равно рангу мат­ри­цы. На
      практике бывает трудно отличить очень малые ненулевые сингулярные
      числа от ошибок прецизионности нуль-значных сингулярных чисел.
      Такие программы, как Python, будут использовать порог допуска, что-
      бы проводить это различие.
    Внешнее произведение k-го левого сингулярного вектора и k-го пра-
      вого сингулярного вектора, скаляр, умноженный на k-е сингулярное
      число, дает одноранговую мат­ри­цу, которую можно интерпретировать
      как «слой» мат­ри­цы. Реконструкция мат­ри­цы на основе слоев имеет
      множество применений, включая подавление шума и сжатие данных.
    В концептуальном плане сингулярное разложение можно получить из
      собственного разложения AAT.
    Суперважная псевдообратная мат­ри­ца Мура–Пенроуза вычисляется
      как VΣ+UT, где Σ+ получается путем инвертирования ненулевых сингу-
      лярных чисел на диагонали.



Упражнения по программированию
  Упражнение 14.1
  Вы узнали, что для симметричной мат­ри­цы сингулярные числа и соб-
ственные числа совпадают. Как насчет сингулярных векторов и собственных
векторов? Для ответа на этот вопрос примените Python с использованием
случайной 5×5-мат­ри­цы ATA. Далее попробуйте еще раз, используя аддитив-
ный метод создания симметричной мат­ри­цы (AT + A). Обратите внимание
на знаки собственных чисел мат­ри­цы AT + A.

  Упражнение 14.2
  Python может дополнительно возвращать «экономное» сингулярное раз-
ложение, то есть мат­ри­цы сингулярных векторов усекаются до меньшего из
M или N. Обратитесь к документационному литералу, чтобы выяснить, как
это сделать. Подтвердите с высокими и широкими мат­ри­ца­ми. Обратите
внимание, что обычно требуется возвращать полные мат­ри­цы; экономное
сингулярное разложение в основном используется для по-настоящему боль-
ших мат­риц и/или реально ограниченной вычислительной мощности.
                                           Упражнения по программированию  265

   Упражнение 14.3
   Одним из важных признаков ортогональных мат­риц (таких как мат­ри­цы
левых и правых сингулярных векторов) является то, что они поворачивают,
но не масштабируют вектор. Это означает, что после умножения на орто-
гональную мат­ри­цу величина вектора сохраняется. Докажите, что ||Uw|| =
||w||. Затем продемонстрируйте это эмпирически на Python, используя сингу-
лярную мат­ри­цу векторов из сингулярного разложения случайной мат­ри­цы
и случайного вектора.

  Упражнение 14.4
  Создайте случайную высокую мат­ри­цу с указанным кондиционным чис-
лом. Сделайте это, создав две случайные квадратные мат­ри­цы U и V и пря-
моугольную Σ. Подтвердите, что это эмпирическое кондиционное число
мат­ри­цы UΣVT совпадает с указанным вами числом. Визуализируйте свои
результаты в виде рисунка, подобного рис. 14.4. (Я использовал кондицион-
ное число 42.41.)




                       Рис. 14.4  Результаты упражнения 14.3

  Упражнение 14.5
  Здесь ваша цель проста: написать исходный код, воспроизводящий
рис. 14.5. Что показывает этот рисунок? На панели A показана случайная
мат­ри­ца 30×40, которую я создал путем сглаживания случайных чисел (реа­
лизованного как 2D-свертка между 2D-гауссианом и случайным числом; если
вы незнакомы с обработкой и фильтрацией изображений, то можете сво-
бодно скопировать исходный код и создать эту мат­ри­цу из моего исходного
кода решения). На остальной части панели А представлены мат­ри­цы сингу-
лярного разложения. Интересно отметить, что более ранние сингулярные
векторы (ассоциированные с большими сингулярными числами) выглядят
глаже, а более поздние – более неровными; это происходит из-за простран-
ственной фильтрации.

1
    Да, и это еще одна отсылка к книге «Автостопом по Галактике».
266  Сингулярное разложение

  На панели B продемонстрирован «график крутого склона», на котором
показаны объясненные сингулярные числа, нормализованные к проценту
объясненной дисперсии. Обратите внимание, что на первые несколько ком-
понент приходится бóльшая часть дисперсии изображения, в то время как
на более поздние компоненты приходится относительно малая дисперсия.
Подтвердите, что сумма по всем нормализованным сингулярным числам
равна 100. Панель C показывает первые четыре «слоя» – одноранговые мат­
ри­цы, определенные как ui σi viT, в верхней строке и совокупную сумму этих
слоев в нижней строке. Вы видите, что каждый слой добавляет в мат­ри­цу
больше информации; правое нижнее изображение (под названием «L 0:3»)
представляет собой одноранговую мат­ри­цу, но визуально очень похоже на
изначальную 30-ранговую мат­ри­цу на панели A.




                 График крутого склона
дисперсия (%)
Объясненная




                    Число компонент
                      Рис. 14.5  Результаты упражнения 14.5

  Упражнение 14.6
  Реализуйте псевдообратную мат­ри­цу Мура–Пенроуза на основе описа-
ния в этой главе. Вам нужно определить допуск для игнорирования крошеч-
ных, но ненулевых сингулярных чисел. Пожалуйста, не ищите реализацию
NumPy – и не возвращайтесь к более раннему исходному коду этой главы –
вместо этого используйте свои знания линейной алгебры, чтобы придумать
свой собственный уровень допуска.
  Протестируйте свой исходный код на 3-ранговой мат­ри­це 5×5. Сравните
свой результат с результатом на выходе из функции NumPy pinv. Наконец,
проинспектируйте исходный код np.linalg.pinv, чтобы убедиться, что вы
понимаете реализацию.

  Упражнение 14.7
  Продемонстрируйте, что псевдообратная мат­ри­ца Мура–Пенроуза рав-
на левообратной мат­ри­це для мат­ри­цы с полным столбцовым рангом пу-
тем вычисления явной левообратной мат­ри­цы высокой полной мат­ри­цы
                                       Упражнения по программированию  267

((ATA)–1AT) и псевдообратной мат­ри­цы мат­ри­цы A. Повторите эти действия
для правообратной мат­ри­цы широкой мат­ри­цы с полным строчным рангом.

   Упражнение 14.8
   Рассмотрите уравнение собственного числа Av = λv. Теперь, когда вы знае­
те о псевдообратной мат­ри­це, вы можете немного поэкспериментировать
с этим уравнением. В частности, используйте мат­ри­цу 2×2, которая при-
менялась в начале главы 13, чтобы вычислить v+ и подтвердить, что vv+ = 1.
Затем подтвердите следующие ниже тождества:

  v+Av = λv+v;
  Avv+ = λvv+.
Глава       15
                                  Применения
                                 собственного
                               и сингулярного
                                  разложений

Собственное и сингулярное разложения – это жемчужины, которыми линей-
ная алгебра наградила современную человеческую цивилизацию. Их важ-
ность в современной прикладной математике невозможно недооценить,
и их применения бесчисленны и разбросаны по множеству дисциплин.
  В этой главе я выделю три применения, с которыми вы, вероятно, столк­
нетесь в науке о данных и смежных областях. Моя главная цель – показать,
что кажущиеся сложными методы науки о данных и машинного обучения на
самом деле вполне разумны и легко понятны, как только вы изучите линей-
но-алгебраические темы этой книги.



Анализ главных компонентс использованием
собственного и сингулярного разложений
Анализ главных компонент (PCA) предназначен для отыскания набора ба-
зисных векторов набора данных, которые указывают в направлении, макси-
мизирующем ковариацию между переменными.
  Представьте, что набор данных N-D существует в пространстве N-D, в ко-
тором каждая точка данных является координатой в этом пространстве. Это
разумно, когда вы думаете о хранении данных в мат­ри­це с N наблюдениями
(каждая строка является наблюдением) по M признаков (каждый столбец
является признаком, также именуемым переменной, или результатом из-
мерений); данные обитают в ℝM и содержат N векторов или координат.
                                                            Анализ главных компонент  269

    Пример в 2D показан на рис. 15.1. Левая панель показывает данные в их
  изначальном пространстве данных, в котором каждая переменная обеспе-
  чивает базисный вектор для данных. Ясно, что две переменные (оси x и y)
  связаны друг с другом, и очевидно, что есть направление в данных, которое
  отражает это отношение лучше, чем любой базисный вектор признаков.

               Данные в пространстве каналов                  Данные в пространстве PCI
Ось данных 2




                                                Ось PCI 2




                        Ось данных 1                                   Ось PCI 1
                              Рис. 15.1  Графический обзор PCA в 2D

    Цель PCA – найти новый набор базисных векторов, такой чтобы линейные
  взаимосвязи между переменными были максимально выровнены с базис-
  ными векторами – это то, что показано на правой панели рис. 15.1. Важно
  отметить, что PCA имеет ограничение, заключающееся в том, что новые
  базисные векторы являются ортогональными поворотами изначальных
  базисных векторов. В упражнениях вы увидите последствия этого ограни-
  чения.
    В следующем разделе я расскажу о математике и процедурах вычисления
  PCA; в упражнениях у вас будет возможность реализовать PCA, используя
  собственное и сингулярное разложения, и сравнить свои результаты с реа-
  лизацией PCA на Python.


  Математика анализа главных компонент
  В анализе главных компонент (PCA) совмещена статистическая концепция
  дисперсии с линейно-алгебраической концепцией линейно-взвешенной
  комбинации. Дисперсия, как вы знаете, является мерой разброса набора
  данных вокруг его среднего значения. Метод PCA основан на том, что дис-
  персия – это хорошо, а направления в пространстве данных, которые имеют
  бóльшую дисперсию, более важны (иначе говоря, «дисперсия = релевант-
  ность»).
270  Применения собственного и сингулярного разложений

  Но в PCA нас интересует не просто дисперсия внутри одной переменной;
вместо этого мы хотим найти линейно-взвешенную комбинацию всех пере-
менных, которая максимизирует дисперсию данной компоненты (компонен-
та – это линейно-взвешенная комбинация переменных).
  Запишем это математически. Матрица X – это наша мат­ри­ца данных (вы-
сокая мат­ри­ца с полным столбцовым рангом, имеющая формат «наблюдения
по признакам»), а w – вектор весов. Наша цель в PCA – найти набор весов в w
такой, что Xw имеет максимальную дисперсию. Дисперсия – это скаляр, по-
этому мы можем записать ее как

    λ = ||Xw||2.

  Квадрат векторной нормы – это на самом деле то же самое, что и дис-
персия, когда данные центрированы по среднему (т. е. каждая переменная
данных имеет нулевое среднее значение)1; я опустил шкалирующий коэф-
фициент 1/(N – 1), потому что он не влияет на решение цели оптимизации.
  Проблема с этим уравнением в том, что w можно задать ОГРОМНЫМИ
числами; чем больше веса, тем больше дисперсия. Решение состоит в шкали-
ровании нормы взвешенной комбинации переменных нормой весов:




  Теперь у нас есть соотношение двух векторных норм. Эти нормы можно
расширить до точечных произведений, чтобы получить ясное представление
об уравнении:




    C = XTX;




  Теперь мы обнаружили, что решение PCA совпадает с решением для отыс­
кания направленного вектора, который максимизирует нормализованную
квадратичную форму (норма вектора – это член нормализации) мат­ри­цы
ковариаций в данных.
  Это все хорошо, но как на самом деле найти элементы вектора w, которые
максимизируют λ?
  Здесь линейно-алгебраический подход заключается в том, чтобы рассмат­
ривать не просто одно векторное решение, а все множество решений. Таким
образом, мы перепишем уравнение, используя вместо вектора w мат­ри­цу
W. В результате получим мат­ри­цу в знаменателе, что в линейной алгебре
недопустимо; поэтому умножаем на взаимообратную:

    Λ = (WTW)–1WTCW.

1
    В онлайновом исходном коде это продемонстрировано.
                                                         Анализ главных компонент  271

    Теперь применим немного алгебры и посмотрим, что произойдет:

       Λ = (WTW)–1WTCW;
       Λ = W–1W–TWTCW;
       Λ = W–1CW;
    WΛ = CW.

   Примечательно, что мы обнаружили, что решение PCA заключается в вы-
полнении собственного разложения мат­ри­цы ковариаций данных. Собствен-
ные векторы – это веса переменных, а соответствующие им собственные
числа – это дисперсии данных по каждому направлению (каждый столбец
мат­ри­цы W).
   Поскольку мат­ри­цы ковариаций симметричны, их собственные векторы
и, следовательно, главные компоненты ортогональны. Это имеет важные по-
следствия для пригодности PCA для анализа данных, которые вы обнаружите
в упражнениях.


                                        Доказательство PCA
    Здесь я изложу доказательство того, что собственное разложение решает задачу опти-
    мизации PCA. Если вы незнакомы с дифференциальным исчислением и множителями
    Лагранжа, то можете свободно пропустить эту вкладку; я вставил ее сюда для полноты,
    а не потому, что вам нужно понять это доказательство, чтобы решать упражнения или при-
    менять PCA на практике.
    Наша цель – максимизировать wTCw, при условии что wTw = 1. Эту оптимизацию можно
    выразить с помощью множителя Лагранжа:

    L(w, λ) = wTCw – λ(wTw – 1);
               d T
         0=      (w Cw – λ(wTw – 1));
              dw
         0 = Cw – λw;
       Cw = λw.

    Вкратце: идея состоит в применении множителя Лагранжа, чтобы сбалансировать опти-
    мизацию ограничением, взять производную по вектору весов, приравнять производную
    к нулю, продифференцировать по w и обнаружить, что w является собственным вектором
    мат­ри­цы ковариаций.




Шаги выполнения PCA
Оставив в стороне математику, ниже перечислены шаги реализации анализа
главных компонент1.

1
    В упражнении 15.3 вы также узнаете, как реализовывать PCA с помощью библио-
    теки Python scikit-learn.
272  Применения собственного и сингулярного разложений

    1.	Вычислить мат­ри­цу ковариаций в данных. Результирующая мат­ри­ца
        коварий будет в формате «признаки по признакам». Перед вычислени-
        ем ковариации каждый признак в данных должен быть центрирован
        по среднему значению.
    2. Взять собственное разложение этой мат­ри­цы ковариаций.
    3.	Отсортировать собственные числа по убыванию модуля и соответству-
        ющим образом отсортировать собственные векторы. Собственные чис-
        ла PCA иногда называются показателями латентных факторов1.
    4.	Вычислить «показатели компонент» как взвешенную комбинацию всех
        признаков данных, где собственный вектор обеспечивает веса. Соб-
        ственный вектор, ассоциированный с наибольшим собственным чис-
        лом, является «наиболее важной» компонентой, то есть с наибольшей
        дисперсией.
    5.	Конвертировать собственные числа в процент объясненной дисперсии,
        чтобы облегчить интерпретацию.


PCA посредством сингулярного разложения
PCA можно выполнить эквивалентным образом посредством собственного
разложения, как описано ранее, либо посредством сингулярного разложения.
При этом существует два способа выполнения PCA с помощью сингулярного
разложения:
    взять сингулярное разложение мат­ри­цы ковариаций. Процедура иден-
      тична ранее описанной, потому что для мат­риц ковариаций сингуляр-
      ное и собственное разложения совпадают;
    взять сингулярное разложение мат­ри­цы данных напрямую. В дан-
      ном случае правые сингулярные векторы (мат­ри­ца V) эквивалентны
      собственным векторам мат­ри­цы ковариаций (это будут левые син-
      гулярные векторы, если мат­ри­ца данных хранится как признаки по
      наблюдениям). Перед вычислением сингулярного разложения данные
      должны быть центрированы по среднему значению. Квадратный ко-
      рень сингулярных чисел эквивалентен собственным числам мат­ри­цы
      ковариаций.
  Какое разложение следует использовать для выполнения PCA? Возможно,
вы подумали, что сингулярное разложение проще, потому что не требует
мат­ри­цы ковариаций. Это верно для относительно малых и чистых наборов
данных. Но большие или более сложные наборы данных потребуют отбора
данных или будут потреблять слишком много памяти, чтобы получить син-
гулярное разложение всей мат­ри­цы данных. В этих случаях сначала вычис-
ление мат­ри­цы ковариаций может повышать гибкость анализа. Но выбор
собственного разложения вместо сингулярного разложения часто зависит
от личных предпочтений.


1
    Англ. latent factor scores. – Прим. перев.
                                        Линейный дискриминантный анализ  273


Линейный дискриминантный анализ
Линейный дискриминантный анализ (LDA)1 – это метод многопеременной
классификации, который часто используется в машинном обучении и ста-
тистике. Первоначально он был разработан Рональдом Фишером2, которо-
го часто считают «дедушкой» статистики за его многочисленные и важные
вклады в математические основы статистики.
   Цель LDA – найти направление в пространстве данных, максимально раз-
деляющее категории данных. Пример набора данных конкретной задачи
показан на графике A рис. 15.2. Визуально очевидно, что эти две категории
можно разделить, но они не разделимы ни по одной из осей данных – это
ясно из визуального осмотра маржинальных (частных) распределений.
   Теперь введем линейный дискриминантный анализ. LDA найдет базисные
векторы в пространстве данных, которые максимально разделяют две кате-
гории. График B на рис. 15.2 показывает те же данные, но в пространстве LDA.
Теперь классификация будет элементарной: наблюдения с отрицательными
значениями по оси 1 помечаются как категория «0», а любые наблюдения
с положительными значениями по оси 1 помечаются как категория «1». Дан-
ные полностью не разделимы на оси 2, что указывает на то, что для точной
категоризации в этом наборе данных одного измерения достаточно.




                      Рис. 15.2  Пример 2D-задачи для LDA

  Звучит здорово, не правда ли? Но как работает такое чудо математики?
На самом деле оно довольно просто устроено и основано на обобщенном
собственном разложении, о котором вы узнали ближе к концу главы 13.

1
    Англ. Linear Discriminant Analysis. – Прим. перев.
2
    Более того, линейный дискриминантный анализ также называют дискриминант-
    ным анализом Фишера.
274  Применения собственного и сингулярного разложений

  Давайте я начну с целевой функции: наша цель – найти набор весов, такой
чтобы взвешенная комбинация переменных максимально разделяла катего-
рии. Эта целевая функция записывается аналогично целевой функции PCA:




   На естественном языке она звучит так: мы хотим найти набор весов при-
знаков w, который максимизирует отношение дисперсии признака XB к дис-
персии признака XW данных. Обратите внимание, что ко всем наблюдени-
ям применяются одинаковые веса. (После изложения математики я напишу
о признаках B и W подробнее.)
   Линейно-алгебраическое решение вытекает из аналогичного аргумен-
та, как описано в разделе PCA. Во-первых, расширяем ||XBw||2 до wTXBT XBw
и представляем его как wTCBw; во-вторых, рассматриваем множество реше-
ний вместо одного решения; в-третьих, заменяем деление взаимообратным
умножением; и, наконец, выполняем несколько алгебраических действий
и смотрим, что произойдет:

         Λ = (WTCWW)–1WTCBW;
                 –1
         Λ = W–1CW  W–TWTCBW;
                 –1
         Λ = W–1CW  CBW;
            –1
      WΛ = CW  CBW;
    CWWΛ = CBW.

   Другими словами, решение LDA получается из обобщенного собственно-
го разложения двух мат­риц ковариаций. Собственные векторы – это веса,
а обобщенные собственные числа – это коэффициенты дисперсии каждой
компоненты1.
   Если не учитывать математику, то какие признаки данных используются
для построения XB и XW? Дело в том, что есть разные способы реализации
этой формулы, в зависимости от природы задачи и конкретной цели анализа.
Но в типичной модели LDA XB происходит из межкатегориальной ковариа-
ции, а XW – из внутрикатегориальной ковариации.
   Внутрикатегориальная ковариация – это просто среднее значение ковариа­
ций образцов данных внутри каждого класса. Межкатегориальная ковариация
происходит из создания новой мат­ри­цы данных, содержащей средние зна-
чения признаков внутри каждого класса. Я проведу вас по данной процеду­
ре в упражнениях. Если вы знакомы со статистикой, то поймете, что эта
формула аналогична отношению межгрупповой суммы квадратов ошибок
к внутригрупповой сумме в моделях ANOVA.
   Два заключительных комментария: собственные векторы обобщенного
собственного разложения не ограничены ортогональностью. Это вызвано

1
    Я не буду приводить нагруженное вычислениями доказательство, но это всего лишь
    второстепенный вариант доказательства, приведенного в разделе PCA.
            Низкоранговая аппроксимация посредством сингулярного разложения  275

          –1
тем, что CW  CB, как правило, не является симметричной матрицей, хотя две
мат­ри­цы ковариаций сами по себе симметричны. Несимметричные мат­
ри­цы не имеют ограничения по ортогональности собственного вектора. Вы
увидите это в упражнениях.
  Наконец, LDA всегда будет находить линейное решение (да, это видно и из
самого названия LDA), даже если данные не являются линейно разделимыми.
Нелинейное разделение потребует преобразования данных или применения
нелинейного метода категоризации, такого как искусственные нейронные
сети. LDA по-прежнему будет работать в смысле получения результата; вам
как исследователю данных решать, подходит ли этот результат для постав-
ленной задачи и можно ли его интерпретировать.



Низкоранговая аппроксимация посредством
сингулярного разложения
Я объяснил концепцию низкоранговых аппроксимаций в предыдущей главе
(например, в упражнении 14.5). Ее идея состоит в том, чтобы взять сингу-
лярное разложение мат­ри­цы данных или изображения, а затем реконструи-
ровать эту мат­ри­цу данных, используя некоторое подмножество компонент
сингулярного разложения.
  Этого можно добиться, задав отобранные значения σ равными нулю или
создав новые прямоугольные мат­ри­цы сингулярного разложения с подлежа-
щими удалению векторами и удаленными сингулярными числами. Этот вто-
рой подход будет предпочтительнее, поскольку он уменьшает объем сохраня-
емых данных, как вы увидите в упражнениях. При таком подходе сингулярное
разложение можно использовать для сжатия данных до меньшего размера.


             Применяется ли сингулярное разложение в компьютерах
                          для сжатия изображений?
 Короткий ответ – нет, не используется.
 Алгоритмы, лежащие в основе распространенных форматов сжатия изображений, таких
 как JPG, предусматривают поблочное сжатие, в которое встроены принципы человеческо-
 го восприятия (включая, например, то, как мы воспринимаем контраст и пространствен-
 ные частоты), что позволяет им достигать более качественных результатов при меньшей
 вычислительной мощности по сравнению с одним сингулярным разложением всего изо-
 бражения.
 Тем не менее принцип остается тем же, что и при сжатии на основе сингулярного раз-
 ложения: выявить небольшое число базисных векторов, которые сберегают важные при-
 знаки изображения, такие что низкоранговая реконструкция является точной аппрокси-
 мацией оригинала в полной разрешающей способности.
 С учетом всего вышесказанного сингулярное разложение обычно используется для сжа-
 тия данных в других областях науки, включая биомедицинскую визуализацию.
276  Применения собственного и сингулярного разложений


Сингулярное разложение для шумоподавления
Шумоподавление посредством сингулярного разложения – это просто при-
менение низкоранговой аппроксимации. Единственное отличие состоит
в том, что компоненты сингулярного разложения отбираются для их исклю-
чения на основании шума, который они представляют, в отличие от внесения
малого вклада в мат­ри­цу данных.
   Подлежащие удалению компоненты могут быть слоями, связанными с наи-
меньшими сингулярными числами, – это может иметь место в случае низко-
амплитудного шума, ассоциированного с малыми дефектами оборудования.
Но более крупные источники шума, оказывающие более сильное влияние на
данные, могут иметь более высокие сингулярные числа. Указанные шумовые
компоненты могут выявляться алгоритмом, основанным на их характерис­
тиках, или путем визуального осмотра. В упражнениях вы увидите пример
использования сингулярного разложения для выделения источника шума,
добавленного в изображение.



Резюме
Вы дочитали книгу до конца (за исключением приведенных ниже упраж-
нений)! Поздравляю! Найдите минутку, чтобы погордиться собой и своим
стремлением учиться и инвестировать в свой мозг (в конце концов, это ваш
самый ценный ресурс). Я вами горжусь, и если бы мы встретились лично, то
дал бы вам пять, ударил бы кулаком о кулак, похлопал локтем или сделал бы
что угодно, что в данный момент приемлемо с социальной/медицинской
точки зрения.
   Надеюсь, вы почувствовали, что эта глава помогла вам увидеть неверо-
ятную важность собственного и сингулярного разложений для применений
в статистике и машинном обучении. Ниже – краткое изложение ключевых
моментов, которые я обязан включить согласно контракту.
     Цель анализа главных компонент (PCA) – найти набор весов, такой
       чтобы линейно-взвешенная комбинация признаков данных имела
       максимальную дисперсию. Эта цель отражает лежащее в основе PCA
       допущение, а именно что «дисперсия равна релевантности».
     PCA-анализ реализуется как собственное разложение мат­ри­цы кова-
       риаций данных. Собственные векторы – это коэффициенты переве-
       совки признаков, а собственные числа могут шкалироваться, чтобы
       кодировать процент дисперсии, учитываемой каждой компонентой
       (компонента – это линейно-взвешенная комбинация).
     PCA-анализ можно эквивалентным образом реализовать с использова-
       нием мат­ри­цы ковариаций либо мат­ри­цы данных сингулярного раз-
       ложения.
                                                               Упражнения  277

      Линейный дискриминантный анализ (LDA) используется для линейной
        категоризации многопеременных данных. Его можно трактовать как
        расширение PCA-анализа: в отличие от PCA, который максимизирует
        дисперсию, LDA максимизирует отношение дисперсий между двумя
        признаками данных.
      LDA-анализ реализуется как обобщенное собственное разложение на
        двух матрицах ковариаций, формируемых из двух разных признаков
        данных. Двумя признаками данных часто являются межклассовая ко-
        вариация (для максимизации) и внутриклассовая ковариация (для ми-
        нимизации).
      Низкоранговые аппроксимации предусматривают реконструкцию
        мат­ри­цы из подмножества сингулярных векторов/чисел и использу-
        ются для сжатия данных и шумоподавления.
      В сжатии данных ассоциированные с наименьшими сингулярными
        числами компоненты удаляются; в шумоподавлении данных удаляют-
        ся компоненты, улавливающие шум или артефакты (соответствующие
        им сингулярные числа могут быть малыми либо большими).



Упражнения
Анализ главных компонент (PCA)
Я люблю кофе по-турецки. Он делается из зерен очень мелкого помола и без
фильтра. Весь ритуал его приготовления и питья выглядит прекрасно. И если
вы пьете его из турки, то весьма возможно, что вам удастся погадать.
  Но это упражнение касается не кофе по-турецки, а проведения PCA-ана­
лиза на наборе данных1, который содержит временной ряд Стамбульской
фондовой биржи, а также биржевые данные из нескольких других фондовых
индексов в разных странах. Мы могли бы использовать этот набор данных,
чтобы узнать, например, управляются ли международные фондовые биржи
одним общим фактором или же в разных странах существуют независимые
финансовые рынки.

   Упражнение 15.1
   Перед выполнением PCA-анализа импортируйте и обследуйте данные.
Я сделал несколько графиков, показанных на рис. 15.3; вы можете воспроиз-
вести эти графики и/или использовать другие методы обследования данных.


1
    Первоисточник данных: Акбилгич Огуз. Стамбульская фондовая биржа. Репозито-
    рий машинного обучения UCI. 2013. Веб-сайт источников данных: https://archive-
    beta.ics.uci.edu/ml/datasets/istanbul+stock+exchange.
    278  Применения собственного и сингулярного разложений

                                                       A) Временной ряд данных
Финансовый возврат рынка




                                                                  Дата




                               B) Матрица корреляций                             C) Матрица ковариаций




                                               Рис. 15.3  Обследование набора данных
                                                    международной фондовой биржи

      Теперь приступим к PCA-анализу. Реализуйте PCA, используя пять описан-
    ных ранее в этой главе шагов. Визуализируйте результаты, как показано на
    рис. 15.4. Используйте исходный код, чтобы продемонстрировать несколько
    признаков PCA:
      1.	Дисперсия компонентного временного ряда (с использованием функ-
          ции np.var) равна собственному числу, ассоциированному с этой ком-
          понентой. Результаты для первых двух компонент приведены ниже:
                             Дисперсия первых двух компонент:
                             [0.0013006 0.00028585]

                             Первые два собственных числа:
                             [0.0013006 0.00028585]

                           2.	Корреляция между главными компонентами (то есть взвешенными
                               комбинациями бирж) 1 и 2 равна нулю, т. е. ортогональна.
                                                                                       Упражнения  279

                    3.	Визуализируйте веса собственных векторов первых двух компонент.
                        Веса показывают величину вклада, вносимую в компонент каждой
                        переменной.

      График крутого склона                                      Корреляция r = –0.00000
Процент дисперсии




                                                                                                     Компонент 1
                                                                                                     Компонент 2




                         Индекс                                       Время (день)
                       компоненты
                                Веса компонента 0                                Веса компонента 1
Вес




                                                           Вес




                                              Рис. 15.4  Результаты PCA
                                    на наборе данных Стамбульской фондовой биржи


     Обсуждение: график крутого склона убедительно свидетельствует о том,
   что международные фондовые биржи управляются общим фактором миро-
   вой экономики: на долю одной крупной компоненты приходится около 64 %
   дисперсии данных, тогда как на другие компоненты приходится менее 15 %
   дисперсии (в чисто случайном наборе данных мы ожидаем, что каждая ком-
   понента будет составлять 100/9 = 11 % дисперсии плюс/минус шум).
     Строгое оценивание статистической значимости этих компонент выходит
   за рамки данной книги, но, основываясь на визуальном осмотре графика
   крутого склона, мы не имеем права интерпретировать компоненты после
   первой; похоже, что бóльшая часть дисперсии в этом наборе данных четко
   укладывается в одно измерение.
     С точки зрения уменьшения размерности мы могли бы сократить весь на-
   бор данных до компоненты, ассоциированной с наибольшим собственным
   числом (она часто называется верхней компонентой), тем самым представив
   этот набор 9D-данных с помощью 1D-вектора. Разумеется, мы теряем инфор-
   мацию – 36 % информации в наборе данных удаляется, если сосредоточиться
   только на верхней компоненте, – но будем надеяться, что важные признаки
   сигнала находятся в верхней компоненте, тогда как менее важные признаки,
   включая случайный шум, игнорируются.
280  Применения собственного и сингулярного разложений

  Упражнение 15.2
  Воспроизведите результаты, используя:
  1) сингулярное разложение мат­ри­цы ковариаций данных и
  2)	сингулярное разложение самой мат­ри­цы данных. Напомню, что соб-
      ственные числа XTX являются квадратами сингулярных чисел X; кроме
      того, для того чтобы найти эквивалентность, шкалирующий коэффи-
      циент на мат­ри­це ковариаций должен применяться к сингулярным
      числам.

  Упражнение 15.3
  Сравните свой «ручной» PCA-анализ с результатом работы PCA-программы
на Python. Вам придется провести небольшое онлайновое исследование,
чтобы понять, как выполнять указанную программу на Python (это один из
самых важных навыков в программировании на Python!), но я дам вам под-
сказку: она находится в библиотеке sklearn.decomposition.

    Библиотека sklearn либо ручная реализация PCA?
    Следует ли вычислять PCA путем написания исходного кода вычисления и собствен-
    ного разложения мат­ри­цы ковариаций или же лучше использовать реализацию
    в биб­лиотеке sklearn? Всегда существует компромисс между использованием соб-
    ственного исходного кода с целью максимальной адаптации под конкретно-при-
    кладную задачу и использованием готового исходного кода с целью максимального
    упрощения. Одно из бесчисленного числа удивительных преимуществ понимания
    математики, лежащей в основе анализа данных, заключается в том, что вы имеете
    возможность адаптировать все виды анализа под свои потребности. В своем соб-
    ственном исследовании я обнаружил, что моя личная реализация PCA дает мне боль-
    ше свободы и гибкости.

  Упражнение 15.4
  Теперь выполните PCA-анализ на симулированных данных, что позволит
выявить одно из потенциальных ограничений PCA. Цель – создать набор дан-
ных, состоящий из двух «потоков» данных, и вывести главные компоненты
поверх, как показано на рис. 15.5.
  Ниже приводится процедура создания данных.
  1.	Создать мат­ри­цу 1000×2 случайных чисел, взятых из нормального (га-
      уссова) распределения, в котором шкала второго столбца уменьшена
      на 0.05.
  2. Создать мат­ри­цу 2×2 чистого поворота (см. главу 7).
  3.	Наложить две копии данных по вертикали: один раз с данными, по-
      вернутыми на θ = –π/6, и один раз с данными, повернутыми на θ = –π/3.
      Результирующая мат­ри­ца данных будет иметь размер 2000×2.
  Примените сингулярное разложение, чтобы реализовать PCA. Для про-
ведения визуального осмотра я прошкалировал сингулярные векторы с ко-
эффициентом 2.
  Обсуждение: PCA отлично подходит для уменьшения размерности вы-
сокоразмерного набора данных. За счет него облегчаются сжатие данных,
очистка данных и сглаживаются проблемы численной стабильности (напри-
мер, представьте, что 200-мерный набор данных с кондиционным числом
                                                             Упражнения  281

1010 сокращается до самых больших 100 измерений с кондиционным числом
105). Но сами измерения могут быть плохим вариантом для извлечения при-
знаков из-за ограничения по ортогональности. И действительно, главные
направления дисперсии на рис. 15.5 верны в математическом смысле, но
я уверен, что у вас сложилось впечатление, что это не лучшие базисные век-
торы, для того чтобы улавливать признаки данных.


                    Компонент 1
                    Компонент 2




                    Рис. 15.5  Результаты упражнения 15.4



Линейный дискриминантный анализ (LDA)
   Упражнение 15.5
   Вы собираетесь выполнить LDA на симулированных 2D-данных. Симу-
лированные данные удобны тем, что можно манипулировать размерами
эффектов, величиной и характером шума, числом категорий и т. д.
   Создаваемые вами данные показаны на рис. 15.2. Создайте два набора
нормально распределенных случайных чисел, каждый размером 200×2, с до-
бавлением второго измерения к первому (это вводит корреляцию между
переменными). Затем добавьте в первый набор чисел xy-смещение, равное
[2 −1]. Будет удобно создать мат­ри­цу 400×2, содержащую оба класса данных,
а также 400-элементный вектор меток классов (я использовал 0 для первого
класса и 1 для второго класса).
   Примените функции sns.jointplot и plot_joint, чтобы воспроизвести гра-
фик A рис. 15.2.
282  Применения собственного и сингулярного разложений

   Упражнение 15.6
   Теперь приступаем к LDA-анализу. Напишите исходный код с использова-
нием NumPy и/или SciPy вместо применения встроенной библиотеки, такой
как sklearn (мы вернемся к ней позже).
   Матрица внутриклассовых ковариаций CW создается путем вычисления
ковариации каждого класса отдельно и последующего усреднения этих мат­
риц ковариаций. Матрица межклассовых ковариаций CB создается путем
вычисления среднего значения каждого признака данных (в данном случае
xy-координат) внутри каждого класса, конкатенации этих векторов усред-
ненных признаков по всем классам (в разультате будет создана мат­ри­ца 2×2
для двух признаков и двух классов), а затем вычисления мат­ри­цы ковариа-
ций этой конкатенированной мат­ри­цы.
   Вспомните из главы 13, что обобщенное собственное разложение реали-
зуется с помощью функции SciPy eigh.
   Проецируемые в пространство LDA данные вычисляются как X      V, где X
                                                                         
содержит конкатенированные данные из всех классов, центрированные по
среднему значению по каждому признаку, а V – это мат­ри­ца собственных
векторов.
   Вычислите классификационную точность, которая заключается просто
в том, имеет ли каждый образец данных отрицательную («класс 0») либо по-
ложительную («класс 1») проекцию на первую компоненту LDA. График C
рис. 15.6 показывает предсказанную метку класса по каждому образцу данных.
   Наконец, покажите результаты, как видно на рис. 15.6.

  Упражнение 15.7
  В главе 13 я утверждал, что для обобщенного собственного разложения
мат­ри­ца собственных векторов V неортогональна, но ортогональна в про-
странстве мат­ри­цы – «знаменателе». Здесь ваша цель – продемонстрировать
это эмпирически.
  Вычислите и обследуйте результаты VTV и VTCWV. Если игнорировать кро-
шечные ошибки вычислительной прецизионности, то какой из них создает
единичную мат­ри­цу?

  Упражнение 15.8
  Теперь воспроизведите результаты, используя библиотеку Python sklearn.
Примените функцию LinearDiscriminantAnalysis из библиотечного модуля
sklearn.distribunt_analysis. Создайте график, подобный рис. 15.7, и убеди-
тесь, что совокупная точность предсказания совпадает с результатами ва-
шего «ручного» LDA-анализа в предыдущем упражнении. Эта функция до-
пускает применение нескольких разных решателей; примените решатель
eigen для соотнесения с предыдущим упражнением, а также выполнения
следующего упражнения.
  Нанесите сверху предсказанные метки из вашего «ручного» LDA; вы долж-
ны обнаружить, что предсказанные метки обоих подходов одинаковы.
                                                                                                                                                                            Упражнения  283

                                                                         A)                                                                                                В)
                                                         Данные в пространстве переменных                                                                      Данные в пространстве
                                                                                            Класс 0                                                      обобщенного собственного разложения
                                                                                            Класс 1                                                                                          Класс 0
                                                                                            С1
Ось обобщенного собственного разложения 2




                                                                                                       Ось обобщенного собственного разложения 2
                                                                                                                                                                                             Класс 1
                                                                                            Сl2




                                                      Ось обобщенного собственного разложения 1                                                         Ось обобщенного собственного разложения 1
                                                                                                                                                   С)
                                                                                                  Точность = 96.25 %




                                            Класс 1
Предсказанный класс




                                            Класс 0




                                                                                                      Номер образца

                                                                          Рис. 15.6  Результаты упражнения 15.6

         Упражнение 15.9
         Давайте воспользуемся библиотекой sklearn, чтобы обследовать эффекты
       усадочной регуляризации. Как я писал в главах 12 и 13, вполне очевидно, что
       усадка снизит результативность на тренировочных данных; важный вопрос
       заключается в том, улучшает ли регуляризация точность предсказания на
       не встречавшихся ранее данных (иногда именуемых валидационным, или
       тестовым, набором). Поэтому вам нужно написать исходный код, чтобы реа-
       лизовать тренировочный/тестовый подраздел. Я сделал это путем случайной
       перестановки индексов выборки между 0 и 399, тренировки на первых 350,
   284  Применения собственного и сингулярного разложений

   а затем тестирования на последних 50. Поскольку такого количества образцов
   будет для усреднения мало, я повторил этот случайный отбор 50 раз и взял
   среднюю точность как точность в расчете на величину усадки на рис. 15.8.

                                                                                                   Точность = 96.00 %
                                                                                                                                   Мой LDA
                                                                                                                                   sklearn LDA

                      Класс 1
Предсказанный класс




                      Класс 2




                                                                                                     Номер образца
                                                                                Рис. 15.7  Результаты упражнения 15.8


                                                                                       Влияние усадки на результативность модели
                                Точность предсказания на валидационных данных




                                                                                                   Величина усадки
                                                                                Рис. 15.8  Результаты упражнения 15.9

     Обсуждение: усадка обычно сказывалась отрицательно на валидацион-
   ной результативности. Хотя, похоже, результативность и улучшалась при
   некоторой усадке, многократное повторение исходного кода показало, что
   это были просто случайные колебания. Более глубокое погружение в регуля-
   ризацию больше подходит для отдельной книги по машинному обучению, но
   здесь я хотел подчеркнуть, что многие разработанные в машинном обучении
   «уловки» не обязательно выгодны во всех случаях.
                                                                Упражнения  285


Сингулярное разложение для низкоранговых
аппроксимаций
   Упражнение 15.10
   Игорь Стравинский был одним из величайших композиторов всех времен
(ИМХО) и, безусловно, одним из самых влиятельных в XX веке. Он также сделал
много наводящих на размышления заявлений о природе искусства, средств
массовой информации и критики, включая одну из моих любимых цитат: «Чем
больше искусство ограничено, тем больше оно свободно». Есть знаменитый
и пленительный рисунок Стравинского, написанный не кем иным, как великим
Пабло Пикассо. Изображение этого рисунка доступно в Википедии, и мы будем
работать с ним в следующих нескольких упражнениях. Как и в случае с другими
изображениями, с которыми мы работали в этой книге, изначально это 3D-мат­
ри­ца (640×430×3), но для удобства мы конвертируем ее в оттенки серого (2D).
   Цель этого упражнения – повторить упражнение 14.5, в котором вы воссоз-
дали близкую аппроксимацию сглаженного по шуму изображения, основы-
ваясь на четырех «слоях» сингулярного разложения (пожалуйста, вернитесь
к этому упражнению, чтобы освежить его в памяти). Создайте рисунок, по-
добный рис. 15.9, используя изображение Стравинского. Вот главный вопрос:
дает ли реконструкция изображения по первым четырем компонентам такой
же хороший результат, как в предыдущей главе?

  Упражнение 15.11
  Что ж, ответ на вопрос, поставленный в конце предыдущего упражнения, –
решительное «Нет!». 4-ранговая аппроксимация выглядит просто ужасно!
Она не похожа на изначальное изображение. Цель этого упражнения – рекон-
струировать изображение, используя большее число слоев, чтобы низкоран-
говая аппроксимация стала достаточно точной, а затем вычислить величину
полученного сжатия.
  Начните с создания рис. 15.10, на котором показаны изначальное изобра-
жение, реконструированное изображение и карта ошибок, представляющая
собой квадрат разницы между изначальным изображением и его аппрокси-
мацией. Для этого рисунка я выбрал k = 80 компонент, но рекомендую вам
обследовать другие значения (то есть другие ранговые аппроксимации).
  Далее вычислите коэффициент сжатия, то есть процент числа байтов, ис-
пользуемых в низкоранговой аппроксимации, по сравнению с числом байтов,
используемых в изначальном изображении. Мои результаты для k = 80 показаны
ниже1. Имейте в виду, что при низкоранговой аппроксимации не требуется хра-
нить ни полное изображение, ни полные мат­ри­цы сингулярного разложения!
Оригинал: 2.10 Мб
Реконструкция: 2.10 Мб
Векторы реконструкции: 0.65 Мб (при k=80 компонентах)
Сжатие: 31.13%

1
    Существует некоторая двусмысленность в отношении того, что считать одним
    мегабайтом: 10002 либо 10242 байт; я использовал последнее, но это не влияет на
    степень сжатия.
286  Применения собственного и сингулярного разложений

    Размер матрицы:
       (640, 430),                                         График крутого склона фотографии Стравинского
        Ранг: 430




                      Сингулярное число




                                                                        Индекс компоненты




                                          Рис. 15.9  Результаты упражнения 15.10


             Изначальное                                Реконструированное
             изображение                                    (k = 80/430)                       Квадраты ошибок




                                          Рис. 15.10  Результаты упражнения 15.11
                                                                        Упражнения  287

  Упражнение 15.12
  Почему я выбрал k = 80, а не, например, 70 или 103? Если честно, то этот
выбор был сделан довольно произвольно. Цель этого упражнения – увидеть,
можно ли использовать карту ошибок для определения подходящего пара-
метра ранга.
  В цикле for над рангами реконструкции от 1 до количества сингулярных
чисел создайте низкоранговую аппроксимацию и вычислите расстояние
Фробениуса между оригиналом и -ранговой аппроксимацией. Затем по-
стройте график ошибки как функции от ранга, как показано на рис. 15.11.
Ошибка, безусловно, уменьшается с увеличением ранга, однако нет четкого
ранга, который выглядит самым лучшим. Иногда в алгоритмах оптимизации
более информативной будет производная от функции ошибок; попробуйте!

                                          Точность реконструкции
       Ошибка по оригиналу




                                             Ранг реконструкции
                             Рис. 15.11  Результаты упражнения 15.12

  Заключительная мысль по поводу этого упражнения: ошибка реконструк-
ции при k = 430 (т. е. полном сингулярном разложении) должна равняться 0.
Так ли это? Очевидно, что нет; иначе я бы не поставил вопрос. Но вы должны
убедиться в этом сами. Это еще одна демонстрация ошибок вычислительной
прецизионности в прикладной линейной алгебре.


Сингулярное разложение для шумоподавления
в изображениях
  Упражнение 15.13
  Давайте посмотрим, сможем ли мы расширить концепцию низкоранговой
аппроксимации, чтобы убрать шум из фотографии Стравинского. Цель этого
288  Применения собственного и сингулярного разложений

упражнения – добавить шум и проинспектировать результаты сингулярного
разложения, а затем следующее упражнение будет содержать «редуцирова-
ние» искажений.
  Шумом здесь будет пространственная синусоидная волна. На рис. 15.12
показаны шум и искаженное изображение.

      Изначальный фотоснимок          Изображение шума         Загрязненный фотоснимок




                      Рис. 15.12  Подготовка к упражнению 15.13

  Теперь я опишу, как создавать двумерную синусоидную волну (также име-
нуемую синусоидной обрешеткой). Это хорошая возможность попрактико-
ваться в навыках переложения математики в исходный код. Формула дву-
мерной синусоидной обрешетки такова:

  Z = sin(2πf (Xcos(θ) + Ysin(θ))).

  В приведенной выше формуле f – это частота синусоиды, θ – параметр
поворота, а π – константа 3.14… . X и Y – это местоположения на решет-
ке, в которых вычисляется функция; я задал их как целые числа от −100 до
100 с числом шагов, установленным в соответствии с размером фотографии
Стравинского. Я установил f = 0.02 и θ = π/6.
  Прежде чем перейти к остальной части упражнения, рекомендую вам по-
тратить некоторое время на исходный код синусоидной обрешетки путем
обследования влияния изменений параметров на результирующее изобра-
жение. Тем не менее, пожалуйста, используйте параметры, которые я на-
писал ранее, чтобы воспроизвести мои результаты, которые показаны ниже.
  Затем испортите фотографию Стравинского, добавив в изображение шум. Вы
должны сначала прошкалировать шум в диапазон от 0 до 1, затем сложить шум
и изначальную фотографию, а потом перешкалировать. Шкалирование изо-
бражения между 0 и 1 достигается применением следующей ниже формулы:
                                                                                            Упражнения  289

  Хорошо, теперь у вас есть искаженное шумом изображение. Воспроизве-
дите рис. 15.13, который аналогичен рис. 15.9, но с использованием зашум-
ленного изображения.

     Размер мат­ри­цы:                                          График крутого склона зашумленной
        (640, 430),                                                  фотографии Стравинского
         Ранг: 430
                         Сингулярное число




                                                                        Индекс компоненты




                                             Рис. 15.13  Результаты упражнения 15.13

  Обсуждение: интересно сравнить рис. 15.13 с рис. 15.9. Хотя мы созда-
ли шум на основе одного признака (синусоидная обрешетка), сингулярное
разложение разделило обрешетку на две компоненты равной важности
(примерно равные сингулярные числа)1. Эти две компоненты не являются

1
    Вероятно, это объясняется тем, что мы имеем двумерную сингулярную плоскость,
    а не пару сингулярных векторов; базисными векторами могли быть любые два
    линейно независимых вектора на этой плоскости, и Python выбрал ортогональную
    пару.
290  Применения собственного и сингулярного разложений

синусоидными обрешетками, а представляют собой вертикально ориенти-
рованные заплатки. Однако их сумма дает диагональные полосы обрешетки.

  Упражнение 15.14
  Теперь обратимся к шумоподавлению. Похоже, что шум содержится во
второй и третьей компонентах, поэтому теперь ваша цель – реконструиро-
вать изображение, используя все компоненты, кроме этих двух. Создайте
рисунок, как на рис. 15.14.

    Зашумленное изображение          Только шум (comps[1,2])       Шум редуцирован




                        Рис. 15.14  Результаты упражнения 15.14

   Обсуждение: шумоподавление выглядит достойно, но, конечно, не иде-
ально. Одна из причин несовершенства заключается в том, что шум содер-
жится в двух измерениях не полностью (обратите внимание, что средняя
часть рис. 15.14 не полностью совпадает с изображением шума). Более того,
редукция шума (изображение, составленное из компонент 1 и 2) имеет от-
рицательные значения и распределена вокруг нуля, хотя синусоидная обре-
шетка не имела отрицательных значений. (Это можно подтвердить, построив
гистограмму изображения шума, которую я показываю в онлайновом исход-
ном коде.) Остальная часть изображения должна иметь колебания значений
с учетом этого, чтобы полная реконструкция имела только положительные
значения.
Глава           16
                    Краткое руководство
                        по языку Python

Как я объяснял в главе 1, эта глава представляет собой ускоренный курс по
программированию на Python. Он предназначен для того, чтобы быстро ос-
воиться и прослеживать исходный код в остальной части книги, но не для
использования в качестве полного источника для овладения языком Python.
Если вы ищете специальную книгу по Python, то рекомендую «Освоение
языка Python» Марка Лутца (O’Reilly)1.
  При работе с данной главой откройте сеанс Python. (Я объясню, как это
делать, позже.) Вы не освоите язык Python, просто прочитав эту главу; вам
нужно читать, набирать исходный код Python на клавиватуре, изменять его,
тестировать и т. д.
  Кроме того, в данной главе вы должны набирать вручную весь исходный
код, который вы видите напечатанным ниже. Исходный код всех остальных
глав этой книги доступен в интернете, но я хотел бы, чтобы вы набирали
исходный код этой главы вручную. Когда вы знакомы с программировани-
ем на Python, ввод большого объема исходного кода вручную представляет
утомительную трату времени. Но когда вы только учитесь программировать,
то вам просто необходимо программировать, то есть использовать пальцы,
чтобы набирать все подряд, а не просто смотреть на исходный код, напеча-
танный на странице.



Почему Python и какие есть альтернативы?
Python разработан как язык программирования общего назначения. Его мож-
но использовать для анализа текста, обработки веб-форм, создания алгорит-
мов и множества других применений. Python также широко используется
в науке о данных и машинном обучении; в этих приложениях Python – в ос-
новном просто калькулятор. Но чрезвычайно мощный и универсальный каль-

1
    См. Learning Python, https://oreil.ly/learning-python. – Прим. перев.
292  Краткое руководство по языку Python

кулятор, и мы используем Python, потому что мы (люди) недостаточно умны,
чтобы выполнять все числовые вычисления в уме или с ручкой и бумагой.
  Python в настоящее время (в 2022 году) является наиболее часто исполь-
зуемой программой числовой обработки в науке о данных (другие претен-
денты включают R, MATLAB, Julia, JavaScript, SQL и С). Останется ли Python
доминирующим языком науки о данных? Понятия не имею, но сомневаюсь.
История информатики полна «финальных языков», которые якобы будут
существовать в веках. (Вы когда-нибудь программировали на FORTRAN,
COBOL, IDL, Pascal и т. д.?) Но Python сейчас очень популярен, и сейчас вы
изучаете прикладную линейную алгебру. В любом случае, хорошая новость
заключается в том, что языки программирования обладают хорошей перено-
симостью знаний, а это означает, что владение Python поможет вам изучить
другие языки. Иными словами, время, потраченное на изучение Python, бу-
дет потрачено не зря.



Интерактивные среды разработки
Python – это язык программирования, и вы можете выполнять Python в са-
мых разных приложениях, именуемых средами. Разные среды создаются
разными разработчиками с разными предпочтениями и потребностями. Не-
которые распространенные интерактивные среды разработки (IDE)1, с ко-
торыми вы можете столкнуться, включают Visual Studio, Spyder, PyCharm
и Eclipse. Возможно, наиболее часто используемая IDE для изучения Python
называется блокнотами Jupyter.
  Исходный код к этой книге был написан с использованием среды Google
Colab Jupyter (подробнее об этом – в следующем разделе). После того как вы
познакомитесь с работой на Python на основе Jupyter, вы можете потратить
некоторое время на опробование других IDE, чтобы увидеть, насколько они
лучше соответствуют вашим потребностям и предпочтениям. Тем не менее
здесь я рекомендую использовать Jupyter, потому что эта среда поможет вам
прослеживать исходный код и воспроизводить рисунки.



Использование Python локально и онлайн
Поскольку Python является бесплатным и легковесным языком, его можно
выполнять в самых разных операционных системах, как на вашем компью-
тере, так и на облачном сервере:

Работа с исходным кодом Python локально
  Вы можете установить Python в любую главенствующую операционную
  систему (Windows, Mac, Linux). Если вы освоились с установкой программ

1
    Англ. Interactive Development Environment. – Прим. перев.
                                  Использование Python локально и онлайн  293

  и пакетов, то можете устанавливать библиотеки по мере необходимости.
  В этой книге вам в основном понадобятся библиотеки NumPy, matplotlib,
  SciPy и sympy.
  Но если вы читаете эту главу, то ваши навыки Python, вероятно, ограни-
  чены. В этом случае я рекомендую установить Python посредством про-
  граммного пакета Anaconda. Этот пакет бесплатен и прост в установке,
  и Anaconda автоматически установит все библиотеки, которые вам пона-
  добятся в этой книге.

Работа с исходным кодом Python онлайн
  Изучая эту книгу, я рекомендую выполнять Python в интернете. Преимущест­
  ва облачного Python заключаются в том, что не нужно ничего устанавливать
  локально, не нужно использовать собственные вычислительные ресурсы,
  и вы можете обращаться к своему исходному коду из любого браузера на
  любом компьютере и в любой операционной системе. Я предпочитаю среду
  Google Colaboratory, потому что она синхронизируется с моим Google Дис-
  ком. За счет этого я могу хранить файлы исходного кода Python на моем
  Google Диске, а затем открывать их по адресу https://colab.research.google.
  com. Если вы избегаете сервисов Google, то можно использовать и другие
  облачные среды Python (хотя я не уверен, что это вообще возможно).
  Google Colab можно использовать бесплатно. Вам понадобится учетная
  запись­Google, чтобы получать к ней доступ, но это тоже бесплатно. И тогда
  вы сможете просто закачивать файлы исходного кода на свой Google Диск
  и открывать их в Colab.


Работа с файлами исходного кода в Google Colab
Теперь я объясню, как скачивать и получать доступ к файлам блокнотов
Python этой книги. Как я уже писал ранее, к этой главе нет файлов исходного
кода.
  Есть два способа переноса исходного кода книги на Google Диск:
    перейти на https://github.com/mikexcohen/LinAlg4DataScience, кликнуть
      по зеленой кнопке с надписью Code (Исходный код), а затем кликнуть
      Download ZIP (Скачать ZIP) (рис. 16.1). В результате репозиторий ис-
      ходного кода будет скачан, и вы сможете закачать эти файлы на свой
      Google Диск. Теперь на вашем Google Диске можно дважды кликнуть
      по файлу либо кликнуть правой кнопкой мыши и выбрать Open with
      (Открыть с помощью), а затем «Google Colaboratory»;
    перейти непосредственно на https://colab.research.google.com, выбрать
      вкладку GitHub и выполнить поиск по «mikexcohen» в поисковой стро-
      ке. Вы найдете все мои общедоступные репозитории GitHub; вам ну-
      жен тот, который называется «LinAlg4DataScience». Оттуда вы можете
      кликнуть по одному из файлов, чтобы открыть блокнот.
  Обратите внимание, что это копия записной книжки, предназначенная
только для чтения; любые сделанные вами изменения сохранены не будут.
Поэтому рекомендую скопировать этот файл на свой Google Диск.
294  Краткое руководство по языку Python




          Рис. 16.1  Перенос исходного кода из GitHub (слева) в Colab (справа)

   Теперь, когда вы знаете, как импортировать файлы исходного кода кни-
ги в Google Colab, самое время создать новый блокнот, чтобы начать ра-
боту с этой главой. Кликните по пункту меню File (Файл), а затем по New
Notebook (Новый блокнот), чтобы создать новый блокнот. Он будет назы-
ваться «Untitled1.ipynb» или что-то подобное. (Расширение ipynb означает
«интерактивный блокнот Python», от англ. interactive Python notebook.) Я ре-
комендую изменить имя файла, кликнув на имени файла в верхнем левом
углу экрана. По умолчанию новые файлы помещаются на ваш Google Диск
в папку Colab Notebooks (Блокноты Colab).



Переменные
Язык Python можно использовать в качестве калькулятора. Давайте попро-
буем; наберите следующее в ячейку исходного кода:
4 + 5.6

  После набора этого фрагмента исходного кода ничего не произойдет. Вам
нужно сообщить Python, что нужно его исполнить. Это делается путем на-
жатия сочетания клавиш Ctrl-Enter (Command-Enter в Mac) на клавиатуре,
когда эта ячейка активна (ячейка исходного кода является активной, если вы
видите, что курсор мигает внутри ячейки). Существуют также опции меню
для исполнения исходного кода в ячейке, но программирование становится
проще и быстрее при использовании сочетаний клавиш.
  Найдите минутку, чтобы обследовать арифметику. Для группировки мож-
но использовать разные числа, круглые скобки и различные операции, такие
как -, / и *. Также обратите внимание, что интервал не влияет на результат:
2*3 – это то же самое, что и 2 * 3. (Интервал важен для других аспектов про-
граммирования Python, и мы вернемся к этому позже.)
  Работа с отдельными числами в приложениях не масштабируется. Имен-
но поэтому мы пользуемся переменными. Переменная – это имя, которое
ссылается на данные, хранящиеся в памяти. Это аналогично использованию
                                                                     Переменные  295

слов в естественных языках для обозначения объектов в реальном мире.
Например, меня зовут Майк, и я человек, состоящий из триллионов клеток,
которые каким-то образом способны ходить, говорить, есть, мечтать, рас-
сказывать плохие шутки и делать множество других вещей. Но это слишком
сложно объяснить, поэтому для удобства люди называют меня «Майк Икс
Коэн». Итак, переменная в Python – это просто удобная ссылка на хранимые
данные, такие как число, изображение, база данных и т. д.
  Мы создаем переменные в Python, присваивая им значение. Наберите
следующее:
var1 = 10
var2 = 20.4
var3 = 'привет, меня зовут Майк'

  Выполнение этой ячейки создаст переменные. Теперь вы можете начать
их использовать! Например, в новой ячейке исполните следующий ниже
фрагмент исходного кода:
var1 + var2

>> 30.4

     Результаты
     Знак >>, который вы видите в блоках исходного кода, является результатом испол-
     нения ячейки исходного кода. Последующий текст – это то, что вы видите на экране,
     когда вычисляете исходный код в ячейке.

  Теперь попробуйте следующее:
var1 + var3

  А-а-а, вы только что получили свою первую ошибку Python! Добро пожа-
ловать в клуб :) Не волнуйтесь, ошибки программирования очень распро-
странены. На самом деле разница между хорошим программистом и плохим
состоит в том, что хорошие программисты учатся на своих ошибках, тогда
как плохие программисты думают, что хорошие программисты никогда не
ошибаются.
  Ошибки в Python бывает трудно понять. Ниже приведено сообщение об
ошибке на моем экране:
TypeError                        Traceback (most recent call last)
<ipython-input-3-79613d4a2a16>   in <module>()
      3 var3 = 'hello, my name   is Mike'
      4
----> 5 var1 + var3
TypeError: unsupported operand   type(s) for +: 'int' and 'str'

  Python указывает стрелкой ошибочную строку. Сообщение об ошибке, ко-
торое, как мы надеемся, поможет нам понять, что именно пошло не так и как
это исправить, напечатано внизу. В этом случае сообщением об ошибке яв-
ляется TypeError (Ошибка типа). Что это значит и что такое «типы»?
296  Краткое руководство по языку Python


Типы данных
Оказывается, у переменных есть типы, описывающие тип данных, которые
эти переменные хранят. Наличие разных типов делает вычисления эффек-
тивнее, поскольку операции работают по-разному с разными типами данных.
  В Python существует много типов данных. Здесь я представлю четыре из
них, и по мере чтения книги вы узнаете больше о других типах данных:

Целочисленный
  Они называются int и представляют собой целые числа, такие как −3, 0,
  10 и 1234.

Вещественный
  Они называются float, но это всего лишь причудливый термин для чисел
  с (плавающей) десятичной точкой, таких как –3.1, 0.12345 и 12.34. Имейте
  в виду, что числа с плавающей точкой и целые могут выглядеть одинаково
  для нас, людей, но функции Python обрабатывают их по-разному. Напри-
  мер, 3 – это целое число, а 3.0 – это число с плавающей точкой.

Строковый
  Они называются str и являются текстом. Здесь также помните о различии
  между 5 (строковым значением, соответствующим символу 5) и 5 (int, со-
  ответствующим числу 5).

Списковый
  Список – это коллекция элементов, каждый из которых может иметь раз-
  ный тип данных.

  Списки очень удобны и повсеместно используются в программировании
на Python. Следующий ниже фрагмент исходного кода иллюстрирует три
важные особенности списков: (1) они обозначаются квадратными скобками
[ ], (2) запятые отделяют элементы списка и (3) отдельные элементы списка
могут иметь разные типы данных:
list1 = [ 1,2,3 ]
list2 = [ 'hello', 123.2, [3, 'qwerty'] ]

   Второй список показывает, что списки могут содержать другие списки.
Иными словами, третий элемент списка list2 сам по себе является списком.
   Что делать, если вы хотите обратиться только ко второму элементу списка
list2? Отдельные элементы списка извлекаются с помощью индексации,
о которой я расскажу в следующем разделе.
   Тип данных определяется с помощью функции type. Например, вычислите
следующее в новой ячейке:
type(var1)

  Эй, подождите, а что такое «функция»? Вы, возможно, с нетерпением хо-
тите узнать о применении и создании функций, но сначала я хотел бы об-
ратиться к индексации.
                                                                         Функции  297

       Как называть свои переменные?
       Есть несколько жестких правил именования переменных. Имена переменных нельзя
       начинать с цифр (хотя они могут содержать цифры), а также нельзя включать пробелы
       или не буквенно-цифровые символы, такие как !@#$%^&*(). Символы подчеркивания
       _ разрешены.
       Существуют также рекомендации по именованию переменных. Самое важное прави-
       ло – делать имена переменных осмысленными и интерпретируемыми. Например, имя
       rawDataMatrix гораздо лучше, чем q. В своем исходном коде можно создавать десятки
       переменных, и совсем не помешает возможность понимать данные, на которые пере-
       менная ссылается, по ее имени.



Индексация
Индексация означает доступ к определенному элементу в списке (и свя-
занным типам данных, включая векторы и мат­ри­цы). Вот как извлекается
второй элемент списка:
aList = [ 19,3,4 ]
aList[1]

>> 3

   Обратите внимание, что индексация выполняется с использованием квад­
ратных скобок после имени переменной, а затем числа, которое вы хотите
индексировать.
   Но постойте: я написал, что нам нужен второй элемент; почему исходный
код обращается к элементу 1? Это не опечатка! В языке Python индексация
начинается с 0, то есть индекс 0 – это первый элемент (в данном случае число
19), индекс 1 – второй элемент и т. д.
   Если вы новичок в языках программирования, в которых применяется от-
счет от 0, то, возможно, это покажется странным и запутанным. Полностью
сочувствую. Хотел бы написать, что это станет второй натурой после некото-
рой практики, но правда в том, что индексация с отсчетом от 0 всегда будет
источником путаницы и ошибок. Это просто то, о чем нужно всегда помнить.
   Как обратиться к числу 4 в списке aList? Его можно индексировать напря-
мую как aList[2]. Но индексация Python имеет удобную функцию, благодаря
которой можно индексировать элементы списка в обратном порядке. Для
того чтобы обратиться к последнему элементу списка, наберите aList[-1].
-1 можно трактовать как отсчет по кругу от конца списка. Аналогичным об-
разом предпоследним элементом списка будет aList[-2] и т. д.



Функции
Функция – это фрагмент исходного кода, который можно выполнять без не-
обходимости снова и снова набирать все отдельные его элементы. Некото-
298  Краткое руководство по языку Python

рые функции – короткие и состоят всего из нескольких строк кода, тогда как
другие состоят из сотен или тысяч строк кода.
  Функции обозначаются в Python с помощью круглых скобок сразу после
имени функции. Вот несколько общих функций:
type() # возвращает тип данных
print() # печатает текстовую информацию в блокнот
sum() # складывает числа


                                     Комментарии
  Комментарии – это куски кода, которые Python игнорирует. Комментарии помогают вам
  и другим интерпретировать и понимать исходный код. Комментарии в Python обозна-
  чаются символом #. Любой текст после # игнорируется. Комментарии могут находиться
  в отдельных строках либо справа от фрагмента исходного кода.


  Функции могут принимать входные данные и могут предоставлять выход-
ные данные. Общая анатомия функции Python выглядит так:
output1, output2 = functionname(input1, input2, input3)

  Возвращаясь к предыдущим функциям:
dtype = type(var1)
print(var1+var2)
total = sum([1,3,5,4])

>> 30.4

  print() – очень полезная функция. Python выводит результат только по-
следней строки в ячейке и только тогда, когда эта строка не содержит при-
сваивание значения переменной. Например, напишите следующий ниже
фрагмент исходного кода:
var1+var2
total = var1+var2
print(var1+var2)
newvar = 10

>> 30.4

  Приведенный выше исходный код состоит из четырех строк, поэтому
можно было ожидать, что Python выдаст четыре результата. Но выдается
только один результат, который соответствует функции print(). Первые две
строки не выводят свой результат, потому что они не являются последней
строкой, а последняя строка – потому что это присваивание значения пере-
менной.
                                                          Функции  299


Методы в качестве функций
Метод – это функция, которая вызывается непосредственно на переменной.
Различные типы данных имеют разные методы, а это означает, что метод,
работающий на списках, не будет работать на строковых литералах.
  Например, списковый тип данных имеет метод append, который добавляет
дополнительный элемент в существующий список. Вот пример:
aSmallList = [ 'one','more' ]
print(aSmallList)

aSmallList.append( 'time' )
print(aSmallList)

>> ['one','more']
['one','more','time']

  Обратите внимание на форматирование синтаксиса: методы похожи на
функции тем, что они имеют круглые скобки, и (у некоторых методов) есть
входные аргументы. Но методы присоединяются к имени переменной точ-
кой и могут изменять переменную напрямую без выдачи явного результата.
  Потратьте немного времени, чтобы изменить исходный код, используя
другой тип данных, например строковое значение вместо списка. Повтор-
ное выполнение исходного кода приведет к следующему ниже сообщению
об ошибке:
AttributeError: 'str' object has no attribute 'append'

  Это сообщение об ошибке означает, что строковый тип данных распозна-
ет функцию append (атрибут – это свойство переменной, а метод – один из
таких атрибутов).
  Методы являются стержневой частью объектно-ориентированного про-
граммирования и классов. Этим аспектам Python можно было бы посвятить
отдельную книгу по Python, но не беспокойтесь – для того чтобы изучать
линейную алгебру с помощью этой книги, полное понимание объектно-ори-
ентированного программирования вам не потребуется.


Написание своих собственных функций
В Python имеется много функций. Слишком много, чтобы сосчитать. Но ни-
когда не найдется той идеальной функции, которая делает именно то, что
вам нужно. И в итоге вам придется писать свои собственные функции.
  Создавать собственные функции легко и удобно; для определения функ-
ции используется встроенное ключевое слово def (ключевое слово – это за-
резервированное имя, которое нельзя переопределять как переменную или
300  Краткое руководство по языку Python

функцию), затем указывается имя функции и любые возможные входные
данные, и строка заканчивается двоеточием.
  Любые строки кода после этого включаются в функцию, если они отделены
двумя пробелами1. Python очень требователен к отступам в начале строки
(но не к отступам в других частях строки). Любые результаты обозначаются
ключевым словом return.
  Начнем с простого примера:
def add2numbers(n1, n2):
  total = n1+n2
  print(total)
  return total

  Эта функция принимает два входных аргумента и вычисляет, печатает
и выводит их сумму. Теперь пришло время вызвать функцию:
s = add2numbers(4,5)
print(s)

>> 9
9

  Почему число 9 появилось дважды? Оно было напечатано один раз, по-
тому что функция print() была вызвана внутри функции, а затем оно было
напечатано во второй раз, когда я вызвал print(s) после функции. Для того
чтобы в этом убедиться, попробуйте поменять строку после вызова функции
на print(s+1). (Видоизменение исходного кода, чтобы увидеть результат на
выходе, – отличный способ изучить Python; только не забывайте отменять
свои изменения.)
  Обратите внимание, что имя переменной, назначенное для вывода внутри
функции (total), может отличаться от имени переменной, которое я исполь-
зовал при вызове функции (s).
  Написание конкретно-прикладных функций обеспечивает бóльшую гиб-
кость, например установка дополнительных входных данных и используемых
по умолчанию параметров, проверка входных данных на тип данных и согласо-
ванность и т. д. Но в этой книге базового понимания функций будет достаточно.


                                 Когда писать функции?
    Если у вас есть строки исходного кода, которые нужно выполнять десятки, сотни или, мо-
    жет быть, миллиарды раз, то написание функции – это, безусловно, правильный путь. Не-
    которым людям действительно нравится писать функции, и они будут писать специальные
    функции, даже если они вызываются только один раз.
    Я предпочитаю создавать функцию только тогда, когда она будет вызываться несколько
    раз в разных контекстах или частях исходного кода. Но вы являетесь хозяином своего
    собственного исходного кода, и по мере накопления опыта программирования вы смо-
    жете выбирать ситуации, когда помещать исходный код в функции.


1
    В некоторых IDE допускается два или четыре пробела; в других принимается толь-
    ко четыре пробела. Я считаю, что два пробела выглядят чище.
                                                                   Функции  301


Библиотеки
Python сконструирован таким образом, чтобы его можно было легко и быстро
устанавливать и выполнять. Но недостатком является то, что базовый пакет
Python поставляется с малым числом встроенных функций.
   Поэтому разработчики создают коллекции функций, ориентированных на
определенную тему, и такие коллекции называются библиотеками. После
того как вы импортируете библиотеку в Python, вы можете обращаться ко
всем функциям, типам переменных и методам, доступным в этой библиотеке.
   Согласно результатам поиска в Google, существует более 130 000 библио­
тек Python. Не волнуйтесь, вам не нужно запоминать их все! В этой книге
мы будем использовать лишь несколько библиотек, предназначенных для
числовой обработки и визуализации данных. Самая важная библиотека для
линейной алгебры называется NumPy; ее название состоит из комбинации
слов numerical Python (численный Python).
   Библиотеки Python не входят в базовую установку Python, а это значит, что
вам нужно скачать их из интернета, а затем импортировать в Python. За счет
этого обеспечивается их доступность для использования внутри Python. Их
нужно скачивать только один раз, но импортировать их придется повторно
в каждом сеансе Python1.


NumPy
Для того чтобы импортировать библиотеку NumPy в Python, наберите:
import numpy as np

  Обратите внимание на общую формулировку импорта библиотек: import
libraryname as abbreviation (импортировать имя библиотеки как аббревиа-
туру). Аббревиатура является удобным сокращением. Для получения досту-
па к функциям в библиотеке пишется сокращенное имя библиотеки, точка
и имя функции. Например:
average = np.mean([1,2,3])
sorted1 = np.sort([2,1,3])
theRank = np.linalg.matrix_rank([[1,2],[1,3]])

  Третья строка исходного кода показывает, что библиотеки могут иметь
вложенные в них подбиблиотеки или модули. В данном случае у NumPy есть
много функций, а внутри NumPy есть библиотека под названием linalg, ко-
торая содержит свои функции, специально связанные с линейной алгеброй.
  NumPy имеет собственный тип данных под названием массив NumPy. Мас-
сивы NumPy изначально кажутся похожими на списки в том смысле, что

1
    Если вы установили Python через Anaconda или используете среду Google Colab, то
    скачивать какие-либо библиотеки для этой книги не нужно, но вам нужно будет
    их импортировать.
302  Краткое руководство по языку Python

в них обоих хранятся коллекции информации. Но в массивах NumPy хра-
нятся только числа, и у них есть атрибуты, удобные для математического
программирования. В следующем ниже фрагменте исходного кода показано,
как создавать массив NumPy:
vector = np.array([ 9,8,1,2 ])



Индексация и нарезка в NumPy
Хотел бы вернуться к теме доступа к одному элементу внутри переменной.
К одному элементу массива NumPy можно обращаться с помощью индекса-
ции точно так же, как индексируется список. В следующем ниже блоке кода
я использую функцию np.arange, чтобы создать массив целых чисел от −4
до 4. Это не опечатка в коде – второй аргумент равен +5, но возвращаемые
значения заканчиваются на 4. В Python часто используются эксклительные
верхние границы; это означает, что Python считает до указанного вами ко-
нечного числа, но не включает его:
ary = np.arange(-4,5)
print(ary)
print(ary[5])

>> [-4 -3 -2 -1 0 1 2 3 4]
1

   Это все хорошо, но что, если вы хотите получить доступ, например, к пер-
вым трем элементам? Или каждому второму элементу? Тут самое время
перейти от индексации к нарезке.
   Нарезка работает просто: надо указать начальный и конечный индексы
с двоеточием между ними. Просто помните, что диапазоны Python имеют
эксклительные верхние границы. Таким образом, для того чтобы получить
первые три элемента массива, мы нарезаем до индекса 3 + 1 = 4, но тогда
нам нужно учитывать индексацию на основе 0, то есть первые три элемента
имеют индексы 0, 1 и 2, и мы нарезаем их, используя 0:3:
ary[0:3]

>> array([-4, -3, -2])

  Каждый второй элемент проиндексируется с помощью оператора про-
пуска:
ary[0:5:2]

>> array([-4, -2, 0])

  Формулировка индексации с пропуском такова: [начало:конец:пропуск].
Весь массив можно просматривать в обратном порядке, указывая пропуск,
равный −1, например вот так: ary[::-1].
                                                                   Визуализация  303

  Знаю, это немного сбивает с толку, но обещаю: на практике все будет легче.



Визуализация
Многие концепции линейной алгебры и большинства других областей мате-
матики лучше всего понять, увидев их на экране компьютера.
  Большинство визуализаций данных на Python обрабатываются библио-
текой matplotlib. Некоторые аспекты графических отображений зависят от
интерактивной среды разработки. Однако весь исходный код в этой книге
работает как есть в любой среде Jupyter (посредством Google Colab, другого
облачного сервера или локально). Если вы используете другую интерактив-
ную среду разработки, то вам может потребоваться внести несколько незна-
чительных изменений.
  Вводить matplotlib.pyplot становится очень утомительно, поэтому назва-
ние этой библиотеки часто сокращают до plt. Вы увидите это в следующем
ниже блоке кода.
  Начнем с рисования точек и линий. Посмотрите, сможете ли вы понять,
как следующий ниже фрагмент исходного кода отображается на рис. 16.2:
import matplotlib.pyplot as plt
import numpy as np

plt.plot(1,2,'ko')            #   1)   начерить черный кружок
plt.plot([0,2],[0,4],'r--')   #   2)   начерить линию
plt.xlim([-4,4])              #   3)   задать пределы оси x
plt.ylim([-4,4])              #   4)   задать пределы оси y
plt.title('Имя графика')      #   5)   указать имя графика


                                                Имя графика




                        Рис. 16.2  Визуализация данных, часть 1

  Удалось расшифровать исходный код? Строка кода № 1 говорит, что нужно
нарисовать черный кружок (ko – k означает черный, а o – кружок) в местопо-
304  Краткое руководство по языку Python

ложении xy 1,2. Строка кода № 2 предоставляет списки чисел вместо отдель-
ных чисел. В ней определяется линия, которая начинается в координате xy
(0, 0) и заканчивается в координате (2, 4). r-- обозначает красную пунктир-
ную линию. Строки кода № 3 и № 4 устанавливают ограничения по осям x
и y, и, конечно же, строка № 5 создает заголовок.
   Прежде чем двигаться дальше, уделите немного времени обследованию
этого исходного кода. Нарисуйте дополнительные точки и линии, попро-
буйте разные маркеры (подсказка: попробуйте буквы о, с и р) и разные цвета
(попробуйте r, k, b, y, g и m).
   В следующем ниже блоке кода представлены графики и изображения. Под-
график – это способ разделения графической области (именуемой figure,
то есть рисунком) на решетку отдельных осей, по которым можно рисовать
разные визуализации. Как и в случае с предыдущим блоком кода, прежде
чем читать мое описание, проверьте свое понимание процедуры создания
рис. 16.3 приведенным ниже фрагментом исходного кода:
_,axs = plt.subplots(1,2,figsize=(8,5)) # 1) создать подграфики
axs[0].plot(np.random.randn(10,5))      # 2) линейный график слева
axs[1].imshow(np.random.randn(10,5))    # 3) изображение справа




                        Рис. 16.3  Визуализация данных, часть 2

   Строка кода № 1 создает подграфики. Первые два аргумента функции plt.
subplots определяют геометрию решетки – в данном случае это мат­ри­ца под-
графиков 1×2, то есть одна строка и два столбца, что означает два графика бок
о бок. Первый аргумент задает общий размер рисунка, причем два элемента
в этом кортеже соответствуют ширине, а затем высоте в дюймах. (Размеры
всегда указываются как ширина, высота. Мнемоника для запоминания по-
рядка – WH как в «White House».) Функция plt.subplots предоставляет два
результата. Первый – это дескриптор всего рисунка, который нам не нужен,
поэтому вместо имени переменной я использовал подчеркивание. Второй
                                     Переложение формул в исходный код  305

результат – это массив NumPy, содержащий дескрипторы каждой оси. Де-
скриптор – это особый тип переменной, которая указывает на объект на
рисунке.
  Теперь о строке кода № 2. Это должно быть знакомо по предыдущему блоку
кода; два новых понятия – построение графика на определенной оси вместо
всего рисунка (с использованием plt.). И передача на вход мат­ри­цы вместо
отдельных чисел. Python создает отдельную строку для каждого столбца мат­
ри­цы, поэтому на рис. 16.3 вы видите пять линий.
  Наконец, строка кода № 3 показывает, как создавать изображение. Как
вы узнали из главы 5, мат­ри­цы нередко визуализируются как изображения.
Цвет каждого маленького блока в изображении соотносится с числовым зна-
чением в мат­ри­це.
  Что ж, о создании графики на Python можно было бы сказать гораздо боль-
ше. Но я надеюсь, что этого введения будет достаточно, чтобы начать работу.



Переложение формул в исходный код
Переложение математических уравнений в исходный код Python иногда не
представляет никаких трудностей, а иногда сопряжено со сложностями. Но
это важный навык, и вы улучшите его с практикой. Начнем с простого при-
мера в уравнении 16.1.

  Уравнение 16.1. Уравнение

  y = x 2.

  Возможно, вы подумаете, что следующий ниже фрагмент исходного кода
будет работать:
y = x**2

  Но вы получите сообщение об ошибке (NameError: name x is not defined).
Проблема в том, что мы пытаемся использовать переменную x до ее опре-
деления. Так как же определять x? На самом деле, когда вы смотрите на ма-
тематическое уравнение, вы определяете x, даже не задумываясь об этом:
x начинается в отрицательной бесконечности и уходит в положительную
бесконечность. Но вы не очерчиваете функцию так далеко – для построе-
ния этой функции вы, вероятно, выбрали бы ограниченный диапазон, воз-
можно от −4 до +4. И этот диапазон – именно то, что необходимо указать
на Python:
x = np.arange(-4,5)
y = x**2

  На рис. 16.4 показан график функции, созданный с использованием plt.
plot(x,y,'s-').
306  Краткое руководство по языку Python




                           Рис. 16.4  Визуализация данных, часть 3

  Выглядит неплохо, но я думаю, что он получился слишком прерывистым;
хотелось бы, чтобы линия выглядела глаже. Этого можно добиться, увеличив
разрешающую способность, то есть увеличив число точек в диапазоне от −4
до +4. Я буду использовать функцию np.linspace(), которая принимает три
аргумента: начальное значение, конечное значение и число промежуточных
точек:
x = np.linspace(-4,4,42)
y = x**2
plt.plot(x,y,'s-')

  Теперь у нас 42 точки, расположенные линейно (равномерно) между −4
и +4. В результате график делается более гладким (рис. 16.5). Обратите вни-
мание, что функция np.linspace выводит вектор, оканчивающийся на +4.
Эта функция имеет инклюзивные границы. Немного дезориентирует, что
приходится запоминать, какие функции имеют инклюзивные, а какие – экс-
клюзивные границы. Не волнуйтесь, вы с этим справитесь.




                           Рис. 16.5  Визуализация данных, часть 4
                                          Переложение формул в исходный код  307

  Давайте попробуем еще один перевод функции в исходный код. Я также
воспользуюсь этой возможностью, чтобы познакомить вас с концепцией,
именуемой мягким программированием, которая означает создание пере-
менных для параметров, которые вы, возможно, захотите изменить позже.
  Пожалуйста, перед тем как смотреть на мой следующий ниже фрагмент ис-
ходного кода, переведите следующую математическую функцию в исходный
код и создайте график:




      α = 1.4;
      β = 2.

  Эта функция называется сигмоидой и часто используется в прикладной
математике, например как нелинейная активационная функция в моделях
глубокого обучения. α и β – это параметры уравнения. Здесь я установил
для них определенные значения. Но после того, как ваш исходный код за-
работает, вы сможете обследовать влияние изменения этих параметров на
результирующий график. На самом деле использование исходного кода для
понимания математики – это, ИМХО1, самый лучший способ изучения ма-
тематики.
  Эту функцию можно запрограммировать двумя способами. Один из них
заключается в помещении числовых значений α и β непосредственно в функ-
цию. Это пример жесткого программирования, поскольку значения парамет­
ров реализуются в функции напрямую.
  Альтернативой является установка переменных Python равными этим
двум параметрам, а затем использование этих параметров при создании
математической функции. Это и есть мягкое программирование, и оно упро-
щает чтение, видоизменение и отладку исходного кода:
x = np.linspace(-4,4,42)
alpha = 1.4
beta = 2

num = alpha               # числитель
den = 1 + np.exp(-beta*x) # знаменатель
fx = num / den
plt.plot(x,fx,'s-')

   Обратите внимание, что я разделил создание функции на три строки кода,
которые определяют числитель и знаменатель, а затем их отношение. Это
делает исходный код чище, и его легче читать. Всегда стремитесь к тому,
чтобы исходный код легко читался, потому что это (1) снижает риск ошибок
и (2) облегчает отладку.


1
    Мне сказали, что ИМХО – это тысячелетний жаргонизм от англ. in my humble opinion
    (по моему скромному мнению).
308  Краткое руководство по языку Python

  На рис. 16.6 показана результирующая сигмоида. Потратьте несколько
минут, чтобы поэкспериментировать с исходным кодом: измените пределы
и разрешающую способность переменной x, измените значения параметров
alpha и beta, возможно, даже измените саму функцию. Математика прекрас-
на, Python – это ваш холст, а исходный код – ваша кисть!




                        Рис. 16.6  Визуализация данных, часть 5




Форматирование печати и F-строки
Вы уже знаете, как распечатывать переменные с помощью функции print().
Но эта фукция применяется только для вывода одной переменной без дру-
гого текста. F-строки дают вам больше контроля над выходным форматом.
Например:
var1 = 10.54
print(f'Значение переменной равно {var1}, и я от этого счастлив.')

>> Значение переменной равно 10.54, и я от этого счастлив.

  Обратите внимание на две ключевые особенности f-строки: начальную
букву f перед первой кавычкой и фигурные скобки {}, заключающие имена
переменных, которые заменяются значениями переменных.
  Следующий ниже блок кода еще больше подчеркивает гибкость f-строк:
theList = ['Майк', 7]
print(f'{theList[0]} ест {theList[1]*100} гр. шоколада каждый день.')

>> Майк ест 700 гр. шоколада каждый день.

  Из этого примера следует усвоить два ключевых момента: (1) не волнуй-
тесь, на самом деле я не ем столько шоколада (ну, не каждый день хотя бы),
                                                    Поток управления  309

и (2) вы можете использовать индексацию и исходный код внутри фигурных
скобок, и Python распечатает результат вычисления.
   И последняя особенность форматирования f-строк:
pi = 22/7
print(f'{pi}, {pi:.3f}')

>> 3.142857142857143, 3.143

  Ключевым дополнением в приведенном выше фрагменте исходного кода
является форматное выражение :.3f, которое управляет форматировани-
ем результата. Этот фрагмент кода сообщает Python, что надо напечатать
три числа после запятой. Посмотрите, что происходит, если заменить 3 на
другое целое число, и что происходит, если вставить целое число перед
двоеточием.
  Существует много других вариантов форматирования и иных способов
гибкого вывода текста, но в этой книге вам требуется лишь базовое понима-
ние принципа работы f-строк.



Поток управления
Сила и гибкость программирования исходят из того, что вы наделяете свой
исходный код способностью адаптировать свое поведение в зависимости от
состояния определенных переменных или вводимых пользователем данных.
Динамичность исходного кода обеспечивается инструкциями управления по-
током.


Компараторы
Компараторы – это специальные символы, которые позволяют сравнивать
разные значения. Результатом компаратора является тип данных, именуе-
мый булевым типом, который принимает одно из двух значений: True либо
False. Вот несколько примеров:
print( 4<5 ) # 1
print( 4>5 ) # 2
print( 4==5 ) # 3

   Результаты этих строк таковы: True в #1 и False в #2 и #3.
   Третье выражение содержит двойной знак равенства. Он сильно отличает-
ся от одиночного знака равенства, который, как вы уже знаете, используется
для присваивания значений переменной.
   Еще два компаратора таковы: <= (меньше или равно) и >= (больше или
равно).
310  Краткое руководство по языку Python


Инструкции if
Инструкции if интуитивно понятны, потому что они применяются все вре-
мя: если (if ) я устану, то я отдохну.
  Базовая инструкция if состоит из трех частей: ключевого слова if, услов-
ного выражения и содержимого исходного кода. Условное выражение – это
фрагмент исходного кода, который вычисляется как истина либо ложь, за
которым следует двоеточие (:). Если условие истинно, то исполняется весь
исходный код, расположенный ниже и с отступом; если условие ложно, то ни
одна строка исходного кода с отступом не исполняется, и Python продолжит
исполнение исходного кода без отступа.
  Вот пример:
var = 4
if var==4:
  print(f'{var} равно 4!')

print("Я -снаружи цикла +for+.")

>> 4 равно 4!
Я - снаружи цикла +for+.

  А вот еще пример:
var = 4
if var==5:
  print(f'{var} равно 5!')

print("Я -снаружи цикла +for+.")

>> Я - снаружи цикла +for+.

   Первое сообщение пропускается, так как 4 не равно 5; следовательно, ус-
ловное выражение ложно, и поэтому Python игнорирует весь исходный код
с отступом.

Инструкции elif и else
Эти два примера показывают базовую форму инструкции if. Инструкции
if могут содержать дополнительные условные конструкции для повышения
сложности потока информации. Прежде чем читать мое объяснение следую-
щего ниже фрагмента исходного кода и прежде чем набирать его на Python на
своем компьютере, попытайтесь понять исходный код и предсказать, какие
сообщения будут напечатаны:
var = 4

if var==5:
  print('var is 5') # фрагмент 1
elif var>5:
                                                     Поток управления  311

  print('var > 5') # фрагмент 2
else:
  print('var < 5') # фрагмент 3

print('За пределами if-elif-else')

  Когда Python встречает такую инструкцию, он вычисляет сверху вниз. Та-
ким образом, Python начнет с первого условного выражения после if. Если
это условие истинно, то Python исполнит фрагмент 1, а затем пропустит все
последующие условия. То есть как только Python встречает истинное условие,
то исполняется исходный код с отступом, и инструкция if заканчивается.
Истинность последующих условных выражений не имеет значения; Python
не будет их проверять, ни исполнять их исходный код с отступом.
  Если первое условное условие ложно, то Python перейдет к следующему
условному выражению, которым является elif (сокращенно от «else if», то
есть «если же» или «иначе если»). Опять же, Python исполнит последующий
исходный код с отступом, если условие истинно, или пропустит исходный
код с отступом, если условие ложно. В этом примере исходного кода показана
одна инструкция elif, но таких инструкций может быть несколько.
  В последней инструкции else условное выражение отсутствует. Это похоже
на «план Б» инструкции if: он исполняется, если все предыдущие условные
выражения ложны. Если хотя бы одно из условий истинно, то исходный код
else не вычисляется.
  Результат этого примера исходного кода:
var <5
За пределами if-elif-else


Несколько условий
Условные выражения можно комбинировать с помощью операторов and и or.
Это кодовый аналог фразы «Если пойдет дождь и мне нужно прогуляться, то
я возьму с собой зонт». Вот несколько примеров:
if 4==4 and 4<10:
  print('Пример исходного кода 1.')

if 4==5 and 4<10:
  print('Пример исходного кода 2.')

if 4==5 or 4<10:
  print('Пример исходного кода 3.')

>> Пример исходного кода 1.
Пример исходного кода 3.

  Текст Пример исходного кода 2 не напечатался, потому что 4 не равно 5. Од-
нако при использовании or по меньшей мере одно из условных выражений
истинно, поэтому был исполнен последующий исходный код.
312  Краткое руководство по языку Python


Циклы for
Теперь ваших навыков Python достаточно, чтобы распечатать числа от 1 до
10. Для этого можно применить следующий ниже фрагмент исходного кода:
print(1)
print(2)
print(3)

И так далее. Но эта стратегия не масштабируется – что, если я попрошу вас
напечатать числа до миллиона?
  Повторение исходного кода в Python осуществляется с помощью циклов.
Самый важный вид цикла называется циклом for. Для того чтобы создать
цикл for, нужно указать итерируемый объект (итерируемый объект – это пе-
ременная, используемая для прокручивания всех элементов в этой перемен-
ной; в качестве итерируемого объекта может использоваться список) и затем
любое число строк исходного кода, которое должно быть исполнено внутри
цикла for. Я начну с очень простого примера, а затем мы его разовьем:
for i in range(0,10):
  print(i+1)

  Выполнение этого фрагмента исходного кода выведет числа от 0 до 10.
Функция range() создает итерируемый объект с собственным типом данных,
именуемым диапазоном, который часто используется в циклах for. Перемен-
ная диапазона содержит целые числа от 0 до 9. (Внимание! Исключительная
верхняя граница. Кроме того, если вы начинаете считать от 0, то первый
входной аргумент не нужен, поэтому range(10) совпадает с range(0,10)) . Но
мои инструкции заключались в том, чтобы печатать числа от 1 до 10, поэтому
внутри функции print нужно добавить 1. В этом примере также показано,
что переменную итерации можно использовать как обычную числовую пере-
менную.
  Циклы for могут прокручивать другие типы данных. Рассмотрим следую-
щий ниже пример:
theList = [ 2, 'hello', np.linspace(0,1,14) ]
for item in theList:
  print(item)

  Теперь мы прокручиваем список в цикле, и на каждой итерации цикла зна-
чение переменной item устанавливается равным каждому элементу в списке.


Вложенные инструкции управления
Вложение инструкций управления потоком в другие инструкции управления
потоком придает исходному коду дополнительный уровень гибкости. По-
пытайтесь выяснить, что именно исходный код делает, и предскажите его
результат. Затем наберите его на Python и проверьте свою гипотезу:
                                Получение помощи и приобретение новых знаний  313

powers = [0]*10

for i in range(len(powers)):
  if i%2==0 and i>0:
    print(f'{i} - это четное число')

  if i>4:
    powers[i] = i**2

print(powers)

  Я еще не рассказал вам об операторе %. Он называется оператором деления
по модулю, и он возвращает остаток после деления. Таким образом, 7%3 = 1,
потому что 3 входит в 7 дважды с остатком 1. Аналогичным образом 6%2 = 0,
потому что 2 входит в 6 три раза с остатком 0. Фактически k%2 = 0 для всех
четных чисел и k%2 = 1 для всех нечетных чисел. Таким образом, выражение
типа i%2==0 позволяет проверять четность/нечетность числовой переменной i.



Измерение времени вычислений
При написании и оценивании исходного кода вам нередко нужно знать про-
должительность времени, которое требуется компьютеру для выполнения
определенных фрагментов исходного кода. В Python есть несколько способов
измерить затрачиваемое время; здесь показан один простой метод с исполь-
зованием библиотеки time:
import time

clockStart = time.time()
# некий исходный код...
compTime = time.time() - clockStart

  Идея состоит в запрашивании локального времени операционной системы
дважды (это результат работы функции time.time()): один раз перед запус­
ком некоторого фрагмента исходного кода или функций и один раз после
выполнения этого фрагмента кода. Разница во времени системных часов
и есть время вычислений. Результатом является прошедшее время в секун-
дах. Часто бывает удобно умножать результат на 1000, чтобы распечатывать
результаты в миллисекундах (мс).



Получение помощи и приобретение новых
знаний
Уверен, вы слышали фразу «Математика – это не зрелищный вид спорта». То
же самое и с программированием: единственный способ научиться програм-
314  Краткое руководство по языку Python

мировать – это программировать. Вы будете делать много ошибок, разочаро-
вываться, потому что не сможете понять, как сообщить Python делать то, что
вам нужно, будете встречать массу ошибок и предупреждающих сообщений,
которые вы не сможете расшифровать, и просто будете сильно злиться на
вселенную и все, что в ней есть (да, вы знаете, о чем я говорю).


Что делать, когда дела идут наперекосяк
Давайте я вам напоследок расскажу анекдот, чтобы вас поддержать: четыре
инженера садятся в машину, а машина не заводится. Инженер-механик гово-
рит: «Вероятно, проблема с зубчатым ремнем распредвала». Инженер-химик
говорит: «Нет, я думаю, проблема в газовоздушной смеси». Инженер-элект­
рик говорит: «Мне кажется, что неисправны свечи зажигания». Наконец,
инженер-программист говорит: «Давайте просто выйдем из машины и за-
ново войдем».
   Мораль этой истории такова: когда вы сталкиваетесь с какими-то необъ-
яснимыми проблемами в своем исходном коде, вы можете попробовать
перезапустить ядро, то есть движок, на котором работает Python. Это не ис-
правит ошибки программирования, но может устранить ошибки, связанные
с перезаписью или переименованием переменных, перегрузкой памяти или
системными сбоями. В блокнотах Jupyter можно перезапускать ядро через
опции меню. Имейте в виду, что перезапуск ядра очищает все переменные
и настройки среды. Возможно, вам придется перезапустить исходный код
с самого начала.
   Если ошибка сохраняется, то поищите в интернете сообщение об ошибке,
имя используемой функции или краткое описание проблемы, которую вы пы-
таетесь решить. Python имеет огромное международное сообщество, и су­щест­
вует множество онлайновых форумов, в которых люди обсуждают и решают
проблемы и недоразумения, связанные с программированием на Python.



Резюме
Овладение таким языком программирования, как Python, требует многих
лет упорного обучения и практики. Даже достижение хорошего начального
уровня занимает от недель до месяцев. Я надеюсь, что данная глава дала вам
достаточно навыков, чтобы закончить эту книгу. Но, как я писал в главе 1,
если вы обнаружите, что понимаете математику, но испытываете затрудне-
ния с исходным кодом, то, возможно, вы захотите отложить эту книгу, еще
немного потренироваться в Python, а затем вернуться.
  С другой стороны, вы также должны рассматривать эту книгу как способ
улучшить свои навыки программирования на Python. Так что если вы не по-
нимаете какой-то фрагмент исходного кода в книге, то изучение линейной
алгебры – отличный повод изучить Python побольше!
Дополнение                        А
                                    Теорема о ранге
                                        и нульности

Доказательство теоремы о ранге и нульности1 относится к изложению мат­
ричных нуль-пространств в разделе «Нуль-пространства» главы 6 данной
книги. Вкратце: нуль-пространство мат­ри­цы – это множество всех векторов
y, таких что Ay = 0 (за исключением тривиального случая y = 0). Доказатель-
ство включает в себя демонстрацию того, что ATA и A имеют одинаковую
размерность нуль-пространства, а это означает, что они должны иметь оди-
наковый ранг. Начнем с доказательства того, что A и ATA имеют одинаковое
нуль-пространство.

    Ay = 0;                                                                          (A.1)
    ATAy = AT0;                                                                      (A.2)
     T
    A Ay = 0.                                                                        (A.3)

  Уравнения A.1 и A.3 показывают, что любой вектор в нуль-простран­стве A
также находится в нуль-пространстве ATA. Это доказывает, что нуль-прост­
ранство ATA является подмножеством нуль-пространства A. Это половина
доказательства, потому что нам также нужно продемонстрировать, что лю-
бой вектор в нуль-пространстве ATA также находится в нуль-прост­ран­стве A.

    ATAy = 0;                                                                        (A.4)
    yTATAy = yT0;                                                                    (A.5)
         T
    (Ay) Ay = 0;                                                                     (A.6)
    ||Ay|| = 0.                                                                      (A.7)

  Уравнения A.4 и A.7 вместе показывают, что любой вектор в нуль-про­
стран­стве ATA также находится в нуль-пространстве A.

1
    Материал взят из учебника автора «Линейная алгебра: теория, интуиция, исходный
    код» (Linear Algebra. Theory, Intuition, Code, Cohen, Sincxpress BV, 2021, стр. 191). –
    Прим. перев.
316  Теорема о ранге и нульности

  Теперь мы доказали, что ATA и A имеют одинаковые нуль-пространства.
Почему это имеет значение? Строчное пространство (множество всех воз-
можных взвешенных комбинаций строк) и нуль-пространство вместе охва-
тывают все ℝN, и поэтому если нуль-пространства одинаковы, то строчные
пространства должны иметь ту же размерность (это называется теоремой
ранга и нульности). А ранг мат­ри­цы – это размерность строчного простран-
ства, следовательно, ранги ATA и A одинаковы. Доказательство этого утверж-
дения для AAT подчиняется тому же доказательству, что и выше, за исклю-
чением того, что вы начинаете с yTA = 0. Приглашаю вас воспроизвести это
доказательство с ручкой и бумагой.
         Тематический указатель

A                                       LIVE EVIL, мнемоника для порядка
                                        следования операций, 89
Anaconda, установщик, 292               LU-разложение, 174, 183
                                          взаимообмен строк посредством
C                                         матриц перестановок, 185
CGI-фильмы и видеоигры, графика           обеспечение уникальности, 184
в них, 134
Colab, среда, 292                       M
  работа с исходным кодом, 293          matplotlib (Python), 77, 303

D                                       N
def, ключевое слово (Python), 299       np.argmin, функция, 70
                                        np.diag, функция, 79
E                                       np.dot, функция, 37, 42
                                        np.eye, функция, 79
elif и else, инструкции (Python), 310
                                        np.min, функция, 70
                                        np.multiply, функция, 82
F                                       np.outer, функция, 42
f-строка, 308                           np.random, функция, 78
float, тип, 296                         np.sum, функция, 70
for, цикл (Python), 312                 np.tril, функция, 79
                                        np.triu, функция, 79
                                        NumPy
G                                         диагонализация матрицы, 237
Google Colab, среда, 292                  импорт в Python, 301
 работа с исходным кодом, 293             индексация и нарезка, 302
                                          функции полиномиальной
                                          регрессии, 218
I                                         функция svd, 256
IDE (интегрированная среда
разработки), 22, 292
   графическое отображение на
                                        P
   Python, 303                          Python, 20
   отступы в функциях, 300                векторы, 26
if, инструкция (Python), 310              вычисление RREF-формы в библиотеке
   инструкции elif и else, 310            sympy, 182
   несколько условий, 311                 интегрированные среды разработки, 22
int, тип, 296                             краткое руководство, 291
                                            варианты использования и
                                            альтернативы, 291
L                                           визуализации, 303
LAPACK, библиотека, 203                     измерение времени вычисления, 313
318  Тематический указатель

      интегрированные среды разработки   T
      (IDE), 292
                                         type, функция, 296
      переложение формул в исходный
      код, 305
      переменные, 294                    А
      получение помощи и приобретение    Алгебра линейная, 17
      новых знаний, 313                    приложения в исходном коде, 20
      поток управления, 309              Анализ главных компонент (PCA), 59, 229
      применение Python локально           доказательство, что собственное
      и онлайн, 292                        разложение решает цель оптимизации
      форматирование печати и              PCA, 271
      f-строки, 308                        использование собственного
      функции, 297                         и сингулярного разложений, 268
    расположенный в облаке, 293            математика PCA-анализа, 269
    решение задачи о наименьших            шаги выполнения PCA, 271
    квадратах                              PCA посредством сингулярного
      переложение левообратной матрицы     разложения, 272
      в исходный код, 192                Анализ линейный дискриминантный
    сингулярное разложение, 256          (LDA), 273
    создание анимаций данных, 131        Анализ независимых компонент, 59
    терминологические разночтения        Анимация данных, создание, 131
    с линейной алгеброй меловой          Аппроксимация низкоранговая, 259
    доски, 35                              посредством сингулярного
    функция для LU-разложения              разложения, 275
    в библиотеке SciPy, 184
                                         Б
Q                                        Базис, 57, 58, 62
QR-разложение                              важность в науке о данных и машинном
 полное, 166                               обучении, 59
 размеры матриц Q и R, 166               Библиотека функций Python, 301
 решение задачи о наименьших             Блокнот Jupyter, 22, 292
 квадратах, 201                          Буквы в линейной алгебре, 41
 экономное или сокращенное, 166
 QR и обратные матрицы, 169              В
QR-разложение , 165
                                         Вектор, 17
                                           базисный, 235
R                                            нуль-пространство, 235
RREF (строчно приведенная ступенчатая      базисный ортогональный, 255
форма), 181                                левый сингулярный, 254
                                           линейная независимость, 51
                                           линейно-взвешенная комбинация, 50
S                                          методы помимо точечного
SciPy, библиотека (Python), 184            произведения векторов, 40
  вычисление обобщенного собственного      методы умножения помимо точечного
  разложения, 247                          произведения
statsmodels, библиотека                      адамарово умножение, 40
  применение для создания                    внешнее произведение, 41
  регрессионной таблицы, 212                 перекрестное и тройное
str, тип, 296                                произведение, 42
sympy, библиотека (Python), 182            модуль и единичные векторы, 35
                                                    Тематический указатель  319

  нуль-пространственный (собственные        приравнивание собственного числа
  векторы), 235                             к нулю, 233
  операции на векторах, 28                Вектор
    геометрия сложения и вычитания          ортогональный, нулевое точечное
    векторов, 30                            произведение, 40
    геометрия умножения вектора на          сингулярный, 272, 277
    скаляр, 32                              собственный, 228
    операция транспонирования, 33         Взаимообмен строк, 185
    сложение и вычитание двух             Взаимообмен строками посредством
    векторов, 28                          матриц перестановок, 185
    транслирование векторов на            Визуализации на Python, 303
    Python, 34                            Визуализация матриц, 76
    умножение вектора на скаляр, 31       Время вычисления, измерение на
    усреднение векторов, 33               Python, 313
  ортогональный собственный               Вычитание векторов, 29
    доказательство ортогональности, 239     в линейно-взвешенной комбинации, 50
  подпространство и охват, 54               геометрия вычитания векторов, 30
  правый сингулярный, 254
  применения, 64
    кластеризация методом k-средних, 68   Г
    корреляция и косинусное               Геометрия вектора, собственные
    сходство, 64                          векторы, 228
    фильтрация временных рядов            Геометрия векторов, 27
    и обнаружение признаков, 67             сложение и вычитание векторов, 30
  сингулярный, ортогональная                точечное произведение, 39
  природа, 259                              умножение вектора на скаляр, 32
  собственный                             График крутого склона, 230
    в нуль-пространстве матрицы,
    сдвинутой на ее собственное
    число, 232
                                          Д
    действительно-значный                 Диагонализация одновременная двух
    в симметричных матрицах, 240          матриц, 247
    из собственного разложения            Диапазон, тип, 312
    сингулярной матрицы, 241              Дисперсия, 129
    ортогональный, 238                     комбинируемая с помощью
    отыскание, 234                         линейно-взвешенной комбинации
      неопределенность собственных         в PCA, 269
      векторов по знаку и шкале, 235       конвертация сингулярных чисел, 260
    хранение в столбцах матрицы,          Длина (векторы), 25
    а не в ее строках, 234                 терминологические разночтения между
  создание и визуализация в NumPy, 24      математикой и Python, 35
  точечное произведение, 36               Длина геометрическая (векторы), 35
    геометрия, 39                         Доказательство
    дистрибутивная природа, 38             математическое в противовес
  умножение левого сингулярного            пониманию на уровне интуиции на
  вектора на правый сингулярный            основе исходного кода, 20
  вектор, 258                              мягкое, 21
  умножение матрицы на вектор, 85          путем отрицания, 153
Вектор единичный, 35
  создание из неединичных векторов, 35
Вектор нулей, 31
                                          З
  линейная независимость, 53, 54          Зависимость линейная, 213
320  Тематический указатель

И                                        Коэффициент корреляции, 64
                                           нормализации, 64
Изображение                              Коэффициент корреляции Пирсона, 38
  визуализация крупных матриц в виде       выраженный в терминах линейной
  изображений, 76
                                           алгебры, 65
  сжатие и сингулярное разложение, 275
                                           по сравнению с косинусным
Индексация
                                           сходством, 66
  в NumPy, 302
                                           формула, 65
  математические соглашения
                                         Коэффициент регрессии, 212
  по сравнению с соглашениями
  в программировании, 37
  матриц, 77                             М
  списков и связанных типов данных       Массив
  в Python, 297                           векторы в виде массивов NumPy, 26
Индекс в формате строка, столбец          матрица в виде двумерного массива, 88
(элементы матрицы), 33                    NumPy, 301
Инструкция управления вложенная, 312     Массив неориентированный, 26
Интерпретация геометрическая обратной    Математика
матрицы, 156                              на меловой доске в противовес
                                          имплементированной в рабочем
К                                         коде, 25
Кадр данных pandas, 211                   отношение к обучению, 19
Квадрат ошибки между предсказанными       усвоение, математические
и наблюдаемыми данными, 195               доказательства в противовес
Квадраты наименьшие                       пониманию на уровне интуиции на
  обычные (OLS), 212                      основе программирования, 20
  применения, поиск в параметрической    Матрица, 17, 76, 91, 97
  решетке для отыскания модельных         адъюгатов, 150
  параметров, 218                         верхнетреугольная
Кластеризация методом k-средних, 68         преобразование плотной матрицы,
Ковариация, 128                             с использованием приведения
Комбинация линейно-взвешенная               строк, 178
  в умножении матрицы на вектор, 86       верхнетреугольные матрицы, 168, 202
  комбинированная с дисперсией            верхние треугольные матрицы, 79
  в PCA, 269                              высокая, 79
Комбинация линейно-взвешенная, 50, 62       вычисление левообратной
Комментарий, 298                            матрицы, 151
Компаратор (Python), 309                    экономное QR-разложение по
Компонента, 270                             сравнению с полным, 166
Компонента                                вычисление определителя, 117
  параллельная (ортогональное             Гилберта, обратная ей матрица и их
  разложение векторов), 44                произведение, 155
  перпендикулярная (ортогональное         диагональная
  разложение векторов), 44                  вычисление обратной
Константа, 175, 191                         матрицы, 148
Корреляция по сравнению с косинусным        матрица сингулярных чисел, 255
сходством, 66                               хранение собственного числа, 236
Коэффициент, 50, 175                      диагональные матрицы, 79, 234
  коэффициент регрессии для               единичная, 79
  регрессоров, 212                          вектор констант в первом
  отыскание множества коффициентов,         столбце, 183
  которое минимизирует квадраты             замена в обобщенном собственном
  ошибок, 196                               разложении, 247
                                               Тематический указатель  321

  обратная матрица, содержащая           вычисление обратной матрицы для
  преобразование матрицы                 матрицы 2×2, 146
  в единичную матрицу, 144               вычисление односторонней обратной
  получение посредством строчно          матрицы, 151
  приведенной ступенчатой формы, 181     геометрическая интерпретация, 156
квадратичная форма, 243                  обратная матрица ортогональной
квадратная                               матрицы, 163
  вычисление обратной матрицы для        односторонняя, 145
  полноранговой матрицы, 149             полная, 145
  диагонализация, 236                    посредством метода устранения по
  для собственного разложения, 231       Гауссу–Жордану, 182
  определенность, 245                    псевдообратная матрица
  уникальное LU-разложение               Мура–Пенроуза, 154
  полноранговой матрицы, 183             типы обратных матриц и условия
квадратные матрицы, 90                   обратимости, 145
  по сравнению с неквадратными, 78       уникальность, 153
ковариаций                               численная стабильность обратной
  использование собственного             матрицы, 155
  разложения в PCA, 271                  QR-разложение, 169
  многопараметрических данных, 128     обратная Мура–Пенроуза, сингулярное
  обобщенное собственное разложение    разложение, 262
  в линейном дискриминантном           одноранговая, 111
  анализе, 273                         операции
ковариаций и корреляций, 128             транспонирование, 88
  матрица корреляций в примере           LIVE EVIL, мнемоника для порядка
  предсказания велопрокатов, 209         следования операций, 89, 92
конвертирование уравнений              определитель, 117
в матрицы, 175                         ортогональная, 132, 162
кофакторов, 150                          матрицы перестановок, 185
левообратная, 145, 151, 192              матрицы сингулярных
математика, 80                           векторов, 255
  сдвиг матрицы, 81                      ортогональные столбцы и столбцы
  сложение и вычитание матриц, 80        с единичной нормой, 162
  умножение на скаляр и адамарово        преобразование неортогональных
  умножение, 82                          матриц процедурой
миноров, 149                             Грама–Шмидта, 164
неквадратная                             преобразование неортогональных
  наличие односторонней обратной         матриц QR-разложением, 165
  матрицы, 145                         особые свойства, относящиеся к
  применение обратной матрицы          собственному разложению, 238
  Мура–Пенроуза, 264                   перестановок, 164, 202
неквадратные матрицы, 90                 взаимообмен строк, 185
  по сравнению с квадратными           плохо обусловленная, 261
  матрицами, 78                          обработка, 262
необратимая, 146                       поворота, 131
нормы матриц, 97                       полноранговая
нулей, 79                                вычисление обратной матрицы для
нуль-пространство, 100, 104              квадратной матрицы, 149
обратная, 144, 202                       отсутствие нуль-значных
  вычисление, 146                        собственных чисел, 242
  вычисление для диагональной          правообратная, 145
  матрицы, 148                         применения, 128
322  Тематический указатель

   геометрические трансформанты               умножение матриц, 84
   посредством умножения матриц на          стобцовое пространство, 100
   векторы, 131                             строчное пространство, 100, 104
   матрицы ковариаций                       тождественного преобразования, 79
   многопараметрических данных, 128         транспонирование, 33
   обнаружение признаков                    треугольная, нижнетреугольное –
   изображения, 135                         верхнетреугольное (LU)
 применения ранга, 114                      разложение, 183
 прямоугольные матрицы, 78                  треугольные матрицы, 79
 псевдообратная, 145                        чистого поворота, 131, 164
 псевдообратная Мура–Пенроуза, 154, 213     широкая, правообратная матрица
 ранг, 108                                  для широкой полноранговой
 рангово-дефицитная, 146                    матрицы, 145
 рангово-дефицитная необратимая, 146        эрмитова, 257
 рангово-пониженная, 146                  Метод, 299
   собственное разложение, 242            Метод аддитивный (создание
 расчетная                                симметричных матриц), 91
   в примере предсказания                 Метод левообратный, 201
   велопрокатов, 210                        по сравнению с решателем методом
   полиномиальной регрессии, 215            наименьших квадратов NumPy, 193
   рангово-пониженная, 214                Метод мультипликативный (создание
   сдвиг матрицы для преобразования       симметричных матриц), 91
   рангово-пониженной матрицы             Метод наименьших квадратов, 192
   в полноранговую матрицу, 214             геометрическая перспектива, 194
 решетка, 150                               почему он работает, 195
 свойства ранга, 108                        применения, 207
 симметричная                                 поиск в параметрической решетке
   особые свойства, относящиеся               для отыскания модельных
   к собственному разложению                  параметров, 218
     действительно-значные                    полиномиальная регрессия, 215
     собственные числа, 240                 решение посредством
     ортогональные собственные              QR-разложения, 201
     векторы, 238                         Метод наименьших квадратов
   положительно (полу)                    посредством QR-разложения, 201
   определенная, 246                      Метод устранения
 симметричные матрицы, 89, 92             по Гауссу–Жордану, 181
   создание из несимметричных             Многочлен характеристический, 119
   матриц, 89                             Многочлен характеристический
 сингулярная, 146                         матрицы, 233
   собственное разложение, 241            Множество базисное, 58
 скалярная, 232                             потребность в линейной
 след матрицы, 99                           независимости, 61
 случайных чисел, 78                      Множество векторов, 49, 62
   применение LU-разложения, 185            линейная независимость, 52
 создание и визуализация в NumPy, 76      Множество декартово базисное, 58
   визуализация, индексация и нарезка     Множество стандартное базисное, 58
   матриц, 76                             Модель линейная общая, 84, 190
   специальные матрицы, 78                  на простом примере, 197
 специальные матрицы, 91                    настройка, 190
 стандартное умножение, 82                  решение, 192
   правила допустимости умножения             геометрическая перспектива
   матриц, 83                                 наименьших квадратов, 194
                                                   Тематический указатель  323

   почему работает метод наименьших     Операция транспонирования, 26, 33
   квадратов, 195                        на матрицах, 88
   точность решения, 193                 порядок следования операций
 решение с мультиколлинеарностью, 213    с матрицами, правило LIVE EVIL, 89
 терминология, 190                       симметричная матрица,
Модель статистическая, 190               эквивалентная результату своего
 регуляризация, 213                      транспонирования, 90
Модуль (векторов), 27                    транспонирование левой матрицы
 единичные векторы и модуль, 35          в матрицу ковариаций, 130
 собственные векторы, 238, 244          Определенность, 245
Мультиколлинеарность, 213                положительная (полу)
                                         определенность, 245
                                        Определенность положительная, 245
Н
                                        Определитель, 117
Наименьшие квадраты, применения, 207     вычисление, 117
  предсказание количеств велопрокатов    приравнивание к нулю определителя
  на основе погоды, 207                  матрицы, сдвинутой на ее собственное
Направление (или угол) вектора,          число, 232
собственные векторы, 238                 свойства, 117
Направление (или угол) векторов, 27      с линейными зависимостями, 119
Нарезка                                 Ориентация
  в NumPy, 302                           вдоль столбца (векторы), 24
  матрицы, 77                            вдоль строки (векторы), 24
Независимость линейная, 51, 62          Ориентация (векторов)
  гарантия уникальности в базисных       в сложении векторов, 30
  множествах, 61                        Ориентация (векторы), 24
  ее определение на практике, 52        Освоение языка Python (Лутц), 291
  математика, 53                        Отражение Хаусхолдера, 164
  независимость и вектор нулей, 54      Очертание (векторы в Python), 25
Независимость нелинейная, 54            Ошибка
Неопределенность собственных векторов    в Python, 295
по знаку и шкале, 235                    квадраты ошибок между
Норма (вектора), 35                      предсказанными и наблюдаемыми
  соотношение двух норм, 270             данными, 195
Норма матрицы, 97
  индуцированная, 98
                                        П
  поэлементная, 98
  уравнение Фробениуса, 98              Переменная зависимая, 190, 198
Норма Фробениуса, 214                    в примере предсказания
Нормализация для коэффициента            велопрокатов, 208
корреляции, 64                          Переменная независимая, 190, 198
Нуль-пространство, 104                   в примере предсказания
                                         велопрокатов, 208
                                        Переменная разряженная, 210
О                                       Переменная (Python), 294
Обнаружение признаков, а также           именование, 297
фильтрация временных рядов, 67           индексация, 297
Обнаружение признаков                    типы данных, 296
изображения, 135                        Пересечение или сдвиг, 175, 191, 200
Объект итерируемый, 312                  добавление члена пересечения
Оператор умножения в Python (*), 40      в наблюдаемые и предсказанные
  умножение матриц, 82                   данные, 200
324  Тематический указатель

Подавление шума, 230                          в умножении матрицы на вектор, 88
Подматрица, 77                                по сравнению с транслированием, 41
Подпространство и охват (вектор), 54, 62    перекрестное, 42
  базис, 60                                 точечное, 36
Подстановка обратная, 181, 182, 202           в коэффициентах корреляции, 65
Поиск в параметрической решетке для           в умножении матрицы на вектор, 88
отыскания модельных параметров, 218           в фильтрации временных рядов, 67
Положение стандартное (векторы), 27           геометрия, 39
(Полу)определенность                          деление на произведение векторных
положительная, 245                            норм, 65
Поток управления в Python, 309                дистрибутивная природа, 38
  вложенные инструкции                        для ортогональных матриц, 163
  управления, 312                             корреляция Пирсона и косинусное
  инструкции if, 310                          сходство на основе точечного
    инструкции elif и else, 310               произведения, 66
    несколько условий, 311                    нулевое точечное произведение
  компараторы, 309                            ортогональных векторов, 40
Предсказание количеств велопрокатов           обозначение по сравнению
на основе погоды (пример), 207                с внешним произведением, 41
  мультиколлинеарность, 213                 тройное, 42
  предсказанные данные по сравнению        Пространство
  с наблюдаемыми данными, 211               столбцовое, 100
  регуляризация матрицы, 213                строчное, 104
  создание регрессионной таблицы           Процедура Грама–Шмидта, алгоритм, 164
  с помощью библиотеки statsmodels, 212
Преобразование                             Р
  Хаусхолдера, 164                         Разложение, 162
  откат геометрического преобразования       векторов ортогональное, 31, 42
  с помощью обратной матрицы, 156            на простые множители, 42
Преподаватель, как эту книгу                 сингулярное, 254
использовать, 23                               вычисление из собственного
Приведение строк, 178                          разложения, 259
  метод устранения                               сингулярное разложение матрицы
  по Гауссу–Жордану, 181                         AtA, 260
  обратная матрица посредством метода          ключевые моменты, 263
  устранения по Гауссу–Жордану, 182            на Python, 256
  цель, 178                                    низкоранговые аппроксимации, 275
Примеры исходного кода из книги, 22            обратная матрица
Программирование                               Мура–Пенроуза, 262
  жесткое, 307                                 общая картина, 254
  линейно-алгебраические приложения              важные признаки, 255
  в рабочем коде, 20                             сингулярные числа и ранг
  мягкое, 307                                    матрицы, 256
  понимание на уровне интуиции на              одноранговые слои матрицы, 257
  основе исходного кода в противовес           применение для
  математическим доказательствам, 20           шумоподавления, 276
Проекция                                       применения собственного
  ортогональная, 44                            и сингулярного разложений, 268
  ортогональная вектора, 194, 195                анализ главных компонент
Проецирование размерности данных, 230            посредством сингулярного
Произведение                                     разложения, 272
  внешнее, 41                                собственное, 227
                                                    Тематический указатель  325

    бесчисленное число тонкостей, 247     Расстояние евклидово, 68, 69
    вычисление для матрицы,               Растягивание собственного вектора
    помноженной на ее                     матрицы, 228
    транспонированную версию, 259         Регрессия
    диагонализация квадратной               полиномиальная, 215
    матрицы, 236                              доля регуляризации, 214
    интерпретации собственных чисел         создание регрессионной таблицы
    и собственных векторов, 228             с помощью библиотеки statsmodels, 212
      анализ главных компонент            Регрессор, 210
      в статистике, 229                   Регуляризация, 213
      подавление шума, 230                Регуляризация L2, 98
      уменьшение размерности (сжатие      Результат геометрического
      данных), 231                        преобразования, 86
    квадратичная форма, определенность    Результат преобразования
    и собственные числа, 245                геометрические результаты
    обобщенное, 246                         преобразования в умножении матрицы
      на матрицах ковариаций                на вектор, 86
      в линейном дискриминантном          Решение с минимальной нормой
      анализе, 273                        (min-norm), 213
    особые свойства симметричных          Решение тривиальное, 31
    матриц, 238
    отыскание собственных векторов, 234
    отыскание собственных чисел, 231
                                          С
    применения собственного               Самообучение, как эту книгу
    и сингулярного разложений, 268        использовать, 23
      анализ главных компонент, 268       Свертка, 68
      линейный дискриминантный            Свертка (изображение), 137
      анализ, 273                         Свойство ранга матрицы, 108
    разные интерпретации, 228             Сдвиг, 175
    сингулярное разложение, 254           Сдвиг матрицы, 81, 91, 214
    сингулярных матриц, 241                 на ее собственное число, 232
  Холецкого, 246                          Сжатие данных, 231, 275
Размерность (векторы), 24                 Сигмоида, функция, 307
  в сложении векторов, 29                 Симуляция методом Монте-Карло, 246
  в точечном произведении, 37             Система уравнений
  геометрические измерения                  конвертирование уравнений
  в Python, 25                              в матрицы, 175
Ранг                                        решение посредством обратной
  матрицы, 108                              матрицы, 182
  применения, 114                           решение с помощью метода устранения
  свойства ранга матрицы, 108               по Гауссу–Жордану, 182
  сдвинутых матриц, 113                   Скаляр
  сложенных и умноженных матриц, 112        обратное значение, 146
  специальных матриц, 110                   отрицательный, изменение
Ранг (матриц)                               направления векторов, 32
  из собственного разложения                сложение скаляра с вектором, 32
  сингулярной матрицы, 242                  сложение скаляра с вектором
Ранг (матрицы)                              и умножение скаляра на вектор, 50
  сингулярное разложение                    умножение вектора на скаляр, 31
  и одноранговые слои матрицы, 257        След матрицы, 99
  сингулярные числа, 256                  Слово ключевое в Python, 299
Расстояние, 69                            Сложение векторов, 28
326  Тематический указатель

  в точечном произведении векторов, 37   Умножение
  геометрия сложения векторов, 30          адамарово, 40
  сложение скаляра с вектором, 32             матриц, 82
  сложение скаляра с вектором              вектора на скаляр, 31
  и умножение скаляра на вектор, 50        геометрия умножения вектора на
Соотнесенность между векторами, 38         скаляр, 32
Список, 296                                матриц, 91
  индексация, 297                          матрицы на вектор, 85
  представление векторов на Python, 26        геометрические трансформанты, 131
  умножение списка на скаляр, 31              линейно-взвешенные
Среда разработки интегрированная              комбинации, 86
(IDE), 292                                    результаты геометрических
Среда Colab, использование в ней              преобразований, 86
блокнотов Jupyter, 22                      матрицы на скаляр, 82
Среда Google Colab, 22                     методы помимо точечного
Стабильность численная обратной            произведения векторов, 40
матрицы, 155                                  адамарово умножение, 40
Статистика                                    внешнее произведение, 41
  анализ главных компонент, 229               перекрестное и тройное
  дисперсия, 129                              произведения, 42
  корреляция, 64                           сложение скаляра с вектором
  общая линейная модель, 84                и умножение скаляра на вектор, 50
  решение обратных задач, 43               точечное произведение векторов, 36
  сдвиг матрицы, 81                        умножение матриц и точечное
Столбец с единичной нормой                 произведение векторов, 37
(ортогональные матрицы), 162               умножение матрицы на скаляр
Сходство косинусное, 66                    и адамарово умножение
Сходство между векторами, 38               матрицы, 82
                                         Уникальность
                                           гарантирование со стороны линейной
Т                                          независимости, 61
Тензор, 137                                обратной матрицы, 153
Тип булев, 309                             QR-разложения, 168
Тип данных                               Упражнения по программированию, 22,
  в Python, 296                          46, 62, 71, 92
  переменных, в которых хранятся         Уравнение
  векторы, 31                              матричное, 144
  представление векторов на Python, 26        для матрицы ковариаций, 129
Транслирование, 69                            работа с матричными
  по сравнению с внешним                      уравнениями, 176
  произведением, 41                           по сравнению со скалярным
  транслирование векторов                     уравнением, 176
  на Python, 34                            переложение формул в исходный
Трансформанта геометрическая,              код, 305
применение умножения матрицы на            системы уравнений, 174, 181
вектор, 131                                   конвертирование уравнений
Требования предварительные, 19                в матрицы, 175
                                              работа с матричными
                                              уравнениями, 176
У                                          скалярное по сравнению с матричным
Угол (или направление) векторов, 27        уравнением, 176
Уменьшение размерности, 231              Усреднение векторов, 33
                                                   Тематический указатель  327

Устранение по Гауссу–Жордану, 181, 204   Число сингулярное, 254
  обратная матрица, 182                    важные свойства, 259
                                           конвертация в дисперсию, 260
Ф                                          ранг матрицы, 256
                                         Число собственное, 228
Фильтрация, 67                             график матрицы ковариаций
  изображение, 135
                                           в данных, 230
Фильтрация временных рядов, 67
                                           действительно-значное, 240
Форма квадратичная матрицы
                                           из собственного разложения
  нормализованная квадратичная форма
                                           сингулярной матрицы, 241
  матрицы ковариаций в данных, 270
                                           комплексно-значное, 240
Форма квадратичная
                                           определение знака квадратичной
нормализованная, 270
Форма матрицы квадратичная, 243            формы, 245
Форма матрицы ступенчатая, 178             отыскание, 231
  предпочтительные формы, 179              подавление шума, 230
Форма ступенчатая строчно приведенная      уравнение собственного числа, 228
(RREF), 181                                  векторно-скалярная версия, 257
Форматирование печати и f-строки, 308        для диагонализации матрицы, 236
Формула, переложение формул
в исходный код, 305                      Ш
Функция (Python), 297
  библиотеки, 301                        Шум
    импорт NumPy, 301                     плохо обусловленная матрица
    индексация и нарезка в NumPy, 302     в качестве фактора усиления, 264
  методы в качестве функций, 299          применение сингулярного разложения
  написание своих собственных             для шумоподавления, 276
  функций, 299
                                         Э
Ц                                        Элемент опорный, 178
Центрирование по среднему                  в устранении по Гауссу–Жордану, 181
значению, 64
Центроид, 68                             Я
                                         Ядро
Ч                                          в фильтрации временных рядов, 67
Число кондиционное матрицы, 155, 201,      в фильтрации изображения, 135
261                                        гауссово (двумерное), 135
                           Книги издательства «ДМК ПРЕСС»
                            можно купить оптом и в розницу
                        в книготорговой компании «Галактика»
                          (представляет интересы издательств
                 «ДМК ПРЕСС», «СОЛОН ПРЕСС», «КТК Галактика»).
                      Адрес: г. Москва, пр. Андропова, 38, оф. 10;
         тел.: (499) 782-38-89, электронная почта: books@alians-kniga.ru.
              При оформлении заказа следует указать адрес (полностью),
                       по которому должны быть высланы книги;
                          фамилию, имя и отчество получателя.
            Желательно также указать свой телефон и электронный адрес.
Эти книги вы можете заказать и в интернет-магазине: http://www.galaktika-dmk.com/.




                                Майк Икс Коэн

                      Прикладная линейная алгебра
                       для исследователей данных

                     Главный редактор       Мовчан Д. А.
                                dmkpress@gmail.com
              Зам. главного редактора       Сенченкова Е. А.
                              Перевод       Логунов А. В.
                           Корректор        Синяева Г. И.
                              Верстка       Чаннова А. А.
                      Дизайн обложки        Мовчан А. Г.


                      Гарнитура PT Serif. Печать цифровая.
                        Усл. печ. л. 26,65. Тираж 200 экз.

                  Веб-сайт издательства: www.dmkpress.com
